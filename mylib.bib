@misc{@nekoHashimaIslandCreated2023,
  title = {Hashima {{Island}} - {{Created}} by @neko with {{Luma}}},
  author = {@neko},
  year = {2023},
  journal = {Hashima Island - Created by @neko with Luma},
  urldate = {2023-12-05},
  abstract = {January 26, 2023},
  howpublished = {https://lumalabs.ai/capture/057109e3-79e1-411a-ab84-016cbd417d36},
  file = {C:\Users\emilm\Zotero\storage\7IDM3XQC\057109e3-79e1-411a-ab84-016cbd417d36.html}
}

@misc{17GOALSSustainable,
  title = {{{THE}} 17 {{GOALS}} | {{Sustainable Development}}},
  urldate = {2023-09-12},
  howpublished = {https://sdgs.un.org/goals}
}

@misc{2008PleaseProvideMore2022,
  title = {Please Provide More Detailed Guidance to Flash {{Jetson Nano}} with {{Windows}} 11 {{PC}}},
  author = {2008},
  year = {2022},
  month = dec,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-15},
  abstract = {RESOLVED 13 December:  After a couple of days of frustration, this is the best advice for Windows 11 users not experienced with Linux other than running containers in WSL2.  It is best to get two USB or external drives with size at least 64 GB, or even buy a small PC to install Linux on it. The small PC will avoid you might kill/ format your Windows installation as I did.  If you have 2 USB's you install first ISO on USB 1 and then boot from that USB to install on USB 2 your final image. This is...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/please-provide-more-detailed-guidance-to-flash-jetson-nano-with-windows-11-pc/236808},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\BLDJJCWT\236808.html}
}

@article{2016,
  year = 2016
}

@misc{2xRJ45ModularJack,
  title = {{{2xRJ45 Modular Jack}} - {{TE Connectivity}} 5-6610000-1 | {{3D CAD Model Library}} | {{GrabCAD}}},
  urldate = {2022-05-15},
  howpublished = {https://grabcad.com/library/2xrj45-modular-jack-te-connectivity-5-6610000-1},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\JBHDFMY6\2xrj45-modular-jack-te-connectivity-5-6610000-1.html}
}

@misc{3m175534X60FT3M,
  title = {1755-3/{{4X60FT}} | {{3M Temflex}} 1755 {{Cloth Tape}}, 18m x 19.1mm, {{Black}}, {{Rubber Finish}} | {{RS Components}}},
  author = {3M},
  urldate = {2022-05-16},
  howpublished = {https://no.rs-online.com/web/p/duct-tapes/8409026/},
  file = {C:\Users\emilm\Zotero\storage\BUIDBHMM\8409026.html}
}

@misc{9.solutionsSolutionsCameraKit,
  title = {9.Solutions {{Camera Kit}}},
  author = {9.Solutions},
  journal = {Musikhaus Thomann},
  urldate = {2022-05-16},
  abstract = {Camera bracket kitConsisting of:     1x Quick Mount for lightweight camera, 1x Quick-mount receiver to handle bar, 1x GoPro multi-tool},
  howpublished = {https://www.thomannmusic.no/intl/9solutions\_camera\_kit.htm},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\HRTSF9H2\9solutions_camera_kit.html}
}

@misc{aastalllAnserL2CacheMar2422,
  title = {Anser to: "{{L2}} Cache Rate Profiled in Nsight Compute Is Confused"},
  author = {AastaLLL},
  year = {Mar 24 '22},
  journal = {NVIDIA Developer Forums},
  urldate = {2023-06-17},
  abstract = {Hi, I am profiling a read-only kernel in Nsight Compute on A100. The kernel is very simple and the complete code is as below.  \#include {$<$}cstdint{$>$} \#include {$<$}cuda.h{$>$} \#include {$<$}cuda\_runtime.h{$>$} \#include {$<$}iostream{$>$}  const int BLOCK = 1024; const int BENCH\_SIZE = (1lu {$<<$} 26); //  const int THREAD\_STRIDE = (1lu {$<<$} 16);  // const int BLOCK\_STRIDE = (1lu {$<<$} 8); const int BENCH\_ITER = 16;  \#define checkCudaErrors(err)  \_\_checkCudaErrors (err, \_\_FILE\_\_, \_\_LINE\_\_)  inline void \_\_checkCudaErrors( CUresult er...},
  chapter = {Development Tools},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\MUDIT9YV\11.html}
}

@misc{ablyWebRTCVsWebSocket2023,
  title = {{{WebRTC}} vs. {{WebSocket}}: {{Key}} Differences and Which to Use},
  shorttitle = {{{WebRTC}} vs. {{WebSocket}}},
  author = {Ably},
  year = {2023},
  month = jan,
  journal = {Ably Realtime},
  urldate = {2023-05-29},
  abstract = {We compare WebRTC with WebSocket. Discover how they are different, their pros \& cons, and their use cases.},
  howpublished = {https://ably.com/topic/webrtc-vs-websocket},
  file = {C:\Users\emilm\Zotero\storage\VJZXHKWI\webrtc-vs-websocket.html}
}

@misc{AcceleratedGStreamer,
  title = {Accelerated {{GStreamer}}},
  urldate = {2023-05-02},
  howpublished = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3231/index.html\#page/Tegra\%2520Linux\%2520Driver\%2520Package\%2520Development\%2520Guide\%2Faccelerated\_gstreamer.html\%23wwpID0E0UM0HA},
  file = {C:\Users\emilm\Zotero\storage\VMUTGN4W\index.html}
}

@misc{adlerAnswerHowCan2021,
  title = {Answer to "{{How}} Can Calculate Mpeg2/Crc32 in {{Python}}?"},
  author = {Adler, Mark},
  year = {2021},
  month = sep,
  urldate = {2022-05-16}
}

@techreport{adlinkPCIeGIE7xSeries,
  type = {Datasheet},
  title = {{{PCIe-GIE7x Series}}},
  author = {Adlink},
  number = {3.1},
  urldate = {2022-05-02},
  file = {C:\Users\emilm\Zotero\storage\RPX7UPLD\PCIe-GIE7x_50-11177-2020_31.pdf}
}

@misc{adminCentOSRHELChrony,
  title = {{{CentOS}} / {{RHEL}} 7 : {{Chrony V}}/s {{NTP}} ({{Differences Between}} Ntpd and Chronyd) {\textendash} {{The Geek Diary}}},
  author = {{admin}},
  urldate = {2022-05-20},
  howpublished = {https://www.thegeekdiary.com/centos-rhel-7-chrony-vs-ntp-differences-between-ntpd-and-chronyd/},
  file = {C:\Users\emilm\Zotero\storage\84AKLFH5\centos-rhel-7-chrony-vs-ntp-differences-between-ntpd-and-chronyd.html}
}

@misc{agfAnswerCreatingSingleton2011,
  title = {Answer to "{{Creating}} a Singleton in {{Python}}"},
  author = {{agf}},
  year = {2011},
  month = jul,
  journal = {Stack Overflow},
  urldate = {2023-04-27},
  file = {C:\Users\emilm\Zotero\storage\CQ5QXFJH\creating-a-singleton-in-python.html}
}

@misc{AGXXavier35,
  title = {{{AGX Xavier}} 35.1.0 Enable Pps Process Full Record - {{Programmer Sought}}},
  urldate = {2023-05-04},
  howpublished = {https://blog.csdn.net/whr19970424/article/details/129824969},
  file = {C:\Users\emilm\Zotero\storage\5QCDRPB4\129824969.html}
}

@misc{AGXXavierPPS2023,
  title = {{{AGX Xavier PPS}} Fetch Timeout {{R35}}.2.1({{R32}}.7.1 Works Fine)},
  year = {2023},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-06},
  abstract = {Hi NV\_Team,  AGX Xavier using L4T-R32.7.1 works fine with pps, but L4T-R35.2.1 can't get pps (timeout), please help to check R35.2.1need patch ?  dtsi  	pps \{ 		// here use gpio for the pin in which you want pps signal. SYNC\_IN 		gpios = {$<\&$}tegra\_main\_gpio TEGRA194\_MAIN\_GPIO(A, 0) GPIO\_ACTIVE\_LOW{$>$};  		compatible = "pps-gpio"; 		assert-falling-edge; 		status = "okay"; 	\};  pps-gpio.c  diff --git a/l4t-r35.2.1-jp5.1ga/kernel/kernel-5.10/drivers/pps/clients/pps-gpio.c b/l4t-r35.2.1-jp5.1ga/kernel/ke...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/agx-xavier-pps-fetch-timeout-r35-2-1-r32-7-1-works-fine/252374},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\FNQI4BWZ\252374.html}
}

@misc{ajitsariaWhatPythonGlobal2018,
  title = {What {{Is}} the {{Python Global Interpreter Lock}} ({{GIL}})? {\textendash} {{Real Python}}},
  shorttitle = {What {{Is}} the {{Python Global Interpreter Lock}} ({{GIL}})?},
  author = {Ajitsaria, Abhinav},
  year = {2018},
  month = mar,
  urldate = {2023-06-26},
  abstract = {Python's Global Interpreter Lock or GIL, in simple words, is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter at any one time. In this article you'll learn how the GIL affects the performance of your Python programs.},
  howpublished = {https://realpython.com/python-gil/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\C6UTHZUB\python-gil.html}
}

@misc{alexisAnswerCreatingIsolated2022,
  title = {Answer to "{{Creating}} an Isolated {{NTP}} Server with Only {{1PPS}} and No External "Time" Input ({{No GPS}})"},
  author = {Alexis},
  year = {2022},
  month = jan,
  journal = {Unix \& Linux Stack Exchange},
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\XUPDPTL9\688159.html}
}

@misc{all3dpBasicsArchives,
  title = {Basics {{Archives}}},
  author = {All3DP},
  journal = {All3DP},
  urldate = {2023-06-16},
  abstract = {Want to learn the basics, or something new? Check out our helpful tutorials, guides, and how-tos for 3D printing, CAD, CNC, laser cutting, and more.},
  howpublished = {https://all3dp.com/basics/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\NJ9E4IIU\basics.html}
}

@misc{alperenRtspclientsinkTestPipeline2021,
  type = {Forum Post},
  title = {Rtspclientsink Test Pipeline from Command Line},
  author = {Alperen},
  year = {2021},
  month = sep,
  journal = {Stack Overflow},
  urldate = {2023-04-10},
  file = {C:\Users\emilm\Zotero\storage\BY78RLDE\rtspclientsink-test-pipeline-from-command-line.html}
}

@techreport{alphawireAlphaWireUnterminated2013,
  title = {Alpha {{Wire Unterminated}} to {{Unterminated Coaxial Cable}}, {{RG174}}/{{U}}, 50 {\textbackslash}{{Omega}}, 30m},
  author = {Alpha Wire},
  year = {2013},
  urldate = {2022-05-26},
  file = {C:\Users\emilm\Zotero\storage\UFI59PTE\TOP106_GNSS_Antenna.pdf}
}

@misc{amphenolrfT1121A1ND3G150AmphenolRF,
  title = {{{T1121A1-ND3G-1-50}} | {{Amphenol RF}}, {{TNC Connector}} | {{RS Components}}},
  author = {Amphenol RF},
  urldate = {2022-05-26},
  howpublished = {https://no.rs-online.com/web/p/coaxial-connectors/1943235},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QT6ZP6RZ\\0900766b8139013d.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3SDRUQRY\\1943235.html}
}

@article{andreffRobotHandEyeCalibration2001,
  title = {Robot {{Hand-Eye Calibration Using Structure-from-Motion}}},
  author = {Andreff, Nicolas and Horaud, Radu and Espiau, Bernard},
  year = {2001},
  month = mar,
  journal = {The International Journal of Robotics Research},
  volume = {20},
  number = {3},
  pages = {228--248},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783640122067372},
  urldate = {2022-11-26},
  abstract = {In this paper, we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques require a calibration rig that is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions, and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows investigation of not only the well-known case of general screw motions but also of such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence, and estimates the camera-to-robot relationship. Such a self-calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments that validate the quality of the method by comparing it with existing ones.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\LGLHIY46\Andreff et al. - 2001 - Robot Hand-Eye Calibration Using Structure-from-Mo.pdf}
}

@misc{annieahujaweb2020LinkLocalAddress2022,
  title = {Link {{Local Address}}},
  author = {{annieahujaweb2020}},
  year = {2022},
  month = jun,
  journal = {GeeksforGeeks},
  urldate = {2023-05-02},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {CCNA},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\Y8AS97AW\link-local-address.html}
}

@misc{armWhatHeterogenousCompute,
  title = {What Is Heterogenous Compute?},
  author = {Arm},
  journal = {Arm | The Architecture for the Digital World},
  urldate = {2023-05-11},
  abstract = {Arm glossary page for heterogenous compute, typically referring to a system that uses multiple types of computing cores, like CPUs, GPUs, ASICs, FPGAs, and NPUs},
  howpublished = {https://www.arm.com/glossary/heterogenous-compute},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\EQH3K6EQ\heterogenous-compute.html}
}

@article{arnabConditionalRandomFields2018a,
  title = {Conditional {{Random Fields Meet Deep Neural Networks}} for {{Semantic Segmentation}}},
  author = {Arnab, Anurag and Zheng, Shuai and Jayasumana, Sadeep and {Romera-Paredes}, Bernardino and Kirillov, Alexander and Savchynskyy, Bogdan and Rother, Carsten and Kahl, Fredrik and Torr, Philip},
  year = {2018},
  journal = {IEEE SIGNAL PROCESSING MAGAZINE},
  abstract = {Semantic Segmentation is the task of labelling every pixel in an image with a pre-defined object category. It has numerous applications in scenarios where the detailed understanding of an image is required, such as in autonomous vehicles and medical diagnosis. This problem has traditionally been solved with probabilistic models known as Conditional Random Fields (CRFs) due to their ability to model the relationships between the pixels being predicted. However, Deep Neural Networks (DNNs) have recently been shown to excel at a wide range of computer vision problems due to their ability to learn rich feature representations automatically from data, as opposed to traditional hand-crafted features. The idea of combining CRFs and DNNs have achieved state-of-the-art results in a number of domains. We review the literature on combining the modelling power of CRFs with the representation-learning ability of DNNs, ranging from early work that combines these two techniques as independent stages of a common pipeline to recent approaches that embed inference of probabilistic models directly in the neural network itself. Finally, we summarise future research directions.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\CQ8BSZZ2\Arnab et al. - 2018 - Conditional Random Fields Meet Deep Neural Network.pdf}
}

@misc{artezioFinerAdjustmentsOffset2021,
  title = {Finer Adjustments to z Offset?},
  author = {Artezio},
  year = {2021},
  month = may,
  journal = {Snapmaker: where creation happens},
  urldate = {2023-06-03},
  abstract = {Is there a way to do finer adjustments of the z offset? I'm having issues with first layers because the nozzle is just a tad too close, but if I raise it up .05 it's too high. My other printers let me adjust by .01. I'm more familiar with GRBL than I am with Marlin, I am aware that Marlin is a very heavily modified GRBL but my experience lies in CNC and not 3D printing. Any help is appreciated.},
  chapter = {Snapmaker 2.0},
  howpublished = {https://forum.snapmaker.com/t/finer-adjustments-to-z-offset/18844},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\Y3TFXUVE\18844.html}
}

@misc{aryavoronovaRP20405VLogic2023,
  title = {{{RP2040 And 5V Logic}} {\textendash} {{Best Friends}}? {{This FX9000P Confirms}}!},
  shorttitle = {{{RP2040 And 5V Logic}} {\textendash} {{Best Friends}}?},
  author = {Arya Voronova},
  year = {2023},
  month = apr,
  journal = {Hackaday},
  urldate = {2023-06-10},
  abstract = {Over the years, we've seen some modern microcontrollers turn out to be 5V-tolerant {\textendash} now, RP2040 joins the crowd. Half a year ago, when we covered an ISA card based on a Pi Pico, [Eben {\ldots}},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\X4HZT8TL\rp2040-and-5v-logic-best-friends-this-fx9000p-confirms.html}
}

@misc{ASGIDocumentationASGI,
  title = {{{ASGI Documentation}} {\textemdash} {{ASGI}} 3.0 Documentation},
  urldate = {2023-05-15},
  howpublished = {https://asgi.readthedocs.io/en/latest/},
  file = {C:\Users\emilm\Zotero\storage\G8SV3E53\latest.html}
}

@misc{aTmpfsArchWiki,
  title = {Tmpfs - {{ArchWiki}}},
  author = {A},
  urldate = {2023-05-26},
  howpublished = {https://wiki.archlinux.org/title/tmpfs},
  file = {C:\Users\emilm\Zotero\storage\MRHNE9UL\tmpfs.html}
}

@article{attaranRise3DPrinting2017,
  title = {The Rise of 3-{{D}} Printing: {{The}} Advantages of Additive Manufacturing over Traditional Manufacturing},
  shorttitle = {The Rise of 3-{{D}} Printing},
  author = {Attaran, Mohsen},
  year = {2017},
  month = sep,
  journal = {Business Horizons},
  volume = {60},
  number = {5},
  pages = {677--688},
  issn = {0007-6813},
  doi = {10.1016/j.bushor.2017.05.011},
  urldate = {2022-05-18},
  abstract = {The use of additive manufacturing technologies in different industries has increased substantially during the past years. Henry Ford introduced the moving assembly line that enabled mass production of identical products in the 20th century. Currently, additive manufacturing enables and facilitates production of moderate to mass quantities of products that can be customized individually. Additive manufacturing technologies are opening new opportunities in terms of production paradigm and manufacturing possibilities. Manufacturing lead times will be reduced substantially, new designs will have shorter time to market, and customer demand will be met more quickly. This article identifies additive manufacturing implementation challenges, highlights its evolving technologies and trends and their impact on the world of tomorrow, discusses its advantages over traditional manufacturing, explores its impact on the supply chain, and investigates its transformative potential and impact on various industry segments.},
  langid = {english},
  keywords = {3-D printing,Additive manufacturing,Global supply chain,Mass customization,Rapid prototyping},
  file = {C:\Users\emilm\Zotero\storage\GULETRD8\S0007681317300897.html}
}

@misc{AustinVSCode,
  title = {Austin {{VS Code}} - {{Visual Studio Marketplace}}},
  urldate = {2023-06-26},
  abstract = {Extension for Visual Studio Code - Austin extension for VS Code},
  howpublished = {https://marketplace.visualstudio.com/items?itemName=p403n1x87.austin-vscode},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\NZWEBD7G\items.html}
}

@misc{autodeskFusion360Logo,
  title = {Fusion 360 {{Logo}}},
  author = {Autodesk}
}

@misc{AutonomousVehicleVisualization,
  title = {Autonomous {{Vehicle Visualization}}},
  urldate = {2022-05-30},
  howpublished = {https://dash.gallery/dash-avs-explorer/},
  file = {C:\Users\emilm\Zotero\storage\GLEKD4EB\dash-avs-explorer.html}
}

@misc{aWau[@aman_gif]HttpsCoNJXG1aYuVb2023,
  type = {Tweet},
  title = {{{https://t.co/NJXG1aYuVb}}},
  author = {{👁‍ {$\Elztrna\Elztrnm\Elztrna$}u 👁‍ [@aman\_gif]}},
  year = {2023},
  month = sep,
  journal = {Twitter},
  urldate = {2023-10-25},
  file = {C:\Users\emilm\Zotero\storage\N5EB9BLM\1699928701478097011.html}
}

@misc{babukrCreatingGstBuffersUsing2021,
  title = {Creating {{GstBuffers}} Using {{NvBufSurfaceCreate}} in App Source},
  author = {Babukr, Abdo},
  year = {2021},
  month = nov,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-06-12},
  abstract = {Hardware Platform (Jetson / GPU): Quadro RTX4000  DeepStream Version: Deepstream 6.0  TensorRT Version: TensorRT 8.0.1  NVIDIA GPU Driver Version: 470.63.01  An example is shown here on how to create NVMM buffers on the jetson using NvBufferCreate():  https://forums.developer.nvidia.com/uploads/short-url/7Jw7yIt3zu4n6vIYXmCKTa2LomX.zip  How would we implement this on a discrete GPU using NvBufSurfaceCreate()? gst\_nvds\_buffer\_pool\_new() can create a pool of buffers, however it is causes jitter is...},
  chapter = {Accelerated Computing},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\YJBT2REF\194800.html}
}

@misc{bakerDSSIMStructuralSimilarity2023,
  title = {{{DSSIM}}: A Structural Similarity Index for Floating-Point Data},
  shorttitle = {{{DSSIM}}},
  author = {Baker, Allison H. and Pinard, Alexander and Hammerling, Dorit M.},
  year = {2023},
  month = mar,
  number = {arXiv:2202.02616},
  eprint = {2202.02616},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.02616},
  urldate = {2023-11-22},
  abstract = {Data visualization is a critical component in terms of interacting with floating-point output data from large model simulation codes. Indeed, postprocessing analysis workflows on simulation data often generate a large number of images from the raw data, many of which are then compared to each other or to specified reference images. In this image-comparison scenario, image quality assessment (IQA) measures are quite useful, and the Structural Similarity Index (SSIM) continues to be a popular choice. However, generating large numbers of images can be costly, and plot-specific (but data independent) choices can affect the SSIM value. A natural question is whether we can apply the SSIM directly to the floating-point simulation data and obtain an indication of whether differences in the data are likely to impact a visual assessment, effectively bypassing the creation of a specific set of images from the data. To this end, we propose an alternative to the popular SSIM that can be applied directly to the floating point data, which we refer to as the Data SSIM (DSSIM). While we demonstrate the usefulness of the DSSIM in the context of evaluating differences due to lossy compression on large volumes of simulation data from a popular climate model, the DSSIM may prove useful for many other applications involving simulation or image data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Computation},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\WN5E3G5Z\\Baker et al_2023_DSSIM.pdf;C\:\\Users\\emilm\\Zotero\\storage\\8PLRDCD6\\2202.html}
}

@misc{baranYUVFormats2018,
  title = {About {{YUV}} Formats},
  author = {Baran, Jean-Marie},
  year = {2018},
  month = may
}

@misc{barathMAGSACMarginalizingSample2019,
  title = {{{MAGSAC}}: Marginalizing Sample Consensus},
  shorttitle = {{{MAGSAC}}},
  author = {Barath, Daniel and Noskova, Jana and Matas, Jiri},
  year = {2019},
  month = jun,
  number = {arXiv:1803.07469},
  eprint = {1803.07469},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.07469},
  urldate = {2022-11-27},
  abstract = {A method called, sigma-consensus, is proposed to eliminate the need for a user-defined inlier-outlier threshold in RANSAC. Instead of estimating the noise sigma, it is marginalized over a range of noise scales. The optimized model is obtained by weighted least-squares fitting where the weights come from the marginalization over sigma of the point likelihoods of being inliers. A new quality function is proposed not requiring sigma and, thus, a set of inliers to determine the model quality. Also, a new termination criterion for RANSAC is built on the proposed marginalization approach. Applying sigma-consensus, MAGSAC is proposed with no need for a user-defined sigma and improving the accuracy of robust estimation significantly. It is superior to the state-of-the-art in terms of geometric accuracy on publicly available real-world datasets for epipolar geometry (F and E) and homography estimation. In addition, applying sigma-consensus only once as a post-processing step to the RANSAC output always improved the model quality on a wide range of vision problems without noticeable deterioration in processing time, adding a few milliseconds. The source code is at https://github.com/danini/magsac.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5MI52UHM\\Barath et al. - 2019 - MAGSAC marginalizing sample consensus.pdf;C\:\\Users\\emilm\\Zotero\\storage\\IVVW6BPQ\\1803.html}
}

@article{barfootAssociatingUncertaintyThreeDimensional2014,
  title = {Associating {{Uncertainty With Three-Dimensional Poses}} for {{Use}} in {{Estimation Problems}}},
  author = {Barfoot, Timothy D. and Furgale, Paul T.},
  year = {2014},
  month = jun,
  journal = {IEEE Transactions on Robotics},
  volume = {30},
  number = {3},
  pages = {679--693},
  issn = {1941-0468},
  doi = {10.1109/TRO.2014.2298059},
  abstract = {In this paper, we provide specific and practical approaches to associate uncertainty with 4 {\texttimes}4 transformation matrices, which is a common representation for pose variables in 3-D space. We show constraint-sensitive means of perturbing transformation matrices using their associated exponential-map generators and demonstrate these tools on three simple-yet-important estimation problems: 1) propagating uncertainty through a compound pose change, 2) fusing multiple measurements of a pose (e.g., for use in pose-graph relaxation), and 3) propagating uncertainty on poses (and landmarks) through a nonlinear camera model. The contribution of the paper is the presentation of the theoretical tools, which can be applied in the analysis of many problems involving 3-D pose and point variables.},
  keywords = {Compounds,Covariance matrices,Estimation,Exponential maps,homogeneous points,lie,matrix Lie groups,Noise,pose uncertainty,Probability density function,Robots,transformation matrices,Uncertainty},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DECQ3XHB\\Barfoot and Furgale - 2014 - Associating Uncertainty With Three-Dimensional Pos.pdf;C\:\\Users\\emilm\\Zotero\\storage\\W4S24ZW9\\6727494.html}
}

@misc{bastian35022AnswerParsingHevc2014,
  title = {Answer to "{{Parsing}} Hevc Bitstream"},
  author = {Bastian35022},
  year = {2014},
  month = jul,
  journal = {Stack Overflow},
  urldate = {2023-01-25},
  file = {C:\Users\emilm\Zotero\storage\QAEY3ZS2\parsing-hevc-bitstream.html}
}

@inproceedings{baySURFSpeededRobust2006,
  title = {{{SURF}}: {{Speeded Up Robust Features}}},
  shorttitle = {{{SURF}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2006},
  author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {404--417},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11744023_32},
  abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.},
  isbn = {978-3-540-33833-8},
  langid = {english},
  keywords = {Hessian Matrix,Integral Image,Interest Point,Robust Feature,Viewpoint Change},
  file = {C:\Users\emilm\Zotero\storage\HCDW6TJI\Bay et al. - 2006 - SURF Speeded Up Robust Features.pdf}
}

@misc{beneaterHowCRCsWork2019,
  title = {How Do {{CRCs}} Work?},
  author = {{Ben Eater}},
  year = {2019},
  month = apr,
  urldate = {2022-05-27},
  abstract = {CRC (cyclic redundancy check) is one of the most common methods of error detection. It uses some interesting mathematical tricks to guarantee that it can catch certain kinds of errors. How does it work? Support these videos on Patreon: https://www.patreon.com/beneater or https://eater.net/support for other ways to support. ------------------ Social media: Website: https://www.eater.net Twitter: https://twitter.com/ben\_eater Patreon: https://patreon.com/beneater Reddit: https://www.reddit.com/r/beneater Special thanks to these supporters for making this video possible: Ben Dyson Ben Kamens Ben Williams Brandon Stranzl Christopher Blackmon Debilu Krastas Eric Dynowski Gonzalo Belascuen Greg Stratton Jay Binks Jayne Gabriele Johnathan Roatch Jordan Scales Manne Moquist Michael Nicholas Moresco Nick Wrightsman Randy True Ric Allinson Sachin Chitale SonOfSofaman}
}

@misc{beneaterSPISerialPeripheral2021,
  title = {{{SPI}}: {{The}} Serial Peripheral Interface},
  shorttitle = {{{SPI}}},
  author = {{Ben Eater}},
  year = {2021},
  month = sep,
  urldate = {2022-05-26},
  abstract = {More 6502 computer info: https://eater.net/6502 Here's the temperature sensor module used in this video: https://amzn.to/2Wye3Ex More info on the sensor: https://tiny.cc/bme280 Support these videos on Patreon: https://www.patreon.com/beneater or https://eater.net/support for other ways to support.    ------------------    Social media:  Website: https://www.eater.net  Twitter: https://twitter.com/ben\_eater  Patreon: https://patreon.com/beneater  Reddit: https://www.reddit.com/r/beneater    Special thanks to these supporters for making this video possible: Aleksey Smolenchuk, Anders Carlsson, Andrew C. Young, Anson VanDoren, Anthanasius, anula, Armin Brauns, Ben, Ben Cochran, Ben Kamens, Ben Williams, Benny Olsson, Bill Cooksey, Binh Tran, Bouke Groenescheij, Bradley Pirtle, Bradley Stach, Brian T Hoover, Bryan Brickman, Burt Humburg, Carlos Ambrozak, Chris, Christian Carter, Christopher Blackmon, Dale Andrew Darling, Daniel Jeppsson, Daniel Tang, Dave Burley, Dave Walter, David Brown, David Clark, David Cox, David Dawkins, David House, David Sastre Medina, David Turner, David Worsham, Dean Bevan, Dean Winger, Dilip Gowda, Dissy, dko, Dmitry Guyvoronsky, Du{\v s}an D{\v z}elebd{\v z}i{\'c}, Dzevad Trumic, Emilio Mendoza, Eric Dynowski, Erik Broeders, Eugene Bulkin, Evan Thayer, Eveli L{\'a}szl{\'o}, George Miroshnykov, Gonzalo Diaz, Harry McDow, hotwire33, Ingo Eble, Ivan Sorokin, James Capuder, james schaefer, Jared Dziedzic, Jason DeStefano, Jason Specland, JavaXP, Jaxon Ketterman, Jay Binks, Jayne Gabriele, Jeremy, Jeremy Cole, Jesse Miller, Jim Kelly, Jim Knowler, Jim Van Meggelen, Joe Beda, Joe OConnor, Joe Pregracke, Joel Miller, John Fenwick, John Hamberger jn., John Meade, Jon Dugan, Joseph Portaro, Joshua King, Jur{\c g}is Brigmanis, Kai Wells, Kefen, Kenneth Christensen, Kitick, Koreo, Lambda GPU Workstations, Larry, L{\'a}szl{\'o} B{\'a}csi, Lucky Resistor, Lukasz Pacholik, Marcos Fujisawa, Marcus Classon, Mark Day, Marko Clemente, Martin Noble, Martin Roth, Mats Fredriksson, Matt Krueger, Matth{\"a}us Pawelczyk, Matthew Duphily, Max Gawletta, Maxim Hansen, melvin2001, Michael Tedder, Michael Timbrook, Michael Weitman, Miguel R{\'i}os, mikebad, Mikel Lindsaar, Miles Macchiaroli, Muqeet Mujahid, My Yiddishe Mama, Nicholas Counts, Nicholas Moresco, Not Yet Wise, {\"O}rn Arnarson, Paul Pluzhnikov, Paul Randal, Pete Dietl, Phil Dennis, Philip Hofstetter, Phillip Glau, PixelSergey, Porus, ProgrammerDor, Randal Masutani, Randy True, raoulvp, Renaldas Zioma, Ric King, Richard Ertel, Rick Hennigan, Robert Comyn, Robert Diaz, Robey Pointer, Roland Bobek,{\textsection}{\c c}{\=i}{\v T}{\o}{\c s}H{\"i} {\v N}{\aa}{\c K}{\k{a}}{\'y}{\H O}b{\'r}{\^O},  Scott Holmes, Sean Patrick O'Brien, Sergey Kruk, Shelton, SonOfSofaman, Stefan Nesinger, Stefanus Du Toit, Stephen, Stephen Kovalcik, Stephen Riley, Stephen Smithstone, Steve  Jones, Tayler Porter, TheWebMachine, Thomas Bruggink, Thomas Eriksen, Tim Walkowski, Tom, Tom Yedwab, Tommaso Palmieri, Tyler Latham, Vincent Bernat, Walter Montalvo, Warren Miller, William, Wim Coekaerts, Wraithan McCarroll, xisente, Yee Lam Wan}
}

@misc{biltemaAkrylplastOpalhvit,
  title = {{Akrylplast, opalhvit}},
  author = {{biltema}},
  urldate = {2022-05-18},
  abstract = {Akrylplast, opalhvit},
  howpublished = {https://www.biltema.no/bygg/platematerialer/akryplast/akrylplast-opalhvit-2000043756},
  langid = {norwegianbokmal},
  keywords = {material},
  file = {C:\Users\emilm\Zotero\storage\JJ45DUBK\akrylplast-opalhvit-2000043756.html}
}

@misc{birchardIntegratePlotlyDash2018,
  title = {Integrate {{Plotly Dash Into Your Flask App}}},
  author = {Birchard, Todd},
  year = {2018},
  month = dec,
  journal = {Hackers and Slackers},
  urldate = {2022-05-30},
  abstract = {Use a clever workaround to embed interactive Plotly Dash interfaces into your Flask applications.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\6FCFVQB3\plotly-dash-with-flask.html}
}

@misc{blackNVIDIACelebratesMillion2023,
  title = {{{NVIDIA Celebrates}} 1 {{Million Jetson Developers Worldwide}} at {{GTC}}},
  author = {Black, Jason},
  year = {2023},
  month = feb,
  journal = {NVIDIA Blog},
  urldate = {2023-05-22},
  abstract = {Register free for GTC to learn more about the NVIDIA Jetson platform and begin developing the next generation of edge AI and robotics.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\DPCN3JGK\million-jetson-developers-gtc.html}
}

@misc{bonghiJtopRbonghiJetson,
  title = {Jtop {$\cdot$} Rbonghi/Jetson\_stats {{Wiki}}},
  author = {Bonghi, Raffaello},
  urldate = {2022-05-20},
  abstract = {📊 Simple package for monitoring and control your NVIDIA Jetson [Xavier NX, Nano, AGX Xavier, TX1, TX2] - jtop {$\cdot$} rbonghi/jetson\_stats Wiki}
}

@misc{booplaEnclosure,
  title = {Enclosure},
  author = {BOOPLA},
  journal = {BOPLA},
  urldate = {2022-05-17},
  abstract = {Here you will find information about the BOPLA product range!},
  howpublished = {https://www.bopla.de/en/enclosure-technology/product/bocube/pc-ul-94-v0-crystal-clear-lid/b-261712-pc-v0-g-7024.html},
  langid = {english},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\XTV59JPI\b-261712-pc-v0-g-7024.html}
}

@misc{BootExternalDrive2021,
  title = {Boot from External Drive},
  year = {2021},
  month = jul,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-11},
  abstract = {Hello,  I understand that there are lots of people asking about how to boot from external drive (e.g. usb/NVMe drive).  And also lots of methods all around this forum.   Thanks for everyone to share their steps to achieve that.  This post is just to give some basic concepts about how jetson platform boots from external drive. And answer some frequently-asked questions.  I believe every method shared on this forum uses the same idea but just in different forms.  1. Jetson is not able to ``fully'' b...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/boot-from-external-drive/182883},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\NKZYC5D8\182883.html}
}

@misc{boplaBocubeEnclosure,
  title = {Bocube Enclosure},
  author = {BOPLA},
  journal = {BOPLA},
  urldate = {2022-05-18},
  abstract = {Here you will find information about the BOPLA product range!},
  howpublished = {https://www.bopla.de/en/enclosure-technology/product/bocube/pc-ul-94-v0-crystal-clear-lid/b-261712-pc-v0-g-7024.html},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\EA43NXDU\b-261712-pc-v0-g-7024.html}
}

@misc{brachmannLimitsPseudoGround2021,
  title = {On the {{Limits}} of {{Pseudo Ground Truth}} in {{Visual Camera Re-localisation}}},
  author = {Brachmann, Eric and Humenberger, Martin and Rother, Carsten and Sattler, Torsten},
  year = {2021},
  month = sep,
  number = {arXiv:2109.00524},
  eprint = {2109.00524},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.00524},
  urldate = {2022-11-27},
  abstract = {Benchmark datasets that measure camera pose accuracy have driven progress in visual re-localisation research. To obtain poses for thousands of images, it is common to use a reference algorithm to generate pseudo ground truth. Popular choices include Structure-from-Motion (SfM) and Simultaneous-Localisation-and-Mapping (SLAM) using additional sensors like depth cameras if available. Re-localisation benchmarks thus measure how well each method replicates the results of the reference algorithm. This begs the question whether the choice of the reference algorithm favours a certain family of re-localisation methods. This paper analyzes two widely used re-localisation datasets and shows that evaluation outcomes indeed vary with the choice of the reference algorithm. We thus question common beliefs in the re-localisation literature, namely that learning-based scene coordinate regression outperforms classical feature-based methods, and that RGB-D-based methods outperform RGB-based methods. We argue that any claims on ranking re-localisation methods should take the type of the reference algorithm, and the similarity of the methods to the reference algorithm, into account.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\W39ICD63\\Brachmann et al. - 2021 - On the Limits of Pseudo Ground Truth in Visual Cam.pdf;C\:\\Users\\emilm\\Zotero\\storage\\QCXT4DWM\\2109.html}
}

@misc{brachmannVisualCameraReLocalization2020,
  title = {Visual {{Camera Re-Localization}} from {{RGB}} and {{RGB-D Images Using DSAC}}},
  author = {Brachmann, Eric and Rother, Carsten},
  year = {2020},
  month = oct,
  number = {arXiv:2002.12324},
  eprint = {2002.12324},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-27},
  abstract = {We describe a learning-based system that estimates the camera position and orientation from a single input image relative to a known environment. The system is flexible w.r.t. the amount of information available at test and at training time, catering to different applications. Input images can be RGB-D or RGB, and a 3D model of the environment can be utilized for training but is not necessary. In the minimal case, our system requires only RGB images and ground truth poses at training time, and it requires only a single RGB image at test time. The framework consists of a deep neural network and fully differentiable pose optimization. The neural network predicts so called scene coordinates, i.e. dense correspondences between the input image and 3D scene space of the environment. The pose optimization implements robust fitting of pose parameters using differentiable RANSAC (DSAC) to facilitate end-to-end training. The system, an extension of DSAC++ and referred to as DSAC*, achieves state-of-the-art accuracy an various public datasets for RGB-based re-localization, and competitive accuracy for RGB-D-based re-localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ESU6RZG4\\Brachmann and Rother - 2020 - Visual Camera Re-Localization from RGB and RGB-D I.pdf;C\:\\Users\\emilm\\Zotero\\storage\\I466SH3A\\2002.12324.pdf;C\:\\Users\\emilm\\Zotero\\storage\\XBTGBS6E\\2002.html}
}

@book{brekkeFundamentalsSensorFusion,
  title = {Fundamentals of {{Sensor Fusion}}},
  author = {Brekke, Edmund},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\LR9H7KZN\17790571.pdf}
}

@article{brekkeMilliAmpereAutonomousFerry2022,
  title = {{{milliAmpere}}: {{An Autonomous Ferry Prototype}}},
  shorttitle = {{{milliAmpere}}},
  author = {Brekke, Edmund F and Eide, Egil and Eriksen, Bj{\o}rn-Olav H and Wilthil, Erik F and Breivik, Morten and Skjellaug, Even and Helgesen, {\O}ystein K and Lekkas, Anastasios M. and Martinsen, Andreas B and Thyri, Emil H. and Torben, Tobias and Veitch, Erik and Alsos, Ole A and Johansen, Tor Arne},
  year = {2022},
  month = jul,
  journal = {Journal of Physics: Conference Series},
  volume = {2311},
  number = {1},
  pages = {012029},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/2311/1/012029},
  urldate = {2023-06-26},
  abstract = {In this paper, we summarize the experiences with the autonomous passenger ferry development prototype milliAmpere, which has been used as a test platform in several research projects at the Norwegian University of Science and Technology (NTNU) since 2017. New algorithms for motion planning, motion control, collision avoidance, docking, multi-target tracking and localization have been developed and verified in full-scale experiments with milliAmpere. The infrastructure surrounding milliAmpere includes several sensor rigs supporting research on multi-sensor fusion and situational awareness, and a shore control lab which can be used to study the interaction between human operators and the autonomous ferry. Building upon the experiences with milliAmpere, the full-scale autonomous ferry milliAmpere2 was recently launched.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\JQH87QCC\Brekke et al. - 2022 - milliAmpere An Autonomous Ferry Prototype.pdf}
}

@article{BrewsterAngle2023,
  title = {Brewster's Angle},
  year = {2023},
  month = mar,
  journal = {Wikipedia},
  urldate = {2023-06-22},
  abstract = {Brewster's angle (also known as the polarization angle) is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with no reflection. When unpolarized light is incident at this angle, the light that is reflected from the surface is therefore perfectly polarized. This special angle of incidence is named after the Scottish physicist Sir David Brewster (1781{\textendash}1868).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1146176507},
  file = {C:\Users\emilm\Zotero\storage\HMA5L7BB\Brewster's_angle.html}
}

@article{broekmanLowcostMobileRealtime2021,
  title = {A Low-Cost, Mobile Real-Time Kinematic Geolocation Service for Engineering and Research Applications},
  author = {Broekman, Andr{\'e} and Gr{\"a}be, Petrus Johannes},
  year = {2021},
  month = oct,
  journal = {HardwareX},
  volume = {10},
  pages = {e00203},
  issn = {24680672},
  doi = {10.1016/j.ohx.2021.e00203},
  urldate = {2022-05-21},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\L8MTDAEH\Broekman and Gräbe - 2021 - A low-cost, mobile real-time kinematic geolocation.pdf}
}

@misc{brossardAssociatingUncertaintyExtended2021,
  title = {Associating {{Uncertainty}} to {{Extended Poses}} for on {{Lie Group IMU Preintegration}} with {{Rotating Earth}}},
  author = {Brossard, Martin and Barrau, Axel and Chauchat, Paul and Bonnabel, Silv{\`e}re},
  year = {2021},
  month = jan,
  number = {arXiv:2007.14097},
  eprint = {2007.14097},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-11},
  abstract = {The recently introduced matrix group SE2(3) provides a 5x5 matrix representation for the orientation, velocity and position of an object in the 3-D space, a triplet we call "extended pose". In this paper we build on this group to develop a theory to associate uncertainty with extended poses represented by 5x5 matrices. Our approach is particularly suited to describe how uncertainty propagates when the extended pose represents the state of an Inertial Measurement Unit (IMU). In particular it allows revisiting the theory of IMU preintegration on manifold and reaching a further theoretic level in this field. Exact preintegration formulas that account for rotating Earth, that is, centrifugal force and Coriolis force, are derived as a byproduct, and the factors are shown to be more accurate. The approach is validated through extensive simulations and applied to sensor-fusion where a loosely-coupled fixed-lag smoother fuses IMU and LiDAR on one hour long experiments using our experimental car. It shows how handling rotating Earth may be beneficial for long-term navigation within incremental smoothing algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BV27FPQ8\\Brossard et al. - 2021 - Associating Uncertainty to Extended Poses for on L.pdf;C\:\\Users\\emilm\\Zotero\\storage\\TQIEYN8E\\2007.html}
}

@article{camposORBSLAM3AccurateOpenSource2021,
  title = {{{ORB-SLAM3}}: {{An Accurate Open-Source Library}} for {{Visual}}, {{Visual}}{\textendash}{{Inertial}}, and {{Multimap SLAM}}},
  shorttitle = {{{ORB-SLAM3}}},
  author = {Campos, Carlos and Elvira, Richard and Rodriguez, Juan J. Gomez and M. Montiel, Jose M. and D. Tardos, Juan},
  year = {2021},
  month = dec,
  journal = {IEEE Transactions on Robotics},
  volume = {37},
  number = {6},
  pages = {1874--1890},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2021.3075644},
  urldate = {2022-11-16},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\J99ZCBC4\Campos et al. - 2021 - ORB-SLAM3 An Accurate Open-Source Library for Vis.pdf}
}

@misc{CanEnablePPS2023,
  title = {Can't Enable {{PPS}} on {{Jetson AGX Xavier}}},
  year = {2023},
  month = feb,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-04},
  abstract = {Hi everyone,  I'm trying to enable PPS on jetson AGX Xavier on the pin 24 from the header pins. I have a ublox SIMPLERTK2B gps connected with usb, timepulse pin connected to pin 24 and grounds are also connected. Also the jetson runs Jetpack 5.1 and the l4t driver I've downloaded is Jetson Linux 35.2.1  I have followed this guide to build the custom kernel. The changes I've made to the kernel are based on this and this.  More specifically I have made these changes to the kernel config  \# PPS sup...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/cant-enable-pps-on-jetson-agx-xavier/243842},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\CYFELSF6\243842.html}
}

@misc{canonicalltdUbuntuManpageEtc2019,
  title = {Ubuntu {{Manpage}}: /Etc/Modules - Kernel Modules to Load at Boot Time},
  author = {Canonical Ltd},
  year = {2019},
  urldate = {2022-05-20},
  howpublished = {http://manpages.ubuntu.com/manpages/bionic/man5/modules.5.html},
  file = {C:\Users\emilm\Zotero\storage\XL7E4M25\modules.5.html}
}

@misc{caudlePythonSpidev2022,
  title = {Python {{Spidev}}},
  author = {Caudle, Stephen},
  year = {2022},
  month = may,
  urldate = {2022-05-27},
  copyright = {MIT}
}

@misc{caulfieldWhatPathTracing2022,
  title = {What {{Is Path Tracing}}?},
  author = {Caulfield, Brian},
  year = {2022},
  month = mar,
  journal = {NVIDIA Blog},
  urldate = {2023-12-04},
  abstract = {Turn on your TV. Fire up your favorite streaming service. Grab a Coke. A demo of the most important visual technology of our time is as close as your living room couch. Propelled by an explosion in computing power over the past decade and a half, path tracing has swept through visual media. It brings Read article {$>$}},
  howpublished = {https://blogs.nvidia.com/blog/what-is-path-tracing/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\RMFB6DK4\what-is-path-tracing.html}
}

@misc{ccittINFORMATIONTECHNOLOGYDIGITAL1992,
  title = {{{INFORMATION TECHNOLOGY DIGITAL COMPRESSION AND CODING OF CONTINUOUS-TONE STILL IMAGES REQUIREMENTS AND GUIDELINES}}},
  author = {CCITT},
  year = {1992},
  month = sep,
  urldate = {2023-05-25},
  file = {C:\Users\emilm\Zotero\storage\FRCTN6NN\itu-t81.pdf}
}

@misc{Chapter18Configuring,
  title = {Chapter 18. {{Configuring NTP Using}} the Chrony {{Suite Red Hat Enterprise Linux}} 7 | {{Red Hat Customer Portal}}},
  urldate = {2023-05-18},
  howpublished = {https://access.redhat.com/documentation/en-us/red\_hat\_enterprise\_linux/7/html/system\_administrators\_guide/ch-configuring\_ntp\_using\_the\_chrony\_suite},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\TBFAFJ88\ch-configuring_ntp_using_the_chrony_suite.html}
}

@misc{chili555AnswerAutomaticallyConnect2014,
  title = {Answer to "{{Automatically}} Connect to a Wireless Network Using {{CLI}}"},
  author = {{chili555}},
  year = {2014},
  month = jan,
  journal = {Ask Ubuntu},
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\IL3THPE9\412394.html}
}

@article{ChromaSubsampling2023,
  title = {Chroma Subsampling},
  year = {2023},
  month = may,
  journal = {Wikipedia},
  urldate = {2023-06-14},
  abstract = {Chroma subsampling is the practice of encoding images by implementing less resolution for chroma information than for luma information, taking advantage of the human visual system's lower acuity for color differences than for luminance.It is used in many video and still image encoding schemes {\textendash} both analog and digital {\textendash} including in JPEG encoding.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1157636594},
  file = {C:\Users\emilm\Zotero\storage\ZH98S3QW\Chroma_subsampling.html}
}

@misc{ChronyIntroduction2021,
  title = {Chrony {\textendash} {{Introduction}}},
  year = {2021},
  month = dec,
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\926GA7U6\chrony.tuxfamily.org.html}
}

@inproceedings{chumLocallyOptimizedRANSAC2003,
  title = {Locally {{Optimized RANSAC}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Chum, Ond{\v r}ej and Matas, Ji{\v r}{\'i} and Kittler, Josef},
  editor = {Michaelis, Bernd and Krell, Gerald},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {236--243},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45243-0_31},
  abstract = {A new enhancement of ransac, the locally optimized ransac (lo-ransac), is introduced. It has been observed that, to find an optimal solution (with a given probability), the number of samples drawn in ransac is significantly higher than predicted from the mathematical model. This is due to the incorrect assumption, that a model with parameters computed from an outlier-free sample is consistent with all inliers. The assumption rarely holds in practice. The locally optimized ransac makes no new assumptions about the data, on the contrary {\textendash} it makes the above-mentioned assumption valid by applying local optimization to the solution estimated from the random sample.},
  isbn = {978-3-540-45243-0},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\2LK8LXWI\Chum et al. - 2003 - Locally Optimized RANSAC.pdf}
}

@book{chungAdaptiveColorFilter2006,
  title = {An {{Adaptive Color Filter Array Interpolation Algorithm}} for {{Digital Camera}}},
  author = {Chung, King-Hong and Chan, Yuk-Hee},
  year = {2006},
  month = nov,
  pages = {2700},
  doi = {10.1109/ICIP.2006.313071},
  abstract = {In this paper, an adaptive color filter array (CFA) interpolation method is presented. By examining the edge levels and the variance of color difference along different edge directions, the missing green samples are first estimated. The missing red and blue samples are then estimated based on the interpolated green plane. This algorithm can effectively preserve the details as well as significantly reduce the color artifacts. As compared with some current state-of-art methods, the proposed algorithm provides outperformed results in terms of both subjective and objective image quality measures},
  file = {C:\Users\emilm\Zotero\storage\VJJBYPDY\Chung and Chan - 2006 - An Adaptive Color Filter Array Interpolation Algor.pdf}
}

@misc{CommonsBayerPattern2020,
  title = {File:{{Bayer}} Pattern.Svg {\textemdash} {{Wikimedia Commons}}, the Free Media Repository},
  author = {Commons, Wikimedia},
  year = {2020}
}

@misc{ConstantMemoryGPU,
  title = {Constant {{Memory}} {\textendash} {{GPU Programming}}},
  urldate = {2023-04-24},
  howpublished = {https://carpentries-incubator.github.io/lesson-gpu-programming/08-constant\_memory/index.html},
  file = {C:\Users\emilm\Zotero\storage\AQQNX3TA\index.html}
}

@misc{CoroutinesTasks,
  title = {Coroutines and {{Tasks}}},
  urldate = {2023-05-19},
  abstract = {This section outlines high-level asyncio APIs to work with coroutines and Tasks. Coroutines, Awaitables, Creating Tasks, Task Cancellation, Task Groups, Sleeping, Running Tasks Concurrently, Shield...},
  howpublished = {https://docs.python.org/3/library/asyncio-task.html},
  file = {C:\Users\emilm\Zotero\storage\75UNTFEN\asyncio-task.html}
}

@misc{CorsairMP600PRO,
  title = {{Corsair MP600 PRO NH NVMe M.2 SSD 8TB - SSD M.2}},
  urldate = {2023-04-26},
  abstract = {Corsair MP600 PRO NH NVMe M.2 SSD 8TB - Opptil 7000MB/s Lese / Opptil 6100MB/s Skrive, PCIe Gen 4.0 x4, No Heatsink},
  howpublished = {https://www.komplett.no/product/1221581/datautstyr/lagring/harddiskerssd/ssd-m2/corsair-mp600-pro-nh-nvme-m2-ssd-8tb},
  langid = {norsk},
  file = {C:\Users\emilm\Zotero\storage\CCKRUYUD\corsair-mp600-pro-nh-nvme-m2-ssd-8tb.html}
}

@misc{crostonRPiGPIOModule2022,
  title = {{{RPi}}.{{GPIO}}: {{A}} Module to Control {{Raspberry Pi GPIO}} Channels},
  shorttitle = {{{RPi}}.{{GPIO}}},
  author = {Croston, Ben},
  year = {2022},
  month = feb,
  urldate = {2022-05-27},
  copyright = {MIT License},
  keywords = {GPIO,Home Automation,{Pi,},{Raspberry,},Software Development,System - Hardware},
  file = {C:\Users\emilm\Zotero\storage\A3YQBN9V\RPi.GPIO.html}
}

@misc{crovellaUsingNsightCompute2019,
  title = {Using {{Nsight Compute}} to {{Inspect}} Your {{Kernels}}},
  author = {Crovella, Bob},
  year = {2019},
  month = sep,
  journal = {NVIDIA Technical Blog},
  urldate = {2023-06-17},
  abstract = {By now, hopefully you read the first two blogs in this series "Migrating to NVIDIA Nsight Tools from NVVP and Nvprof" and "Transitioning to Nsight Systems from NVIDIA Visual Profiler / nvprof{\ldots}},
  howpublished = {https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\BYXSCGFQ\using-nsight-compute-to-inspect-your-kernels.html}
}

@article{CUDA2023,
  title = {{{CUDA}}},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-05-09},
  abstract = {CUDA (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.CUDA is designed to work with programming languages such as C, C++, and Fortran. This accessibility makes it easier for specialists in parallel programming to use GPU resources, in contrast to prior APIs like Direct3D and OpenGL, which required advanced skills in graphics programming. CUDA-powered GPUs also support programming frameworks such as OpenMP, OpenACC and OpenCL; and HIP by compiling such code to CUDA. CUDA was created by Nvidia. When it was first introduced, the name was an acronym for Compute Unified Device Architecture, but Nvidia later dropped the common use of the acronym.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1150771145},
  file = {C:\Users\emilm\Zotero\storage\UQZGY43G\CUDA.html}
}

@misc{CUDATegra,
  title = {{{CUDA}} for {{Tegra}}},
  urldate = {2023-04-26},
  howpublished = {https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html\#memory-selection\%5B/url\%5D},
  file = {C:\Users\emilm\Zotero\storage\FVARR3BG\index.html}
}

@misc{cupyInteroperabilityCuPy12,
  title = {Interoperability {\textemdash} {{CuPy}} 12.1.0 Documentation},
  author = {CuPy},
  urldate = {2023-06-21},
  howpublished = {https://docs.cupy.dev/en/stable/user\_guide/interoperability.html},
  file = {C:\Users\emilm\Zotero\storage\INF8L53Z\interoperability.html}
}

@misc{danQueueingLinuxNetwork2013,
  title = {Queueing in the {{Linux Network Stack}} | {{Linux Journal}}},
  author = {Dan, Siemon},
  year = {2013},
  month = sep,
  urldate = {2023-04-27},
  howpublished = {https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/comment-page-2/},
  file = {C:\Users\emilm\Zotero\storage\9B65CA5S\queueing-linux-network-stack.html}
}

@misc{datasciencecastnetGaussianSplattingExplorations2023,
  title = {Gaussian {{Splatting}} Explorations},
  author = {{DataScienceCastnet}},
  year = {2023},
  month = sep,
  urldate = {2023-10-18},
  abstract = {Let's dive into Gaussian Splatting: what is it, how are scenes represented, and what fun things can we do with it? This is a fairly informal and code-heavy video - let me know if you like this format!  GS website (with links to paper): My lesson on optimizing things with fun losses such as CLIP: https://johnowhitaker.github.io/tglco... My Twitter, if you want updates on this as I go: https://twitter.com/johnowhitaker As mentioned in the video, please do let me know if you have any further questions or suggestions :)}
}

@misc{datatilsynetDelingAvBilder2019,
  title = {{Deling av bilder}},
  author = {Datatilsynet},
  year = {2019},
  journal = {Datatilsynet},
  urldate = {2023-09-13},
  abstract = {Publisering eller deling bilder av andre},
  howpublished = {https://www.datatilsynet.no/personvern-pa-ulike-omrader/internett-og-apper/bilder-pa-nett/},
  langid = {norsk},
  file = {C:\Users\emilm\Zotero\storage\RQF5FNUU\bilder-pa-nett.html}
}

@misc{datatilsynetDronerHvaEr2018,
  title = {{Droner - hva er lov?}},
  author = {Datatilsynet},
  year = {2018},
  journal = {Datatilsynet},
  urldate = {2023-09-13},
  abstract = {Bruk av droner blir stadig mer vanlig. De blir brukt av b{\aa}de private og profesjonelle. Dersom dronene har kamera, kan de samle inn personopplysninger. Det er da viktig {\aa} kjenne til regelverket slik at ikke bruken blir ulovlig.},
  howpublished = {https://www.datatilsynet.no/personvern-pa-ulike-omrader/overvaking-og-sporing/droner---hva-er-lov/},
  langid = {norsk},
  file = {C:\Users\emilm\Zotero\storage\XSVCUKH7\virksomheters-bruk-av-droner.html}
}

@misc{davisSnakeViz2023,
  title = {{{SnakeViz}}},
  author = {Davis, Matt},
  year = {2023},
  month = jun,
  urldate = {2023-06-26},
  abstract = {An in-browser Python profile viewer}
}

@misc{DebayeringVPI2021,
  title = {Debayering with {{VPI}}},
  year = {2021},
  month = sep,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-03-23},
  abstract = {I've been digging in to the new VPI API and I've been looking for a way to convert a bayered image (RGGB8) to an RGB8 or BGR8 format. I can't seem to find any bayered image formats in the VPI API.  Is there any example code or any way to debayer/demosaic a raw image with the VPI interface. It seems like most of the conversions available are between RGB/YUV/grayscale. Am I missing something? Or is the VPI API not currently capable of running an accelerated conversion from RGGB8 to RGB8/BGR8?},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/debayering-with-vpi/188730},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\SNBEJ6ES\188730.html}
}

@misc{DecoderErrorBt2022,
  title = {Decoder {{Error}} with {{Bt}}.709 {{H264}}},
  year = {2022},
  month = mar,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-06-11},
  abstract = {There is a problem with the decoder if the fields colour\_primaries, transfer\_characteristics and matrix\_coefficients in the SPS NAL Unit are all set to 1 (Bt.709).  cuGraphicsEGLRegisterImage will fail with CUDA\_ERROR\_INVALID\_VALUE when called on a EGLImageKHR from a capture-plane-buffer dma\_fd.  If the same capture-plane-buffer is queued to the V4L2 Video Converter instead as done in the 02\_video\_dec\_cuda sample, you get following error:  libv4l2\_nvvidconv (0):(1688) (ERROR) : NvDdkVicConfigure...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/decoder-error-with-bt-709-h264/208353},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VX9JMIXK\6.html}
}

@misc{DeepBlendingFreeViewpoint,
  title = {Deep {{Blending}} for {{Free-Viewpoint Image-Based-Rendering}}},
  urldate = {2023-11-22},
  howpublished = {http://visual.cs.ucl.ac.uk/pubs/deepblending/datasets.html},
  file = {C:\Users\emilm\Zotero\storage\42Y3QVRN\datasets.html}
}

@misc{DeepStreamPythonApps2023,
  title = {{{DeepStream Python Apps}}},
  year = {2023},
  month = may,
  urldate = {2023-05-30},
  abstract = {DeepStream SDK Python bindings and sample applications},
  howpublished = {NVIDIA AI IOT}
}

@misc{DeepStreamPythonApps2023a,
  title = {{{DeepStream Python Apps}}},
  year = {2023},
  month = may,
  urldate = {2023-05-30},
  abstract = {DeepStream SDK Python bindings and sample applications},
  howpublished = {NVIDIA AI IOT}
}

@misc{delaragoCuraLogo2022,
  title = {Cura {{Logo}}},
  author = {{de l'Arago}, Joey},
  year = {2022},
  month = feb
}

@misc{detoneSuperPointSelfSupervisedInterest2018,
  title = {{{SuperPoint}}: {{Self-Supervised Interest Point Detection}} and {{Description}}},
  shorttitle = {{{SuperPoint}}},
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2018},
  month = apr,
  number = {arXiv:1712.07629},
  eprint = {1712.07629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.07629},
  urldate = {2023-09-14},
  abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4HSD8FP5\\DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detecti.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AWTTU834\\1712.html}
}

@misc{dickinsTmpfsLinuxKernel2010,
  title = {Tmpfs {\textemdash} {{The Linux Kernel}} Documentation},
  author = {Dickins, Hugh},
  year = {2010},
  month = mar,
  urldate = {2023-05-26},
  howpublished = {https://www.kernel.org/doc/html/latest/filesystems/tmpfs.html},
  file = {C:\Users\emilm\Zotero\storage\39LBEMSM\tmpfs.html}
}

@misc{DifferenceConcurrencyParallelism2019,
  title = {Difference between {{Concurrency}} and {{Parallelism}}},
  year = {2019},
  month = oct,
  journal = {GeeksforGeeks},
  urldate = {2023-05-12},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {Operating Systems},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\75MHFQQC\difference-between-concurrency-and-parallelism.html}
}

@misc{DifferentCRCAlgorithms,
  title = {9. {{Different CRC}} Algorithms},
  journal = {How does it work? Automatics, computers, etc...},
  urldate = {2022-05-16},
  abstract = {How does CRC work? Chapter~ 9 Different CRC algorithms Chapter. 9.1 Introduction The chapter 4 CRC arithmetic is a all CRC algorithms base. 1 bit shifts algorithm is realised on registers{\textendash}{$>$}chapter 5Read More...},
  langid = {british},
  file = {C:\Users\emilm\Zotero\storage\Q2ZQ8JID\9-rozne-algorytmy-crc.html}
}

@misc{DigitalLowVoltage,
  title = {Digital {{Low Voltage Protector Disconnect Switch Cut Off 12V Over-Discharge Protection Module}} for 12-{{36V Lead Acid Lithium Battery}} : {{Patio}}, {{Lawn}} \& {{Garden}}},
  urldate = {2022-06-06},
  howpublished = {https://www.amazon.com/Digital-Battery-Low-Voltage-Protection/dp/B07929Y5SZ},
  file = {C:\Users\emilm\Zotero\storage\7P3KBDWJ\B07929Y5SZ.html}
}

@techreport{diodesincorperated74HC32Datasheet2013,
  type = {Datasheet},
  title = {{{74HC32}} Datasheet},
  author = {{DIODES incorperated}},
  year = {2013},
  month = jan,
  number = {DS35324 Rev. 3 - 2},
  file = {C:\Users\emilm\Zotero\storage\R3HYM9WL\74HC32.pdf}
}

@misc{dockerinc.DockerOverview2022,
  title = {Docker Overview},
  author = {Docker Inc.},
  year = {2022},
  month = may,
  journal = {Docker Documentation},
  urldate = {2022-05-19},
  abstract = {Docker explained in depth},
  howpublished = {https://docs.docker.com/get-started/overview/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\P3APWQ67\overview.html}
}

@misc{dorsselaerUsbipdwin2023,
  title = {Usbipd-Win},
  author = {van Dorsselaer, Frans},
  year = {2023},
  month = may,
  urldate = {2023-05-15},
  abstract = {Windows software for sharing locally connected USB devices to other machines, including Hyper-V guests and WSL 2.},
  copyright = {GPL-3.0},
  keywords = {hyper-v,usb,usbip,usbip-win,usbipd,windows,wsl,wsl2}
}

@misc{doughertyUltraLowLatency2022,
  title = {Ultra {{Low Latency}}: {{What It Means}} for {{Video Streaming}}},
  shorttitle = {Ultra {{Low Latency}}},
  author = {Dougherty, Brittney},
  year = {2022},
  month = feb,
  journal = {Wowza},
  urldate = {2023-05-26},
  abstract = {Often defined as sub-second delivery,~ultra low latency streaming can be achieved in a few different ways. Learn how the approaches compare.},
  howpublished = {https://www.wowza.com/blog/ultra-low-latency},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\V7BKKCFN\ultra-low-latency.html}
}

@misc{drewbatgit10bit16bitYUV2022,
  title = {10-Bit and 16-Bit {{YUV Video Formats}} - {{Win32}} Apps},
  author = {{drewbatgit}},
  year = {2022},
  month = nov,
  urldate = {2023-04-13},
  abstract = {This topic describes the 10- and 16-bit YUV formats that are recommended for capturing, processing, and displaying video in the Microsoft Windows operating system.},
  howpublished = {https://learn.microsoft.com/en-us/windows/win32/medfound/10-bit-and-16-bit-yuv-video-formats},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\D8GSD28H\10-bit-and-16-bit-yuv-video-formats.html}
}

@misc{dtyuImbalancedPerformanceRead2018,
  title = {Imbalanced {{Performance}} between {{Read}} and {{Write Performance}}},
  author = {{dtyu}},
  year = {2018},
  month = nov,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-06-18},
  abstract = {Dear WayneWWW:  After I turn on the ./jetson\_clocks.sh and set mode 0.  I doubled the performance.  However, it is still far away from the spec. Did I hit the limit of Xavier?  Is the M2 port on PCI Gen2 bus or Gen3 bus?  What is the physical limitation of the M2 port?  If I mount the SSD to the PCI Gen3X16 bus on Xavier? Will I improve the performance?  Thanks  Dantong  nvidia@jetson-0423018054929:{\textasciitilde}\$ sudo ./jetson\_clocks.sh --show  SOC family:tegra194  Machine:jetson-xavier  Online CPUs: 0-7  C...},
  chapter = {Autonomous Machines},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\5QDWW3AJ\67556.html}
}

@misc{duaneAnswerRtspclientsinkTest2022,
  title = {Answer to "Rtspclientsink Test Pipeline from Command Line"},
  author = {Duane},
  year = {2022},
  month = jul,
  journal = {Stack Overflow},
  urldate = {2023-04-10},
  file = {C:\Users\emilm\Zotero\storage\JJDRV7X8\rtspclientsink-test-pipeline-from-command-line.html}
}

@misc{DXFLaserFusion,
  title = {{{DXF}} for {{Laser}} | {{Fusion}} 360 | {{Autodesk App Store}}},
  urldate = {2022-05-15},
  howpublished = {https://apps.autodesk.com/FUSION/en/Detail/Index?id=7634902334100976871\&os=Win64\&appLang=en},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\DTB5TX7Z\Index.html}
}

@misc{Dynamic3DGaussians,
  title = {Dynamic {{3D Gaussians}}: {{Tracking}} by {{Persistent Dynamic View Synthesis}}},
  urldate = {2023-10-18},
  howpublished = {https://dynamic3dgaussians.github.io/},
  file = {C:\Users\emilm\Zotero\storage\CLA82YU7\dynamic3dgaussians.github.io.html}
}

@misc{easycomposites30mm27mmWoven,
  title = {30mm (27mm) {{Woven Finish Carbon Fibre Tube}}; 1m, 2m - {{Easy Composites}}},
  author = {Easy Composites},
  urldate = {2022-05-16},
  howpublished = {https://www.easycomposites.co.uk/30mm-woven-finish-carbon-fibre-tube},
  file = {C:\Users\emilm\Zotero\storage\8XRB8Y85\30mm-woven-finish-carbon-fibre-tube.html}
}

@misc{edgewallsoftwareRTCMNtrip,
  title = {{{RTCM-Ntrip}}},
  author = {Edgewall Software},
  urldate = {2022-05-02},
  howpublished = {https://software.rtcm-ntrip.org/},
  file = {C:\Users\emilm\Zotero\storage\MTL5IZHQ\software.rtcm-ntrip.org.html}
}

@misc{EdSheeranTour,
  title = {Ed {{Sheeran}} +{\textendash}={$\div$}x {{Tour}} - {{Created}} by @{{VibrantNebula}}\_{{Luma}} with {{Luma}}},
  journal = {Ed Sheeran +{\textendash}={$\div$}x Tour - Created by @VibrantNebula\_Luma with Luma},
  urldate = {2023-10-25},
  abstract = {August 20, 2023},
  howpublished = {https://lumalabs.ai/capture/443a844e-525f-41e1-9750-1dff62782466},
  file = {C:\Users\emilm\Zotero\storage\N2XHJLR6\443a844e-525f-41e1-9750-1dff62782466.html}
}

@misc{EfficientVariantsICP,
  title = {Efficient {{Variants}} of the {{ICP Algorithm}} | {{Request PDF}}},
  urldate = {2022-11-27},
  howpublished = {https://www.researchgate.net/publication/3897594\_Efficient\_Variants\_of\_the\_ICP\_Algorithm},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\NQNZLABX\\Efficient Variants of the ICP Algorithm  Request .pdf;C\:\\Users\\emilm\\Zotero\\storage\\99E94HDG\\3897594_Efficient_Variants_of_the_ICP_Algorithm.html}
}

@misc{eigenEigenUsingEigen,
  title = {Eigen: {{Using Eigen}} in {{CUDA}} Kernels},
  author = {Eigen},
  urldate = {2023-06-26},
  howpublished = {https://eigen.tuxfamily.org/dox/TopicCUDA.html},
  file = {C:\Users\emilm\Zotero\storage\AZA5PDCM\TopicCUDA.html}
}

@misc{ekinssolutionsllcNiftyDogbonesFusion,
  title = {Nifty {{Dogbones}} for {{Fusion}} 360},
  author = {Ekins Solutions LLC},
  journal = {Ekins Solutions, LLC},
  urldate = {2022-05-16},
  abstract = {A fast and robust tool for adding dogbone fillets to the inside corners of a model. See the video and the full product description below for complete information about what this add-in does and how to use it.      Click the correct button below to download a fully-functional version of the add-in that will work for 60 days. After 60 days you must purchase a license to continue to use it. Purchasing this product will provide you with a license key that you can use to activate the product so it will continue to run after the 60-day trial period. You DO NOT need to purchase it to use the 60-day trial.  Version 1.4.3 is now available. This version supports the March 2022 and later releases of Fusion 360. To update, or install for the first time, download and install the app using the button below.~ The app is the full version with full capabilities. If you have a trial it will no longer function after the trial period ends. Please let me know if you have any trouble installing or running the app.~ You can see information about what's new in each version at the bottom of the description below.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\IPNT9ZWV\nifty-dogbones-f360.html}
}

@misc{emilmartensEmailExchange2022,
  title = {Email Exchange On},
  author = {Emil Martens, Lie Jan Roger},
  year = {2022},
  month = jan,
  urldate = {2022-05-17},
  file = {C:\Users\emilm\Zotero\storage\IU6QCXRX\0.html}
}

@misc{EnablePpsSupporta,
  title = {Enable\_pps\_support\_raspian [{{Fish Otter Project Documentation}}]},
  urldate = {2023-05-16},
  howpublished = {https://otter.itk.ntnu.no/doku.php?id=enable\_pps\_support\_raspian},
  file = {C:\Users\emilm\Zotero\storage\9R2Z4ZWK\doku.html}
}

@misc{engelDirectSparseOdometry2016,
  title = {Direct {{Sparse Odometry}}},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  year = {2016},
  month = oct,
  number = {arXiv:1607.02565},
  eprint = {1607.02565},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1607.02565},
  urldate = {2022-11-28},
  abstract = {We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RH4I668Y\\Engel et al. - 2016 - Direct Sparse Odometry.pdf;C\:\\Users\\emilm\\Zotero\\storage\\5FYEJKIB\\1607.html}
}

@misc{ericPuffedLipoBattery2017,
  title = {Puffed {{Lipo Battery}}: {{Why}} They Swell and What to Do about It {\textbullet} {{LearningRC}}},
  shorttitle = {Puffed {{Lipo Battery}}},
  author = {Eric},
  year = {2017},
  month = mar,
  journal = {LearningRC},
  urldate = {2022-05-19},
  abstract = {Anybody who uses lipos will eventually encounter a puffy or swollen battery. And the first question that inevitably comes up is "What should I do?"},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\828PQ87W\puffed-lipos.html}
}

@misc{eriksenDashExtensions,
  title = {Dash {{Extensions}}},
  author = {Eriksen, Emil Haldrup},
  urldate = {2023-05-15},
  abstract = {Dash Extensions is a collection of utility functions, syntax extensions, and Dash components that aim to improve the Dash development experience},
  howpublished = {https://www.dash-extensions.com/},
  file = {C:\Users\emilm\Zotero\storage\E2A8KHLD\www.dash-extensions.com.html}
}

@misc{eriksenDashExtensionsWebSocket,
  title = {Dash {{Extensions}}, {{WebSocket}}},
  author = {Eriksen, Emil Haldrup},
  urldate = {2023-06-03},
  abstract = {Dash Extensions is a collection of utility functions, syntax extensions, and Dash components that aim to improve the Dash development experience},
  howpublished = {https://www.dash-extensions.com/components/websocket},
  file = {C:\Users\emilm\Zotero\storage\5UHEFY7F\websocket.html}
}

@misc{ErrorFlashingSSD2023,
  title = {Error Flashing {{SSD}}: /{{Linux}}\_for\_{{Tegra}}/Bootloader/Signed/Flash.Idx Is Not Found},
  shorttitle = {Error Flashing {{SSD}}},
  year = {2023},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-12},
  abstract = {Hello,  I am trying to flash the Orin to boot from SSD.  I am applying these steps, first I flash the EMMC and then I use l4t\_initrd\_flash.sh to flash the external storage device.  Steps:  systemctl stop udisks2.service  wget https://developer.download.nvidia.com/embedded/L4T/r35\_Release\_v1.0/Release/secureboot\_R35.1.0\_aarch64.tbz2 wget https://developer.nvidia.com/embedded/l4t/r35\_release\_v1.0/release/jetson\_linux\_r35.1.0\_aarch64.tbz2 wget https://developer.nvidia.com/embedded/l4t/r35\_release\_v...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/error-flashing-ssd-linux-for-tegra-bootloader-signed-flash-idx-is-not-found/240046},
  langid = {english}
}

@misc{esabknowledgecenterWhatCuttingKerf,
  title = {What Is Cutting Kerf?},
  author = {ESAB KNOWLEDGE CENTER},
  urldate = {2022-05-18},
  howpublished = {https://www.esabna.com/us/en/education/blog/what-is-cutting-kerf.cfm},
  file = {C:\Users\emilm\Zotero\storage\QEMNJAIZ\what-is-cutting-kerf.html}
}

@misc{EthzaslKalibr2022,
  title = {Ethz-Asl/Kalibr},
  year = {2022},
  month = may,
  urldate = {2022-05-30},
  abstract = {The Kalibr visual-inertial calibration toolbox},
  howpublished = {ETHZ ASL},
  keywords = {calibration,calibration-toolbox,camera,imu}
}

@misc{EthzaslKalibr2023,
  title = {Ethz-Asl/Kalibr},
  year = {2023},
  month = jun,
  urldate = {2023-06-11},
  abstract = {The Kalibr visual-inertial calibration toolbox},
  howpublished = {ETHZ ASL},
  keywords = {calibration,calibration-toolbox,camera,imu}
}

@misc{farhutsWebSocketsBeginnersPart2019,
  title = {{{WebSockets For Beginners}}. {{Part}} 1.},
  author = {Farhuts, Tetiana},
  year = {2019},
  month = jun,
  journal = {Medium},
  urldate = {2023-06-03},
  abstract = {Theory.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\FVVBZPSC\websockets-for-beginners-part-1-10796106e207.html}
}

@misc{fedora13CheckingIf,
  title = {13.3.5.~{{Checking}} If Chrony Is {{Synchronized}}},
  author = {Fedora},
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\M39TTF6S\sect-Checking_if_chrony_is_synchronized.html}
}

@misc{fieldofviewPrinterSettingsUltimaker,
  title = {Printer {{Settings}} - {{Ultimaker Cura Marketplace}}},
  author = {{fieldOfView}},
  urldate = {2023-06-04},
  howpublished = {https://marketplace.ultimaker.com/app/cura/plugins/fieldofview/PrinterSettingsPlugin},
  file = {C:\Users\emilm\Zotero\storage\W24FI8BL\PrinterSettingsPlugin.html}
}

@misc{fischerRe15406LUT2022,
  title = {Re:[\#\# 15406 \#\#]  {{LUT}} and Debayering on {{TRI050S1-QC}}},
  author = {Fischer, Felix and Martens, Emil},
  year = {2022},
  month = oct
}

@misc{fisherRe15406LUT2023,
  title = {Re:[\#\# 15406 \#\#]  {{LUT}} and Debayering on {{TRI050S1-QC}}},
  author = {Fisher, Felix},
  year = {2023},
  month = feb
}

@misc{fisherStaticIPAddresses2021,
  title = {Static {{IP Addresses}}: {{Everything You Need}} to {{Know}}},
  shorttitle = {Static {{IP Addresses}}},
  author = {Fisher, Tim},
  year = {2021},
  month = sep,
  journal = {Lifewire},
  urldate = {2023-05-03},
  abstract = {What is a static IP address? It's any manually configured IP address, sometimes referred to as a fixed IP address. Learn the difference between static vs dynamic IP addresses.},
  chapter = {Lifewire},
  howpublished = {https://www.lifewire.com/what-is-a-static-ip-address-2626012},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\3PNFJK3V\what-is-a-static-ip-address-2626012.html}
}

@misc{fishotterprojectEnablePpsSupport,
  title = {Enable\_pps\_support\_raspian [{{Fish Otter Project Documentation}}]},
  author = {Fish Otter Project},
  urldate = {2023-05-08},
  howpublished = {https://otter.itk.ntnu.no/doku.php?id=enable\_pps\_support\_raspian},
  file = {C:\Users\emilm\Zotero\storage\K4WISKMV\doku.html}
}

@misc{FlashNVMeSSD2021,
  title = {Flash of {{NVMe SSD}}},
  year = {2021},
  month = sep,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-11},
  abstract = {Hi  I recently installed an NVMe SSD in to my AGX Xavier as the native boot came out with jetpack 4.6. I thought that would be the easiest way to go around the issue that Docker images often filled up the internal space.  I tryed to get it flashed using the l4t\_initrd\_flash.sh but not with any luck. so i went and used the jetsonhacks repo and I managed to get the boot up on the SSD but then the nvidia container did not seem to work. Now I just want to flash my NVMe SSD with a fresh jetpack insta...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/flash-of-nvme-ssd/189770},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\6NUZJZKX\2.html}
}

@misc{formlabs20223DPrinting,
  title = {The 2022 {{3D Printing Applications Report}}},
  author = {{formlabs}},
  urldate = {2022-05-18},
  abstract = {Read the new report from Formlabs that examines how a new generation of businesses are using 3D printing.},
  howpublished = {https://formlabs.drift.click/74c0506e-fddb-4221-b325-3fdaa037a281},
  file = {C:\Users\emilm\Zotero\storage\ZVIJ4JPE\74c0506e-fddb-4221-b325-3fdaa037a281.html}
}

@inproceedings{forsterIMUPreintegrationManifold2015,
  title = {{{IMU Preintegration}} on {{Manifold}} for {{Efficient Visual-Inertial Maximum-a-Posteriori Estimation}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  year = {2015},
  month = jul,
  doi = {10.15607/RSS.2015.XI.006},
  abstract = {Recent results in monocular visual-inertial navigation (VIN) have shown that optimization-based approaches outperform filtering methods in terms of accuracy due to their capability to relinearize past states. However, the improvement comes at the cost of increased computational complexity. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes. The preintegration allows us to accurately summarize hundreds of inertial measurements into a single relative motion constraint. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group and carefully deals with uncertainty propagation. The measurements are integrated in a local frame, which eliminates the need to repeat the integration when the linearization point changes while leaving the opportunity for belated bias corrections. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated in a visual-inertial pipeline under the unifying framework of factor graphs. This enables the use of a structureless model for visual measurements, further accelerating the computation. The third contribution is an extensive evaluation of our monocular VIN pipeline: experimental results confirm that our system is very fast and demonstrates superior accuracy with respect to competitive state-of-the-art filtering and optimization algorithms, including off-the-shelf systems such as Google Tango.},
  keywords = {lie},
  file = {C:\Users\emilm\Zotero\storage\BDGIXSEV\Forster et al. - 2015 - IMU Preintegration on Manifold for Efficient Visua.pdf}
}

@article{forsterOnManifoldPreintegrationRealTime2017,
  title = {On-{{Manifold Preintegration}} for {{Real-Time Visual-Inertial Odometry}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Robotics},
  volume = {33},
  number = {1},
  eprint = {1512.02363},
  primaryclass = {cs},
  pages = {1--21},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2597321},
  urldate = {2022-10-27},
  abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time, this problem is further emphasized by the fact that inertial measurements come at high rate, hence leading to fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a {\textbackslash}emph\{preintegration theory\} that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the {\textbackslash}emph\{maximum a posteriori\} state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a-posteriori bias correction in analytic form. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a {\textbackslash}emph\{structureless\} model for visual measurements, which avoids optimizing over the 3D points, further accelerating the computation. We perform an extensive evaluation of our monocular {\textbackslash}VIO pipeline on real and simulated datasets. The results confirm that our modelling effort leads to accurate state estimation in real-time, outperforming state-of-the-art approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LXEDJZ63\\Forster et al. - 2017 - On-Manifold Preintegration for Real-Time Visual-In.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3MLWJJQU\\1512.html}
}

@misc{FramesNotRecommended2022,
  title = {B Frames Not Recommended in Nvv4l2h265enc Gstreamer Plugin},
  year = {2022},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-01},
  abstract = {Running gst-inspect-1.0 nvv4l2h265enc shows:  ...   num-B-Frames        : Number of B Frames between two reference frames (not recommended)(Supported only on Xavier)                         flags: readable, writable, changeable only in NULL or READY state                         Unsigned Integer. Range: 0 - 2 Default: 0 ...  I am wondering why this setting is ``(not recommended)''.},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/b-frames-not-recommended-in-nvv4l2h265enc-gstreamer-plugin/213833},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\LVSU2JME\213833.html}
}

@misc{franzenTrueColorKodak2013,
  title = {True {{Color Kodak Images}}},
  author = {Franzen, Rich},
  year = {2013},
  month = jan,
  urldate = {2023-05-23},
  howpublished = {https://r0k.us/graphics/kodak/},
  file = {C:\Users\emilm\Zotero\storage\NNLA4G42\kodak.html}
}

@article{FresnelEquations2023,
  title = {Fresnel Equations},
  year = {2023},
  month = jun,
  journal = {Wikipedia},
  urldate = {2023-06-22},
  abstract = {The Fresnel equations (or Fresnel coefficients) describe the reflection and transmission of light (or electromagnetic radiation in general) when incident on an interface between different optical media. They were deduced by Augustin-Jean Fresnel () who was the first to understand that light is a transverse wave, even though no one realized that the "vibrations" of the wave were electric and magnetic fields. For the first time, polarization could be understood quantitatively, as Fresnel's equations correctly predicted the differing behaviour of waves of the s and p polarizations incident upon a material interface.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1158170414},
  file = {C:\Users\emilm\Zotero\storage\STJELWED\Fresnel_equations.html}
}

@inproceedings{furgaleUnifiedTemporalSpatial2013,
  title = {Unified Temporal and Spatial Calibration for Multi-Sensor Systems},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Furgale, Paul and Rehder, Joern and Siegwart, Roland},
  year = {2013},
  month = nov,
  pages = {1280--1286},
  publisher = {{IEEE}},
  address = {{Tokyo}},
  doi = {10.1109/IROS.2013.6696514},
  urldate = {2022-11-26},
  isbn = {978-1-4673-6358-7 978-1-4673-6357-0},
  file = {C:\Users\emilm\Zotero\storage\TR8QE7MI\Furgale et al. - 2013 - Unified temporal and spatial calibration for multi.pdf}
}

@misc{GaussianSplattingVR2023,
  title = {Gaussian {{Splatting VR Viewer}}},
  year = {2023},
  month = oct,
  urldate = {2023-10-25},
  abstract = {A VR viewer for gaussian splatting models developped as native plugin for unity with the original CUDA rasterizer.},
  howpublished = {CLARTE-LAB},
  keywords = {cuda,gaussian-splatting,unity3d,vr}
}

@inproceedings{geigerMEMSIMUAHRS2008,
  title = {{{MEMS IMU}} for {{AHRS}} Applications},
  booktitle = {2008 {{IEEE}}/{{ION Position}}, {{Location}} and {{Navigation Symposium}}},
  author = {Geiger, W. and Bartholomeyczik, J. and Breng, U. and Gutmann, W. and Hafen, M. and Handrich, E. and Huber, M. and Jackle, A. and Kempfer, U. and Kopmann, H. and Kunz, J. and Leinfelder, P. and Ohmberger, R. and Probst, U. and Ruf, M. and Spahlinger, G. and Rasch, A. and {Straub-Kalthoff}, J. and Stroda, M. and Stumpf, K. and Weber, C. and Zimmermann, M. and Zimmermann, S.},
  year = {2008},
  month = may,
  pages = {225--231},
  issn = {2153-3598},
  doi = {10.1109/PLANS.2008.4569973},
  abstract = {Northrop Grumman, LITEF is developing MEMS (micro-electro-mechanical systems) based Inertial Measurement Units (IMU) for future attitude and heading reference systems (AHRS) with a target accuracy of 5 deg/h for the gyroscopes and 2.5 mg for the accelerometers. Within the technology development phase, prototype single axis gyroscopes have been realized and extensively tested for effects including temperature, acoustic and vibration sensitivities. These devices employ micro-machined all-silicon gyroscope sensor chips processed with deep reactive ion etching (DRIE). Silicon fusion bonding ensures pressures smaller than 3middot10-2 mbar. Sophisticated analog electronics and digital signal processing condition the capacitive pick-off signals and realize full closed loop operation. The current results with overall bias error smaller than 2 deg/h to 5 deg/h, scale factor error {$<$}1200 ppm, measurement range {$>$}1000 deg/s and angular random walk {$<$}0.4 radic/vh indicate that stable production of 5 deg/h gyroscopes is realistic. The fabrication technology for capacitive, pendulous accelerometer chips is based on that used for the gyros with only an increase in the enclosed pressure to obtain overcritical damping. Pulse width modulation (PWM) within a digital control loop is used to realize closed loop operation. Accelerometer chips have been tested over temperature with a residual bias error {$<$}2.0 mg and a scale factor error {$<$}1400 ppm. These sensor chips have been integrated into an IMU whereby the power budget and size of the sensor electronics have been optimized. In this paper the salient features of the gyro and accelerometer designs are presented together with an overview of the IMU system architecture. Measurement results, with a focus on environmental characteristics and robustness, are included.},
  keywords = {Accelerometers,Acoustic testing,Gyroscopes,Measurement units,Microelectromechanical systems,Micromechanical devices,Prototypes,Pulse width modulation,Semiconductor device measurement,Temperature sensors},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\WRVD8KCR\\Geiger et al. - 2008 - MEMS IMU for AHRS applications.pdf;C\:\\Users\\emilm\\Zotero\\storage\\HQ3Z4AVZ\\4569973.html}
}

@article{getreuerMalvarHeCutlerLinearImage2011,
  title = {Malvar-{{He-Cutler Linear Image Demosaicking}}},
  author = {Getreuer, Pascal},
  year = {2011},
  month = aug,
  journal = {Image Processing On Line},
  volume = {1},
  doi = {10.5201/ipol.2011.g_mhcd},
  abstract = {Image demosaicking (or demosaicing) is the interpolation problem of estimating complete color information for an image that has been captured through a color filter array (CFA), particularly on the Bayer pattern. In this paper we review a simple linear method using 5 x 5 filters, proposed by Malvar, He, and Cutler in 2004, that shows surprisingly good results.},
  file = {C:\Users\emilm\Zotero\storage\YB2BMGXK\Getreuer - 2011 - Malvar-He-Cutler Linear Image Demosaicking.pdf}
}

@misc{ghargeCuraVaseMode2022,
  title = {Cura {{Vase Mode}}: {{The Basics Simply Explained}}},
  shorttitle = {Cura {{Vase Mode}}},
  author = {Gharge, Pranav},
  year = {2022},
  month = may,
  journal = {All3DP},
  urldate = {2023-05-27},
  abstract = {Vase mode is a common slicer mode for "aesthetic prints". Follow along as we demystify Cura's vase mode to create some seamless models.},
  howpublished = {https://all3dp.com/2/cura-vase-mode-all-you-need-to-know/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\FBD8UWG2\cura-vase-mode-all-you-need-to-know.html}
}

@article{gioiaDataDrivenConvolutionalModel2021,
  title = {Data-{{Driven Convolutional Model}} for {{Digital Color Image Demosaicing}}},
  author = {Gioia, Francesco and Fanucci, Luca},
  year = {2021},
  month = oct,
  journal = {Applied Sciences},
  volume = {11},
  pages = {9975},
  doi = {10.3390/app11219975},
  abstract = {Modern digital cameras use specific arrangement of Color Filter Array to sample light wavelength corresponding to visible colors. The most common Color Filter Array is the Bayer filter that samples only one color per pixel. To recover the full resolution image, an interpolation algorithm can be used. This process is called demosaicing and it is one of the first processing stages of a digital imaging pipeline. We introduce a novel data-driven model for demosaicing that takes into account the different requirements for reconstruction of the image Luma and Chrominance channels. The final model is a parallel composition of two reconstruction networks with individual architecture and trained with distinct loss functions. In order to solve the overfitting problem, we prepared a dataset that contains groups of patches that share common chromatic and spectral characteristics. We reported the reconstruction error on noise-free images and measured the effect of random noise and quantization noise in the demosaicing reconstruction. To test our model performance, we implemented the network on NVIDIA Jetson Nano, obtaining an end-to-end running time of less than one second for a full frame 12 MPixel image.},
  file = {C:\Users\emilm\Zotero\storage\HU5VCLXN\Gioia and Fanucci - 2021 - Data-Driven Convolutional Model for Digital Color .pdf}
}

@misc{giomettiLinuxPPSWikiLinuxPPS2007,
  title = {{{LinuxPPS}} Wiki [{{LinuxPPS}}]},
  author = {{giometti}},
  year = {2007},
  urldate = {2023-05-04},
  howpublished = {https://www.kernel.org/doc/html/next/driver-api/pps.html},
  file = {C:\Users\emilm\Zotero\storage\7DNNM6H7\doku.html}
}

@misc{gmvDifferentialGNSSNavipedia2018,
  title = {Differential {{GNSS}} - {{Navipedia}}},
  author = {GMV},
  year = {2018},
  month = jul,
  urldate = {2022-05-21},
  howpublished = {https://gssc.esa.int/navipedia/index.php/Differential\_GNSS},
  file = {C:\Users\emilm\Zotero\storage\78WRJE3S\Differential_GNSS.html}
}

@misc{gnatAnswerChangingMetric2017,
  title = {Answer to "{{Changing}} the Metric of an Interface Permanently"},
  author = {Gnat},
  year = {2017},
  month = dec,
  journal = {Unix \& Linux Stack Exchange},
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\QAWC79SX\changing-the-metric-of-an-interface-permanently.html}
}

@misc{GoogleColaboratory,
  title = {Google {{Colaboratory}}},
  urldate = {2023-10-25},
  howpublished = {https://colab.research.google.com/drive/1YniEH63VfZPuRGTddviUvNH48cDaLqtg\#scrollTo=7A2BV9kPwyMU},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\PGMJE6GN\1YniEH63VfZPuRGTddviUvNH48cDaLqtg.html}
}

@misc{GopStructureGoogle,
  title = {Gop Structure - {{Google Search}}},
  urldate = {2023-04-12},
  howpublished = {https://www.google.com/search?q=gop+structure\&rlz=1C1CHBF\_enNO1047NO1047\&oq=GOP+structure\&aqs=chrome.0.0i512l4j0i10i22i30j0i22i30l5.226j0j4\&sourceid=chrome\&ie=UTF-8\#imgrc=LP4rOEfMTtQP3M}
}

@misc{GPSGovOther,
  title = {{{GPS}}.Gov: {{Other Global Navigation Satellite Systems}} ({{GNSS}})},
  urldate = {2022-05-21},
  howpublished = {https://www.gps.gov/systems/gnss/},
  file = {C:\Users\emilm\Zotero\storage\HB2UBRGU\gnss.html}
}

@misc{grusinSerialPeripheralInterface,
  title = {Serial {{Peripheral Interface}} ({{SPI}}) - Learn.Sparkfun.Com},
  author = {Grusin, Mike},
  urldate = {2022-05-23},
  file = {C:\Users\emilm\Zotero\storage\SQWQSUMI\all.html}
}

@misc{Gsplat,
  title = {Gsplat},
  urldate = {2023-11-22},
  howpublished = {https://gsplat.tech/},
  file = {C:\Users\emilm\Zotero\storage\6FR9XVRR\gsplat.tech.html}
}

@misc{Gstlaunch1,
  title = {Gst-Launch-1.0},
  urldate = {2023-05-27},
  howpublished = {https://gstreamer.freedesktop.org/documentation/tools/gst-launch.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\9IH64CYR\gst-launch.html}
}

@misc{GstMemoryNew,
  title = {Gst.{{Memory}}.New\_wrapped {\textendash} Gstreamer-1.0},
  urldate = {2023-05-30},
  abstract = {The canonical source for Vala API references.},
  howpublished = {https://valadoc.org/gstreamer-1.0/Gst.Memory.new\_wrapped.html},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\6IVNQV4D\Gst.Memory.new_wrapped.html}
}

@misc{gstreamerAPIReference,
  title = {{{API}} Reference},
  author = {GStreamer},
  urldate = {2023-05-30},
  howpublished = {https://gstreamer.freedesktop.org/documentation/libs.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\KFFG97Z3\libs.html}
}

@misc{gstreamerGstBuffer,
  title = {{{GstBuffer}}},
  author = {GStreamer},
  urldate = {2023-05-30},
  howpublished = {https://gstreamer.freedesktop.org/documentation/gstreamer/gstbuffer.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\3UB988VR\gstbuffer.html}
}

@misc{gstreamerGStreamerOpenSource,
  title = {{{GStreamer}}: Open Source Multimedia Framework},
  author = {GStreamer},
  urldate = {2023-05-30},
  howpublished = {https://gstreamer.freedesktop.org/},
  file = {C:\Users\emilm\Zotero\storage\JSTJYAGD\gstreamer.freedesktop.org.html}
}

@misc{gstreamerProbes,
  title = {Probes},
  author = {GStreamer},
  urldate = {2023-06-17},
  howpublished = {https://gstreamer.freedesktop.org/documentation/additional/design/probes.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\IRHQQUER\probes.html}
}

@misc{gstreamerVideoFormat,
  title = {Video Format},
  author = {GStreamer},
  urldate = {2023-06-14},
  howpublished = {https://gstreamer.freedesktop.org/documentation/video/video-format.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\ZEQCQSC3\video-format.html}
}

@misc{gstreamerWebrtcMasterGStreamer2021,
  title = {Webrtc {$\cdot$} Master {$\cdot$} {{GStreamer}} / Gst-Examples {$\cdot$} {{GitLab}}},
  author = {GStreamer},
  year = {2021},
  month = aug,
  journal = {GitLab},
  urldate = {2023-05-26},
  abstract = {ARCHIVED REPOSITORY: GStreamer example applications This code has been moved to the GStreamer mono repo, please submit new issues and merge requests there!},
  howpublished = {https://gitlab.freedesktop.org/gstreamer/gst-examples/-/tree/master/webrtc},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\F8RNDDYH\webrtc.html}
}

@misc{GStreamNvv4l2h264encBFrames2019,
  title = {{{GStream}}: Nvv4l2h264enc: {{B-Frames}} Display Ordering Is Not Right},
  shorttitle = {{{GStream}}},
  year = {2019},
  month = jul,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-11},
  abstract = {This bug requires to enable the new b-frames fix see: [url]https://devtalk.nvidia.com/default/topic/1056338/jetson-nano/gstreamer-h264-encoder-bug-gstv4l2h264enc-h-num-b-frames-is-set-to-boolean-instead-int/post/5359591/\#5359591[/url]  It appears that the nvv4l2h264enc encoder is setting the display timestamps the same as the decoder timestamps.  A strict player will show the lack of proper order of each frame i.e. IPbbPbbPbb{\ldots} instead of IbbPbbPbb{\ldots} (Other players will skip one of the b frames an...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/gstream-nvv4l2h264enc-b-frames-display-ordering-is-not-right/77852},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VNILYA4M\77852.html}
}

@article{haavardsholmHandbookVisualSLAM,
  title = {A Handbook in {{Visual SLAM}}},
  author = {Haavardsholm, Trym Vegard},
  pages = {95},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\3ASH5QUG\Haavardsholm - A handbook in Visual SLAM.pdf}
}

@misc{hagemannInferringBiasUncertainty2021,
  title = {Inferring Bias and Uncertainty in Camera Calibration},
  author = {Hagemann, Annika and Knorr, Moritz and Janssen, Holger and Stiller, Christoph},
  year = {2021},
  month = jul,
  number = {arXiv:2107.13484},
  eprint = {2107.13484},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-25},
  abstract = {Accurate camera calibration is a precondition for many computer vision applications. Calibration errors, such as wrong model assumptions or imprecise parameter estimation, can deteriorate a system's overall performance, making the reliable detection and quantification of these errors critical. In this work, we introduce an evaluation scheme to capture the fundamental error sources in camera calibration: systematic errors (biases) and uncertainty (variance). The proposed bias detection method uncovers smallest systematic errors and thereby reveals imperfections of the calibration setup and provides the basis for camera model selection. A novel resampling-based uncertainty estimator enables uncertainty estimation under non-ideal conditions and thereby extends the classical covariance estimator. Furthermore, we derive a simple uncertainty metric that is independent of the camera model. In combination, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups. We evaluate the proposed methods with simulations and real cameras.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\45KCNPAY\\Hagemann et al. - 2021 - Inferring bias and uncertainty in camera calibrati.pdf;C\:\\Users\\emilm\\Zotero\\storage\\9LT83DD9\\2107.html}
}

@article{HalfprecisionFloatingpointFormat2023,
  title = {Half-Precision Floating-Point Format},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-04-21},
  abstract = {In computing, half precision (sometimes called FP16 or float16) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential, in particular image processing and neural networks. Almost all modern uses follow the IEEE 754-2008 standard, where the 16-bit base-2 format is referred to as binary16, and the exponent uses 5 bits. This can express values in the range {$\pm$}65,504, with the minimum value above 1 being 1 + 1/1024. Depending on the computer, half-precision can be over an order of magnitude faster than double precision, e.g. 550 PFLOPS for half-precision vs 37 PFLOPS for double precision on one cloud provider.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1148024680},
  file = {C:\Users\emilm\Zotero\storage\IHEZRLUR\Half-precision_floating-point_format.html}
}

@misc{hanTechnicalOverviewAV12021,
  title = {A {{Technical Overview}} of {{AV1}}},
  author = {Han, Jingning and Li, Bohan and Mukherjee, Debargha and Chiang, Ching-Han and Grange, Adrian and Chen, Cheng and Su, Hui and Parker, Sarah and Deng, Sai and Joshi, Urvang and Chen, Yue and Wang, Yunqing and Wilkins, Paul and Xu, Yaowu and Bankoski, James},
  year = {2021},
  month = feb,
  number = {arXiv:2008.06091},
  eprint = {2008.06091},
  primaryclass = {eess},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {The AV1 video compression format is developed by the Alliance for Open Media consortium. It achieves more than 30\% reduction in bit-rate compared to its predecessor VP9 for the same decoded video quality. This paper provides a technical overview of the AV1 codec design that enables the compression performance gains with considerations for hardware feasibility.},
  archiveprefix = {arxiv},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ZG236SUU\\Han et al_2021_A Technical Overview of AV1.pdf;C\:\\Users\\emilm\\Zotero\\storage\\A5PC6MU2\\2008.html}
}

@article{haugoTTK25ComputerVision,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 3},
  author = {Haugo, Simen},
  pages = {76},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\ND7NXFQJ\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisiona,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 4},
  author = {Haugo, Simen},
  pages = {86},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\RGWCX6N6\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisionb,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 6},
  author = {Haugo, Simen},
  pages = {44},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\H6BC4KM9\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisionc,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 1},
  author = {Haugo, Simen},
  pages = {149},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\YBM35CCL\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisiond,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 2},
  author = {Haugo, Simen},
  pages = {89},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\NL7EGP7C\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisione,
  title = {{{TTK25}} {\textendash} {{Computer Vision}} for {{Control}} - 5},
  author = {Haugo, Simen},
  pages = {62},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\Z8VPNANK\TTK25 – Computer Vision for Control - Simen Haugo.pdf}
}

@article{he75InventorsHenrique,
  title = {(75) {{Inventors}}: {{Henrique S}}. {{Malvar}}, {{Sammamish}}, {{WA}}},
  author = {He, Li-wei and Cutler, Ross and Nguyen, Truong-Thao},
  abstract = {A gradient-corrected linear interpolation method and system for the demosaicing of color images. The method and system compute an interpolation using some a current technique (preferably abilinear interpolation technique to reduce com putational complexity), compute a correction term (such as a gradient of a desired color at a given pixel), and linearly combine the interpolation and the correction term to produce a corrected, high-quality interpolation of a missing color value at a pixel. The correction term may be a gradient cor rection term computed from the current color of the current pixel. This gradient is directly used to affect and correct the estimated color value produced by the prior art interpolation technique. The gradient-corrected linear interpolation method and system may also apply a gradient-correction gain to the gradient correction term. This gradient-correction gain affects the amount of gradient correction that is applied to the interpolation.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\JG73EINC\He et al. - (75) Inventors Henrique S. Malvar, Sammamish, WA.pdf}
}

@misc{hellerWhatCUDANVIDIA2022,
  title = {What {{Is CUDA}} | {{NVIDIA Official Blog}}},
  author = {Heller, Martin},
  year = {2022},
  month = sep,
  urldate = {2023-05-10},
  howpublished = {https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/},
  file = {C:\Users\emilm\Zotero\storage\4G27ES6Z\what-is-cuda-2.html}
}

@misc{hermannswAnswerSettingUsb2021,
  title = {Answer to; "{{Setting}} the Usb Baud Rate ?" - {{Raspberry Pi Forums}}},
  author = {HermannSW},
  year = {Wed Mar 31, 2021 6:02 pm}
}

@misc{hippyAnswerSettingUsb2021,
  title = {Answer to; "{{Setting}} the Usb Baud Rate ?" - {{Raspberry Pi Forums}}},
  author = {{hippy}},
  year = {2021},
  month = mar,
  urldate = {2023-06-11},
  file = {C:\Users\emilm\Zotero\storage\NXPT5U36\viewtopic.html}
}

@inproceedings{hoExploitingHalfPrecision2017,
  title = {Exploiting Half Precision Arithmetic in {{Nvidia GPUs}}},
  booktitle = {2017 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  author = {Ho, Nhut-Minh and Wong, Weng-Fai},
  year = {2017},
  month = sep,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Waltham, MA, USA}},
  doi = {10.1109/HPEC.2017.8091072},
  urldate = {2023-06-20},
  abstract = {With the growing importance of deep learning and energy-saving approximate computing, half precision floating point arithmetic (FP16) is fast gaining popularity. Nvidia's recent Pascal architecture was the first GPU that offered FP16 support. However, when actual products were shipped, programmers soon realized that a na{\textasciidieresis}{\i}ve replacement of single precision (FP32) code with half precision led to disappointing performance results, even if they are willing to tolerate the increase in error precision reduction brings. In this paper, we developed an automated conversion framework to help users migrate their CUDA code to better exploit Pascal's half precision capability. Using our tools and techniques, we successfully convert many benchmarks from single precision arithmetic to half precision equivalent, and achieved significant speedup improvement in many cases. In the best case, a 3{\texttimes} speedup over the FP32 version was achieved. We shall also discuss some new issues and opportunities that the Pascal GPUs brought.},
  isbn = {978-1-5386-3472-1},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\9M9SDICJ\Ho and Wong - 2017 - Exploiting half precision arithmetic in Nvidia GPU.pdf}
}

@misc{HTTPRequestResponse,
  title = {2.1. {{The HTTP Request}}/{{Response Model}} - {{JavaServer Pages}}, 3rd {{Edition}} [{{Book}}]},
  urldate = {2023-06-03},
  abstract = {The HTTP Request/Response Model HTTP and all extended protocols based on HTTP are based on a very simple communications model. Here's how it works: a client, typically a web browser, {\ldots} - Selection from JavaServer Pages, 3rd Edition [Book]},
  howpublished = {https://www.oreilly.com/library/view/javaserver-pages-3rd/0596005636/ch02s01.html},
  isbn = {9780596005634},
  langid = {english}
}

@misc{ibmcloudeducationWhatDockerIBM2021,
  title = {What Is {{Docker}}? | {{IBM}}},
  author = {IBM Cloud Education},
  year = {2021},
  month = jun,
  urldate = {2022-06-07}
}

@misc{ibmIBMDocumentationTCPIP2021,
  title = {{{IBM Documentation}}, {{TCPIP IPv4}} Settings},
  author = {IBM},
  year = {2021},
  month = mar,
  urldate = {2023-05-02},
  abstract = {IBM Documentation.},
  copyright = {{\textcopyright} Copyright IBM Corporation 2012},
  howpublished = {https://www.ibm.com/docs/en/linux-on-systems?topic=tuning-tcpip-ipv4-settings},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\BBUIYZI7\linux-on-systems.html}
}

@misc{ibmIBMDocumentationTuning2021,
  title = {{IBM Documentation, Tuning your Linux system for more efficient parallel job performance}},
  author = {IBM},
  year = {2021},
  month = mar,
  urldate = {2023-04-27},
  abstract = {The Linux default network and network device settings might not produce optimum throughput (bandwidth) and latency numbers for large parallel jobs. The information that is provided describes how to tune the Linux network and certain network devices for better parallel job performance.},
  copyright = {{\textcopyright} Copyright IBM Corporation 2018},
  howpublished = {https://www.ibm.com/docs/de/smpi/10.2?topic=mpi-tuning-your-linux-system},
  langid = {ngerman},
  file = {C:\Users\emilm\Zotero\storage\29GPU545\10.html}
}

@article{ieeeIEEEStandardsInterpretation2002,
  title = {{{IEEE Standards Interpretation}} for {{IEEE Std}} 802.{{1D}}{\texttrademark}-1999, {{IEEE Std}} 802.{{1Q}}{\texttrademark}- 1998 and {{IEEE Std}} 802.3{\texttrademark}, 2000 {{Edition}}},
  author = {IEEE},
  year = {2002},
  month = jul
}

@misc{ImbalancedPerformanceRead2018,
  title = {Imbalanced {{Performance}} between {{Read}} and {{Write Performance}}},
  year = {2018},
  month = nov,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-22},
  abstract = {I installed a Samsung 970 EVO NVMe M.2 SSD to the M.2 port on Jetson Xavier by following the instruction on Installing an NVMe SSD on Nvidia Jetson Xavier | Medium.   I created ext4 file system on the SSD drive.  nvidia@jetson-0423018054929:/xavier\_ssd\$ df -h  Filesystem      Size  Used Avail Use\% Mounted on  /dev/root        28G   17G  9.5G  64\% /  devtmpfs        7.7G     0  7.7G   0\% /dev  tmpfs           7.8G   26M  7.8G   1\% /dev/shm  tmpfs           7.8G   36M  7.7G   1\% /run  tmpfs       ...},
  chapter = {Autonomous Machines},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\MI6QGUY7\17.html}
}

@techreport{intelcorporationIntelDualBand2016,
  title = {{{Intel}}{\textregistered} {{Dual Band Wireless-AC}} 8265},
  author = {Intel Corporation},
  year = {2016},
  number = {334064-005}
}

@misc{IntelI350am4Chip,
  title = {Intel {{I350am4 Chip Pcie X4 Rj45 Quad}} 4 {{Port Industrial Network Card Poe Vision Frame Grabber Nics Gigabit Ethernet Lan}} 1000mbps - {{Buy Poe Network Card}},{{I350am4 Chip Network Card}},1000m {{Network Card Product}} on {{Alibaba}}.Com},
  urldate = {2023-04-27},
  howpublished = {https://www.alibaba.com/product-detail/Intel-I350AM4-Chip-PCIE-X4-RJ45\_1600345054974.html?spm=a2756.order-detail-ta-bn-b.0.0.24142fc28OCUh7},
  file = {C:\Users\emilm\Zotero\storage\X8JU5XDF\Intel-I350AM4-Chip-PCIE-X4-RJ45_1600345054974.html}
}

@misc{itsfiveHowCrimpTNC2011,
  title = {How to {{Crimp TNC Male Connector}} Using {{Crimping}} Tool},
  author = {{Its Five}},
  year = {2011},
  month = sep,
  urldate = {2022-05-26},
  abstract = {MX TNC Plug Connector is coaxial RF connectors. It is used to connect the panels. MX TNC (threaded Neill-Concelman) connector is a threaded version of the BNC Connector. The connector has 50 OHMS impedance and operates best in the 0 --11 GHz frequency spectrum.  MX TNC has better performance than the BNC connector at microwave frequencies. MX TNC connector has been employed in a wide range of radio and wired applications. To Crimp TNC Male Connector You Require RG58 Cable Stripping Tool Crimping Tool TNC Male Connector Step1: Insert The Boot \& Crimping Ring Over The Cable Before Striping It. Step2: Strip The Cable According To The Length Of Connector. Step3: Strip The Center Core of Cable According To Tip's Length. Step4: Insert The Tip \& Solder It For Better Performance. Step5: Crimp The Connector. Product Link: http://mdrelectronics.com/tools.asp Please subscribe to our YOUTUBE Channel: MX Electronics (MDRElex) You can also Like our Facebook Page: http://www.facebook.com/mxelectronics This video is a complete guide, however if you have any questions feel free to comment or send us an email on info@mdrelectronics.com or visit our website www.mdrelectronics.com If You Have Any Complaints, Queries Or Suggestions, Call Us On   (022) 4253 6666 Or You Can Mail Us At info@mdrelectronics.com}
}

@techreport{itu-tHighEfficiencyVideo,
  title = {High Efficiency Video Coding ({{H}}.265)},
  author = {{ITU-T}},
  file = {C:\Users\emilm\Zotero\storage\J2YT8L2X\ITU-T - High efficiency video coding (H.265).pdf}
}

@misc{jcmbNTRIPNtripClientPy,
  title = {{{NTRIP}}/{{NtripClient}}.Py at Master {$\cdot$} Jcmb/{{NTRIP}}},
  author = {{jcmb}},
  journal = {GitHub},
  urldate = {2022-05-28},
  abstract = {NTRIP, VRS, IBSS Tools. Contribute to jcmb/NTRIP development by creating an account on GitHub.},
  howpublished = {https://github.com/jcmb/NTRIP},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\HJHFRG57\NTRIP.html}
}

@misc{Jetpack5FailedFlash2023,
  title = {Jetpack-5.0.2 - {{Failed}} to Flash {{NVMe}} with Custom Dtb and Kernel's {{Image}}},
  year = {2023},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-11},
  abstract = {Dear Community,  Below are the steps that I was using for flashing to NVMe drive mounted to the carrier board of Jetson Xavier NX :   Apply patches and cross-build the customized kernel :  I followed the instructions in the Developer Guide to apply my modification to official kernel and successfully built the Image, tegra194-p3668-0001-p3509-0000.dtb and nvgpu.ko for my custom carrier board.  Note : I haven't known yet how to update the loadable kernel modules with Jetpack-5.0.2 so I configured ...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/jetpack-5-0-2-failed-to-flash-nvme-with-custom-dtb-and-kernels-image/238855},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\Y426WBZM\3.html}
}

@misc{JetPackSDK2014,
  title = {{{JetPack SDK}}},
  year = {2014},
  month = oct,
  journal = {NVIDIA Developer},
  urldate = {2023-05-22},
  abstract = {Builds end-to-end accelerated AI applications and supports edge AI development.},
  howpublished = {https://developer.nvidia.com/embedded/jetpack},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\2Z7M5J43\jetpack.html}
}

@misc{JetpackUARTCTS2022,
  title = {Jetpack 5.0.2: {{UART}}: {{CTS}} Pin Not Working},
  shorttitle = {Jetpack 5.0.2},
  year = {2022},
  month = aug,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-06},
  abstract = {Hi,  I want to use uart's cts pin as an pps signal source,  and use ppscheck tool to monitor the signal, but it seem's not working. here is my debug info    connect the MCLK05 pin with UART1\_CTS pin on Expansion 40-pin Header       run ppscheck tool \& output level 0/1 on MCLK05 pin         here is the pinmux dts snapshot},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/jetpack-5-0-2-uart-cts-pin-not-working/224203},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\D938SASG\224203.html}
}

@misc{JetsonLinux352023,
  title = {Jetson {{Linux}} 35.3.1},
  year = {2023},
  month = mar,
  journal = {NVIDIA Developer},
  urldate = {2023-05-16},
  abstract = {NVIDIA{\textregistered} Jetson{\texttrademark} Linux Driver Package is the board support package for Jetson. It includes Linux Kernel, UEFI bootloader, NVIDIA drivers, flashing utilities, sample filesystem based on Ubuntu, and more for the Jetson platform. NVIDIA Jetson Linux 35.3.1 Jetson Linux 35.3.1 is a production quality release which brings support for Jetson Orin Nano Developer Kit, Jetson Orin NX 8GB, Jetson Orin Nano series and Jetson AGX Orin 64GB modules. It includes Linux Kernel 5.10, an Ubuntu 20.04 based root file system, a UEFI based bootloader, and OP-TEE as Trusted Execution Environment.},
  howpublished = {https://developer.nvidia.com/embedded/jetson-linux-r3531},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\Y8P2RLMM\jetson-linux-r3531.html}
}

@misc{JetsonLinuxAPI,
  title = {Jetson {{Linux API Reference}}: {{NvVideoEncoder Class Reference}} | {{NVIDIA Docs}}},
  urldate = {2023-05-01},
  howpublished = {https://docs.nvidia.com/jetson/l4t-multimedia/classNvVideoEncoder.html\#a627a6d299d913c10aa56720d4b690b32},
  file = {C:\Users\emilm\Zotero\storage\XD99M687\classNvVideoEncoder.html}
}

@misc{JetsonNanoPPS2021,
  title = {Jetson Nano: {{PPS GPIO}} Interrupt Not Getting Registered},
  shorttitle = {Jetson Nano},
  year = {2021},
  month = dec,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-16},
  abstract = {Hello,  We are using gpio(B,7) i.e. physical pin 18 in J41 as PPS input from ardusimple simpleRTK2B.  Below is our environment:  Board: Jetson Nano Developer kit B01 Model: P3450 L4T 32.6.1  Kernel 4.9 (L4T Driver Package (BSP) Sources)  I have followed steps in Enabling PPS on Jetson Nano and configured the device tree \& enabled PPS.  \# \# PPS support \# CONFIG\_PPS=y \# CONFIG\_PPS\_DEBUG is not set  \# \# PPS clients support \# CONFIG\_PPS\_CLIENT\_KTIMER=y CONFIG\_PPS\_CLIENT\_LDISC=y CONFIG\_PPS\_CLIENT\_GPI...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/jetson-nano-pps-gpio-interrupt-not-getting-registered/198085},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\K8H3IJZ6\198085.html}
}

@misc{JohnsonCinchBel,
  title = {Johnson - {{Cinch}} - {{Bel}} 2.92 Mm 40 {{GHz}}, 50 {{Ohm}}, 0.048" {{PCB Edge}} Connector | {{3D CAD Model Library}} | {{GrabCAD}}},
  urldate = {2022-05-15},
  howpublished = {https://grabcad.com/library/johnson-cinch-bel-2-92-mm-40-ghz-50-ohm-0-048-pcb-edge-connector-1},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\2H8LIT7G\johnson-cinch-bel-2-92-mm-40-ghz-50-ohm-0-048-pcb-edge-connector-1.html}
}

@misc{johnstonGeneratingGStreamerPipeline2018,
  title = {Generating {{GStreamer Pipeline Graphs}}},
  author = {Johnston, Phillip},
  year = {2018},
  month = feb,
  journal = {Embedded Artistry},
  urldate = {2023-05-25},
  abstract = {22 February 2018 by Phillip Johnston {\textbullet} Last updated 15 December 2021I've been working with GStreamer quite a bit recently. I often find it difficult to visualize the pipelines I'm working with, especially for complex pipelines involving multiple video streams. I've been manually creating my own pipeline graphs to keep the details straight. However, maintaining {\ldots} Continue reading "Generating GStreamer Pipeline Graphs"},
  howpublished = {https://embeddedartistry.com/blog/2018/02/22/generating-gstreamer-pipeline-graphs/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\ER3J9HBX\generating-gstreamer-pipeline-graphs.html}
}

@misc{jonathanraynerPlenoxelsVsNeRF2022,
  title = {Plenoxels vs. {{NeRF}} - {{Optimization Speed Comparison}}},
  author = {{Jonathan Rayner}},
  year = {2022},
  month = apr,
  urldate = {2023-11-21},
  abstract = {Clipped from video provided on the authors' project page https://alexyu.net/plenoxels/ . This clip was created purely for educational purposes - all credit goes to the authors Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa.}
}

@misc{jonathanwhitaker[@johnowhitaker]ExploringScanVR2023,
  type = {Tweet},
  title = {Exploring a Scan in {{VR}}. {{Stuttery}} When {{I}} Zoomed out (Tons of Splats) but Otherwise Pretty Smooth! {{This}} Was Captured Just in a Quick Circle of the Truck One Morning Walk, Using My Phone. {{https://github.com/clarte53/GaussianSplattingVRViewerUnity}} {{https://t.co/dIVGj5M1Jl}}},
  author = {{Jonathan Whitaker [@johnowhitaker]}},
  year = {2023},
  month = oct,
  journal = {Twitter},
  urldate = {2023-10-25},
  langid = {english}
}

@article{jonesCUDAOPTIMIZATIONTIPS,
  title = {{{CUDA OPTIMIZATION TIPS}}, {{TRICKS AND TECHNIQUES}}},
  author = {Jones, Stephen},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\9X6YDEVW\Jones - CUDA OPTIMIZATION TIPS, TRICKS AND TECHNIQUES.pdf}
}

@article{JPEG2023,
  title = {{{JPEG}}},
  year = {2023},
  month = feb,
  journal = {Wikipedia},
  urldate = {2023-02-16},
  abstract = {JPEG ( JAY-peg) is a commonly used method of lossy compression for digital images, particularly for those images produced by digital photography. The degree of compression can be adjusted, allowing a selectable tradeoff between storage size and image quality. JPEG typically achieves 10:1 compression with little perceptible loss in image quality. Since its introduction in 1992, JPEG has been the most widely used image compression standard in the world, and the most widely used digital image format, with several billion JPEG images produced every day as of 2015.The term "JPEG" is an acronym for the Joint Photographic Experts Group, which created the standard in 1992. JPEG was largely responsible for the proliferation of digital images and digital photos across the Internet, and later social media.JPEG compression is used in a number of image file formats. JPEG/Exif is the most common image format used by digital cameras and other photographic image capture devices; along with JPEG/JFIF, it is the most common format for storing and transmitting photographic images on the World Wide Web. These format variations are often not distinguished, and are simply called JPEG. The MIME media type for JPEG is image/jpeg, except in older Internet Explorer versions, which provides a MIME type of image/pjpeg when uploading JPEG images. JPEG files usually have a filename extension of .jpg or .jpeg. JPEG/JFIF supports a maximum image size of 65,535{\texttimes}65,535 pixels, hence up to 4 gigapixels for an aspect ratio of 1:1. In 2000, the JPEG group introduced a format intended to be a successor, JPEG 2000, but it was unable to replace the original JPEG as the dominant image standard.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1139412723},
  file = {C:\Users\emilm\Zotero\storage\9EIWDM5L\JPEG.html}
}

@misc{juanaragonRG59MakingTNC2020,
  title = {{{RG59 Making}} a {{TNC}} Connection},
  author = {{juan aragon}},
  year = {2020},
  month = may,
  urldate = {2022-05-26}
}

@misc{juicesshJuiceSSHFreeSSH,
  title = {{{JuiceSSH}} - {{Free SSH}} Client for {{Android}}},
  author = {{juiceSSH}},
  urldate = {2022-05-20},
  howpublished = {https://juicessh.com/},
  file = {C:\Users\emilm\Zotero\storage\UPL9JKKT\juicessh.com.html}
}

@misc{jupi007GcodeIconProposal2020,
  title = {Gcode Icon Proposal},
  author = {Jupi007},
  year = {2020},
  month = sep
}

@misc{kanakaAnswerDifferencesTCP2013,
  title = {Answer to "{{Differences}} between {{TCP}} Sockets and Web Sockets, One More Time"},
  author = {{kanaka}},
  year = {2013},
  month = jun,
  journal = {Stack Overflow},
  urldate = {2023-06-03},
  file = {C:\Users\emilm\Zotero\storage\26KJAFE7\differences-between-tcp-sockets-and-web-sockets-one-more-time.html}
}

@misc{kangalowSPIJetsonUsing2020,
  title = {{{SPI}} on {{Jetson}} - {{Using Jetson-IO}}},
  author = {{kangalow}},
  year = {2020},
  month = may,
  journal = {JetsonHacks},
  urldate = {2022-05-20},
  abstract = {Remember when you had to jump through all sorts of hoops to get your NVIDIA Jetson NVIDIA Kit to be able to understand Serial Protocol Interface (SPI) devices? Those days be gone. Looky here: Background The introduction of JetPack 4.3 brings with it a new tool, Jetson-IO. All of the Read more ...},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\SWJKQ8KD\spi-on-jetson-using-jetson-io.html}
}

@misc{kartchnerAutoTowersGeneratorUltimaker2022,
  title = {{{AutoTowers Generator}} - {{Ultimaker Cura Marketplace}}},
  author = {Kartchner, Brad},
  year = {2022},
  month = dec,
  urldate = {2023-06-03},
  file = {C:\Users\emilm\Zotero\storage\VNF7D3HY\AutoTowersGenerator.html}
}

@misc{kartverketBrukerveiledningPosisjonstjenester,
  title = {{Brukerveiledning posisjonstjenester}},
  author = {Kartverket},
  journal = {Kartverket.no},
  urldate = {2022-05-02},
  howpublished = {https://kartverket.no/til-lands/posisjon/brukerveiledning-posisjonstjenester},
  langid = {norwegianbokmal},
  file = {C:\Users\emilm\Zotero\storage\8XBUL86E\brukerveiledning-posisjonstjenester.html}
}

@misc{kartverketGuideCPOS,
  title = {Guide to {{CPOS}}},
  author = {Kartverket},
  journal = {Kartverket.no},
  urldate = {2022-05-02},
  howpublished = {https://www.kartverket.no/en/on-land/posisjon/guide-to-cpos},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\ENVAZTKJ\guide-to-cpos.html}
}

@misc{kartverketNorgeskart,
  title = {Norgeskart},
  author = {Kartverket},
  urldate = {2023-05-15},
  howpublished = {https://www.norgeskart.no/},
  file = {C:\Users\emilm\Zotero\storage\MGNDH4C5\www.norgeskart.no.html}
}

@misc{karumbunathanNVIDIAJetsonAGX2022,
  title = {{{NVIDIA Jetson AGX Orin Series Technical Brief}}},
  author = {Karumbunathan, Leela S.},
  year = {2022},
  month = jul,
  file = {C:\Users\emilm\Zotero\storage\YUIWXVZJ\nvidia-jetson-agx-orin-technical-brief.pdf}
}

@article{kerbl3Dgaussians,
  title = {{{3D}} Gaussian Splatting for Real-Time Radiance Field Rendering},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  year = {2023},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {42},
  number = {4}
}

@article{kerbl3DGaussianSplatting2023,
  title = {{{3D}} Gaussian Splatting for Real-Time Radiance Field Rendering},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimk{\"u}hler, Thomas and Drettakis, George},
  year = {2023},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {42},
  number = {4},
  file = {C:\Users\emilm\Zotero\storage\IPAYW2ZT\Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field.pdf}
}

@article{KernelOperatingSystem2023,
  title = {Kernel (Operating System)},
  year = {2023},
  month = may,
  journal = {Wikipedia},
  urldate = {2023-05-16},
  abstract = {The kernel is a computer program at the core of a computer's operating system and generally has complete control over everything in the system. It is the portion of the operating system code that is always resident in memory and facilitates interactions between hardware and software components. A full kernel controls all hardware resources (e.g. I/O, memory, cryptography) via device drivers, arbitrates conflicts between processes concerning such resources, and optimizes the utilization of common resources e.g. CPU \& cache usage, file systems, and network sockets. On most systems, the kernel is one of the first programs loaded on startup (after the bootloader). It handles the rest of startup as well as memory, peripherals, and input/output (I/O) requests from software, translating them into data-processing instructions for the central processing unit. The critical code of the kernel is usually loaded into a separate area of memory, which is protected from access by application software or other less critical parts of the operating system. The kernel performs its tasks, such as running processes, managing hardware devices such as the hard disk, and handling interrupts, in this protected kernel space. In contrast, application programs such as browsers, word processors, or audio or video players  use a separate area of memory, user space. This separation prevents user data and kernel data from interfering with each other and causing instability and slowness, as well as preventing malfunctioning applications from affecting other applications or crashing the entire operating system. Even in systems where the kernel is included in application address spaces, memory protection is used to prevent unauthorized applications from modifying the kernel. The kernel's interface is a low-level abstraction layer. When a process requests a service from the kernel, it must invoke a system call, usually through a wrapper function. There are different kernel architecture designs. Monolithic kernels run entirely in a single address space with the CPU executing in supervisor mode, mainly for speed. Microkernels run most but not all of their services in user space, like user processes do, mainly for resilience and modularity. MINIX 3 is a notable example of microkernel design. Instead, the Linux kernel is monolithic, although it is also modular, for it can insert and remove loadable kernel modules at runtime. This central component of a computer system is responsible for executing programs. The kernel takes responsibility for deciding at any time which of the many running programs should be allocated to the processor or processors.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1154807052},
  file = {C:\Users\emilm\Zotero\storage\3XVCMZ2E\Kernel_(operating_system).html}
}

@misc{kernLineProfilerKernprof2023,
  title = {Line\_profiler and Kernprof},
  author = {Kern, Robert},
  year = {2023},
  month = jun,
  urldate = {2023-06-26},
  abstract = {(OLD REPO) Line-by-line profiling for Python - Current repo -{$>$}}
}

@misc{kovacevicStreamLiveVideo2020,
  title = {Stream Live Video to Browser Using {{GStreamer}}},
  author = {Kovacevic, Dusan},
  year = {2020},
  month = jun,
  journal = {4Young Padawans},
  urldate = {2023-05-26},
  abstract = {Fully functional example of how to capture live video (camera or screen) with Gstreamer and prepare live stream for viewing in browser. Gstreamer CLI and WEB page HTML code examples are included.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\5VUQSVRC\stream-live-video-to-browser-using-gstreamer.html}
}

@misc{kruseUkrainasSjodronerUtvider2023,
  title = {{Ukrainas sj{\o}droner utvider krigen i Svartehavet}},
  author = {Kruse, Jan Espen},
  year = {2023},
  month = aug,
  journal = {NRK},
  urldate = {2023-09-12},
  abstract = {Den siste tiden har Ukraina sendt sj{\o}g{\aa}ende droner mot en rekke russiske m{\aa}l i Svartehavet. En norsk ekspert sier at taktikken har v{\ae}rt vellykket.},
  chapter = {nyheter},
  howpublished = {https://www.nrk.no/urix/ukrainas-sjodroner-utvider-krigen-i-svartehavet-1.16508593},
  langid = {norwegianbokmal},
  file = {C:\Users\emilm\Zotero\storage\IUTQC4T8\ukrainas-sjodroner-utvider-krigen-i-svartehavet-1.html}
}

@article{kuphaldtLessonsElectricCircuits2007,
  title = {Lessons {{In Electric Circuits}}},
  author = {Kuphaldt, Tony R},
  year = {2007},
  month = nov,
  volume = {Volume IV -- Digital},
  pages = {517},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\7TXCZYK8\Kuphaldt - Lessons In Electric Circuits, Volume IV -- Digita.pdf}
}

@misc{kurzgesagt-inanutshell3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting Demo}}},
  author = {{Kurzgesagt {\textendash} In a Nutshell}},
  year = {2023},
  month = sep,
  urldate = {2023-11-21},
  abstract = {Demonstration video of a new aproach to computer generated 3D modelling.  Its starting point is the Neural Radiance Field (NeRF) technology, which has been cleverly managed to dismantle by optimizing the heavy neural network calculation and replace it with the lighter Gaussian Splatting method. In this way, three-dimensional models can be presented in real time, as very accurate images that closely imitate their source material. Content footage for these 3D models was shot with iPhone XR smart phone. Specs: These samples were rendered with Graphics card Nvidia RTX 3070 8Gb Vram PC: Asus ROG Ryzen 7 64Gb ram  \#gaussiansplatting \#nerf \#3dscanning}
}

@article{kwanComparisonDeepLearning2019,
  title = {Comparison of {{Deep Learning}} and {{Conventional Demosaicing Algorithms}} for {{Mastcam Images}}},
  author = {Kwan, Chiman and Chou, Bryan and III, James},
  year = {2019},
  month = mar,
  journal = {Electronics},
  volume = {8},
  pages = {308},
  doi = {10.3390/electronics8030308},
  abstract = {Bayer pattern filters have been used in many commercial digital cameras. In National Aeronautics and Space Administration's (NASA) mast camera (Mastcam) imaging system, onboard the Mars Science Laboratory (MSL) rover Curiosity, a Bayer pattern filter is being used to capture the RGB (red, green, and blue) color of scenes on Mars. The Mastcam has two cameras: left and right. The right camera has three times better resolution than that of the left. It is well known that demosaicing introduces color and zipper artifacts. Here, we present a comparative study of demosaicing results using conventional and deep learning algorithms. Sixteen left and 15 right Mastcam images were used in our experiments. Due to a lack of ground truth images for Mastcam data from Mars, we compared the various algorithms using a blind image quality assessment model. It was observed that no one algorithm can work the best for all images. In particular, a deep learning-based algorithm worked the best for the right Mastcam images and a conventional algorithm achieved the best results for the left Mastcam images. Moreover, subjective evaluation of five demosaiced Mastcam images was also used to compare the various algorithms.},
  file = {C:\Users\emilm\Zotero\storage\YWQ3TQD6\Kwan et al. - 2019 - Comparison of Deep Learning and Conventional Demos.pdf}
}

@article{lambWhyRodsCones2016,
  title = {Why Rods and Cones?},
  author = {Lamb, T D},
  year = {2016},
  month = feb,
  journal = {Eye},
  volume = {30},
  number = {2},
  pages = {179--185},
  issn = {0950-222X},
  doi = {10.1038/eye.2015.236},
  urldate = {2023-06-14},
  abstract = {Under twenty-first-century metropolitan conditions, almost all of our vision is mediated by cones and the photopic system, yet cones make up barely 5\% of our retinal photoreceptors. This paper looks at reasons why we additionally possess rods and a scotopic system, and asks why rods comprise 95\% of our retinal photoreceptors. It considers the ability of rods to reliably signal the arrival of individual photons of light, as well as the ability of the retina to process these single-photon signals, and it discusses the advantages that accrue. Drawbacks in the arrangement, including the very slow dark adaptation of scotopic vision, are also considered. Finally, the timing of the evolution of cone and rod photoreceptors, the retina, and the camera-style eye is summarised.},
  pmcid = {PMC4763127},
  pmid = {26563661},
  file = {C:\Users\emilm\Zotero\storage\IRIAL97D\Lamb - 2016 - Why rods and cones.pdf}
}

@misc{laneGStreamerPipelineSamples2020,
  title = {{{GStreamer Pipeline Samples}}},
  author = {Lane, Adrian},
  year = {2020},
  month = aug,
  journal = {Gist},
  urldate = {2023-05-27},
  abstract = {GStreamer Pipeline Samples. GitHub Gist: instantly share code, notes, and snippets.},
  howpublished = {https://gist.github.com/liviaerxin/bb34725037fd04afa76ef9252c2ee875},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\B8TUWCQE\cda96fb07a34300cdb2c0e314c14df0a.html}
}

@misc{lanzaniInstallNvidiaDeepstream2023,
  title = {Install {{Nvidia Deepstream}} 6.0.1 {{With Python Bindings}} on {{Jetson}}.},
  author = {Lanzani, Federico},
  year = {2023},
  month = may,
  journal = {Medium},
  urldate = {2023-05-30},
  abstract = {And start developing!},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\F98L7A8G\nvidia-deepstream-with-python-bindings-on-jetson-f9ffdcb16b06.html}
}

@article{laprayExploitingRedundancyColorpolarization2020,
  title = {Exploiting Redundancy in Color-Polarization Filter Array Images for Dynamic Range Enhancement},
  author = {Lapray, Pierre-Jean},
  year = {2020},
  month = oct,
  journal = {Optics Letters},
  volume = {45},
  number = {19},
  pages = {5530--5533},
  publisher = {{Optica Publishing Group}},
  issn = {1539-4794},
  doi = {10.1364/OL.398258},
  urldate = {2023-06-20},
  abstract = {Color-polarization filter array (CPFA) sensors are able to capture linear polarization and color information in a single shot. For a scene that contains a high dynamic range of irradiance and polarization signatures, some pixel values approach the saturation and noise levels of the sensor. The most common CPFA configuration is overdetermined, and contains four different linear polarization analyzers. Assuming that not all pixel responses are equally reliable in CPFA channels, one can therefore apply the high dynamic range imaging scheme to improve the Stokes estimation from a single CPFA image. Here I present this alternative methodology and show qualitative and quantitative results on real data.},
  copyright = {\&\#169; 2020 Optical Society of America},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KBDKICY3\ol-45-19-5530.pdf}
}

@article{laprayFPGAbasedPipelineMicropolarizer2018,
  title = {An {{FPGA-based}} Pipeline for Micropolarizer Array Imaging},
  author = {Lapray, Pierre-Jean and Gendre, Luc and Foulonneau, Alban and Bigu{\'e}, Laurent},
  year = {2018},
  month = may,
  journal = {International Journal of Circuit Theory and Applications},
  doi = {10.1002/cta.2477},
  abstract = {The enhancement of current camera performances, in terms of framerate, image resolution, and pixel width, has direct consequences on the amount of resources needed to process video data. Stokes imaging permits to estimate polarization of light and create multiple polarization descriptors of the scene. Therefore, such video cameras need fast processing for critical applications like overseeing, defect detection or surface characterization. A field-programmable gate array hardware implementation of Stokes processing is presented here that embeds dedicated pipeline for micropolarizer array sensors. An optimized fixed-point pipeline is used to compute polarimetric images, ie, Stokes vector, degree of polarization, and angle of polarization. Simulation and experimental studies are done. The hardware design contains parallel processing, low latency, and low power and could meet actual real-time and embeddable requirements for smart camera systems.},
  file = {C:\Users\emilm\Zotero\storage\NHLMX2CA\Lapray et al. - 2018 - An FPGA-based pipeline for micropolarizer array im.pdf}
}

@inproceedings{lebedaFixingLocallyOptimized2012,
  title = {Fixing the Locally Optimized {{RANSAC}}},
  author = {Lebeda, Karel and Matas, Jiri and Chum, Ondrej},
  year = {2012},
  month = sep,
  doi = {10.5244/C.26.95},
  abstract = {The paper revisits the problem of local optimization for RANSAC. Improvements of the LO-RANSAC procedure are proposed: a use of truncated quadratic cost function, an introduction of a limit on the number of inliers used for the least squares computation and several implementation issues are addressed. The implementation is made publicly available. Extensive experiments demonstrate that the novel algorithm called LO + -RANSAC is (1) very stable (almost non-random in nature), (2) very precise in a broad range of con-ditions, (3) less sensitive to the choice of inlier-outlier threshold and (4) it offers a sig-nificantly better starting point for bundle adjustment than the Gold Standard method advocated in the Hartley-Zisserman book.},
  file = {C:\Users\emilm\Zotero\storage\6HL7IJEP\Lebeda et al. - 2012 - Fixing the locally optimized RANSAC.pdf}
}

@misc{legrandCodeFrequencyRichlegrand,
  title = {Code Frequency {$\cdot$} Richlegrand/Dash\_devices},
  author = {LeGrand, Rich},
  urldate = {2023-06-02},
  abstract = {A fork of plotly/dash to help Dash deal with devices. - Code frequency {$\cdot$} richlegrand/dash\_devices},
  file = {C:\Users\emilm\Zotero\storage\RLLZJFXV\dash_devices.html}
}

@misc{lekAddFlaskRequest2022,
  title = {Add Flask.Request to the Patched Objects by Vlabakje {$\cdot$} {{Pull Request}} \#1 {$\cdot$} Snehilvj/Async-Dash},
  author = {Lek, Joost},
  year = {2022},
  month = jul,
  journal = {GitHub},
  urldate = {2023-06-03},
  abstract = {needed for dash pages functionality},
  howpublished = {https://github.com/snehilvj/async-dash/pull/1},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\4XV9DZ5W\1.html}
}

@misc{lemireWhatSpaceOverhead2019,
  title = {What Is the Space Overhead of {{Base64}} Encoding?},
  author = {Lemire, Author Daniel},
  year = {2019},
  month = jan,
  journal = {Daniel Lemire's blog},
  urldate = {2023-05-26},
  abstract = {Many Internet formats from email (MIME) to the Web (HTML/CSS/JavaScript) are text-only. If you send an image or executable file by email, it often first gets encoded using base64. The trick behind base64 encoding is that we use 64 different ASCII characters including all letters, upper and lower case, and all numbers. Not all non-textual {\ldots} Continue reading What is the space overhead of Base64 encoding?},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\GR9AJG3C\what-is-the-space-overhead-of-base64-encoding.html}
}

@misc{lexfridmanGuidoVanRossum2022,
  title = {Guido van {{Rossum}}: {{Python}} and the {{Future}} of {{Programming}} | {{Lex Fridman Podcast}} \#341},
  shorttitle = {Guido van {{Rossum}}},
  author = {{Lex Fridman}},
  year = {2022},
  month = nov,
  urldate = {2023-06-26},
  abstract = {Guido van Rossum is the creator of Python programming language. Please support this podcast by checking out our sponsors: - GiveDirectly: https://givedirectly.org/lex to get gift matched up to \$1000 - Eight Sleep: https://www.eightsleep.com/lex to get special savings - Fundrise: https://fundrise.com/lex - InsideTracker: https://insidetracker.com/lex to get 20\% off - Athletic Greens: https://athleticgreens.com/lex to get 1 month of fish oil EPISODE LINKS: Guido's Twitter: https://twitter.com/gvanrossum  Guido's Website: https://gvanrossum.github.io/ Python's Website: https://python.org PODCAST INFO: Podcast website: https://lexfridman.com/podcast Apple Podcasts: https://apple.co/2lwqZIr Spotify: https://spoti.fi/2nEwCF8 RSS: https://lexfridman.com/feed/podcast/ Full episodes playlist: ~~~{\textbullet}~Lex~Fridman~Podcast~~ Clips playlist: ~~~{\textbullet}~Lex~Fridman~Podca...~~ OUTLINE: 0:00 - Introduction 0:48 - CPython 6:01 - Code readability 10:22 - Indentation 26:58 - Bugs 38:26 - Programming fads 53:37 - Speed of Python 3.11 1:18:31 - Type hinting 1:23:49 - mypy 1:29:05 - TypeScript vs JavaScript 1:45:05 - Best IDE for Python 1:55:05 - Parallelism 2:12:58 - Global Interpreter Lock (GIL) 2:22:36 - Python 4.0 2:34:53 - Machine learning 2:44:35 - Benevolent Dictator for Life (BDFL) 2:56:11 - Advice for beginners 3:02:43 - GitHub Copilot 3:06:10 - Future of Python SOCIAL: - Twitter: https://twitter.com/lexfridman - LinkedIn: https://www.linkedin.com/in/lexfridman - Facebook: https://www.facebook.com/lexfridman - Instagram: https://www.instagram.com/lexfridman - Medium: https://medium.com/@lexfridman - Reddit: https://reddit.com/r/lexfridman - Support on Patreon: https://www.patreon.com/lexfridman}
}

@inproceedings{liImageDemosaicingSystematic2008,
  title = {Image Demosaicing: A Systematic Survey},
  shorttitle = {Image Demosaicing},
  booktitle = {Visual {{Communications}} and {{Image Processing}} 2008},
  author = {Li, Xin and Gunturk, Bahadir and Zhang, Lei},
  year = {2008},
  month = jan,
  volume = {6822},
  pages = {489--503},
  publisher = {{SPIE}},
  doi = {10.1117/12.766768},
  urldate = {2023-05-23},
  abstract = {Image demosaicing is a problem of interpolating full-resolution color images from so-called color-filter-array (CFA) samples. Among various CFA patterns, Bayer pattern has been the most popular choice and demosaicing of Bayer pattern has attracted renewed interest in recent years partially due to the increased availability of source codes/executables in response to the principle of "reproducible research". In this article, we provide a systematic survey of over seventy published works in this field since 1999 (complementary to previous reviews\textsuperscript{22, 67}). Our review attempts to address important issues to demosaicing and identify fundamental differences among competing approaches. Our findings suggest most existing works belong to the class of sequential demosaicing - i.e., luminance channel is interpolated first and then chrominance channels are reconstructed based on recovered luminance information. We report our comparative study results with a collection of eleven competing algorithms whose source codes or executables are provided by the authors. Our comparison is performed on two data sets: Kodak PhotoCD (popular choice) and IMAX high-quality images (more challenging). While most existing demosaicing algorithms achieve good performance on the Kodak data set, their performance on the IMAX one (images with varying-hue and high-saturation edges) degrades significantly. Such observation suggests the importance of properly addressing the issue of mismatch between assumed model and observation data in demosaicing, which calls for further investigation on issues such as model validation, test data selection and performance evaluation.},
  file = {C:\Users\emilm\Zotero\storage\NA5QSET3\Li et al. - 2008 - Image demosaicing a systematic survey.pdf}
}

@article{lindebergScaleInvariantFeature2012,
  title = {Scale {{Invariant Feature Transform}}},
  author = {Lindeberg, Tony},
  year = {2012},
  journal = {Scholarpedia},
  volume = {7},
  number = {5},
  pages = {10491},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.10491},
  urldate = {2023-09-14},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\4AFKLHJL\Lindeberg - 2012 - Scale Invariant Feature Transform.pdf}
}

@misc{LineProfilerKernprof2023,
  title = {Line\_profiler and Kernprof},
  year = {2023},
  month = jun,
  urldate = {2023-06-26},
  abstract = {Line-by-line profiling for Python},
  howpublished = {OpenPyUtils}
}

@book{lingUniversityPhysicsVolume2016,
  title = {University {{Physics Volume}} 3},
  author = {Ling, Samuel J. and Sanny, Jeff and Moebs, William},
  year = {2016},
  series = {University {{Physics}}},
  volume = {3},
  publisher = {{Rice University}},
  address = {{Rice University}},
  isbn = {978-1-947172-22-7},
  file = {C:\Users\emilm\Zotero\storage\T2KY75CP\University Physics Volume 3.pdf}
}

@misc{linUsingCUDAWarpLevel2018,
  title = {Using {{CUDA Warp-Level Primitives}}},
  author = {Lin, Yuan and Grover, Vinod},
  year = {2018},
  month = jan,
  journal = {NVIDIA Technical Blog},
  urldate = {2023-05-24},
  abstract = {NVIDIA GPUs execute groups of threads known as warps in SIMT (Single Instruction, Multiple Thread) fashion. Many CUDA programs achieve high performance by taking advantage of warp execution.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\PS85M83K\using-cuda-warp-level-primitives.html}
}

@misc{LinuxRtspclientsinkTest,
  title = {Linux - Rtspclientsink Test Pipeline from Command Line - {{Stack Overflow}}},
  urldate = {2023-04-26},
  howpublished = {https://stackoverflow.com/questions/69098720/rtspclientsink-test-pipeline-from-command-line},
  file = {C:\Users\emilm\Zotero\storage\SGX3WIC5\rtspclientsink-test-pipeline-from-command-line.html}
}

@article{littleFORUMLittleLaw2011,
  title = {{{OR FORUM}}{\textemdash}{{Little}}'s {{Law}} as {{Viewed}} on {{Its}} 50th {{Anniversary}}},
  author = {Little, John D. C.},
  year = {2011},
  month = jun,
  journal = {Operations Research},
  volume = {59},
  number = {3},
  pages = {536--549},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.1110.0940},
  urldate = {2023-05-15},
  abstract = {Fifty years ago, the author published a paper in Operations Research with the title, ``A proof for the queuing formula: L = {$\lambda$}W'' [Little, J. D. C. 1961. A proof for the queuing formula: L = {$\lambda$}W. Oper. Res. 9(3) 383{\textendash}387]. Over the years, L = {$\lambda$}W has become widely known as ``Little's Law.'' Basically, it is a theorem in queuing theory. It has become well known because of its theoretical and practical importance. We report key developments in both areas with the emphasis on practice. In the latter, we collect new material and search for insights on the use of Little's Law within the fields of operations management and computer architecture.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\G6AE7E7N\Little - 2011 - OR FORUM—Little's Law as Viewed on Its 50th Annive.pdf}
}

@misc{LongtermVisualLocalization,
  title = {Long-Term Visual Localization Revisited - {{Google Search}}},
  urldate = {2022-11-27},
  howpublished = {https://www.google.com/search?q=long-term+visual+localization+revisited\&rlz=1C1CHBF\_enNO1002NO1002\&oq=Long-term+visual+localization+revisited\&aqs=chrome.0.0i512j0i390l2.325j0j4\&sourceid=chrome\&ie=UTF-8}
}

@misc{lordHdparmLinuxManual2018,
  title = {Hdparm(8) - {{Linux}} Manual Page},
  author = {Lord, Mark},
  year = {2018},
  month = mar,
  urldate = {2023-05-22},
  file = {C:\Users\emilm\Zotero\storage\9W29RZ2F\hdparm.8.html}
}

@misc{lorenzkuhnHereAre172021,
  type = {Reddit {{Post}}},
  title = {[{{D}}] {{Here}} Are 17 Ways of Making {{PyTorch}} Training Faster {\textendash} What Did {{I}} Miss?},
  author = {{lorenzkuhn}},
  year = {2021},
  month = jan,
  journal = {r/MachineLearning},
  urldate = {2022-10-28},
  keywords = {ml}
}

@book{loretoRealTimeCommunicationWebRTC2014,
  title = {Real-{{Time Communication}} with {{WebRTC}}: {{Peer-to-Peer}} in the {{Browser}}},
  shorttitle = {Real-{{Time Communication}} with {{WebRTC}}},
  author = {Loreto, Salvatore and Romano, Simon Pietro},
  year = {2014},
  month = apr,
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Deliver rich audio and video real-time communication and peer-to-peer data exchange right in the browser, without the need for proprietary plug-ins. This concise hands-on guide shows you how to use the emerging Web Real-Time Communication (WebRTC) technology to build a browser-to-browser application, piece by piece.The authors' learn-by-example approach is perfect for web programmers looking to understand real-time communication, and telecommunications architects unfamiliar with HTML5 and JavaScript-based client-server web programming. You'll use a ten-step recipe to create a complete WebRTC system, with exercises that you can apply to your own projects.Tour the WebRTC development cycle and trapezoid architectural modelUnderstand how and why VoIP is shifting from standalone functionality to a browser componentUse mechanisms that let client-side web apps interact with browsers through the WebRTC APITransfer streaming data between browser peers with the RTCPeerConnection APICreate a signaling channel between peers for setting up a WebRTC sessionPut everything together to create a basic WebRTC system from scratchLearn about conferencing, authorization, and other advanced WebRTC features},
  googlebooks = {RPNfAwAAQBAJ},
  isbn = {978-1-4493-7185-2},
  langid = {english},
  keywords = {Computers / Internet / Web Programming,Computers / Internet / Web Services \& APIs}
}

@misc{loshinWhatSSHSecure,
  title = {What Is {{SSH}} ({{Secure Shell}}) and {{How Does}} It {{Work}}? {{Definition}} from {{TechTarget}}},
  shorttitle = {What Is {{SSH}} ({{Secure Shell}}) and {{How Does}} It {{Work}}?},
  author = {Loshin, Peter},
  journal = {SearchSecurity},
  urldate = {2022-05-19},
  abstract = {Learn how the network protocol SSH, or Secure Shell, ensures secure access to a computer over an unsecured network. Read its use cases, history and more.},
  howpublished = {https://www.techtarget.com/searchsecurity/definition/Secure-Shell},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\UISXZTX2\Secure-Shell.html}
}

@misc{LosslessVideoEncoding2022,
  title = {Lossless Video Encoding on {{Jetson Xavier}} Using {{GStreamer}}},
  year = {2022},
  month = mar,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-12},
  abstract = {I am trying to save a video on Jetson Xavier NX using OpenCV and Gstreamer.  I have an 8UC3 cv::Mat that I am writing to a mp4 video using a GStreamer pipeline. The pipeline uses nvv4l2h264enc bitrate=8000000 insert-aud=1 insert-sps-pps=1  to encod and uses UYVY format. However, I understand I can be losing information here and I really can't afford to.  Are there ways to make sure the compression is lossless? I can handle larger sizes of the file.},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/lossless-video-encoding-on-jetson-xavier-using-gstreamer/205456},
  langid = {english}
}

@misc{lucascaroloSolidWorks2022Vs2021,
  title = {{{SolidWorks}} 2022 vs {{Fusion}} 360: {{The Differences}}},
  shorttitle = {{{SolidWorks}} 2022 vs {{Fusion}} 360},
  author = {Lucas Carolo},
  year = {2021},
  month = aug,
  journal = {All3DP},
  urldate = {2022-05-18},
  abstract = {SolidWorks vs. Fusion 360: Dive straight in to find out all about the differences and which CAD program is best for your needs.},
  howpublished = {https://all3dp.com/2/fusion-360-vs-solidworks-cad-software-compared-side-by-side/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\TXVF87PC\fusion-360-vs-solidworks-cad-software-compared-side-by-side.html}
}

@inproceedings{lucasIterativeImageRegistration1981,
  title = {An {{Iterative Image Registration Technique}} with an {{Application}} to {{Stereo Vision}}},
  booktitle = {{{IJCAI}}'81: 7th International Joint Conference on {{Artificial}} Intelligence},
  author = {Lucas, Bruce D. and Kanade, Takeo},
  year = {1981},
  month = aug,
  volume = {2},
  pages = {674},
  urldate = {2023-06-14},
  abstract = {Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\5G2RSLW4\Lucas and Kanade - 1981 - An Iterative Image Registration Technique with an .pdf}
}

@misc{lucidvisionlabs3DDepthSurface2021,
  title = {{{3D Depth}} from {{Surface Normals}} Using {{Polarization Camera}} ({{Shape-from-Polarization}}, {{SfP}}) - {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  year = {2021},
  month = jan,
  urldate = {2023-12-04},
  abstract = {Inspecting object shape using surface normals calculated from LUCID's polarization camera (Shape-from-Polarization, SfP) with sample code.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\75ZLBWQQ\3d-depth-from-polarization-sfp.html}
}

@misc{lucidvisionlabsAppNoteBandwidth2022,
  title = {App {{Note}}: {{Bandwidth}} Sharing in Multi-Camera Systems | {{LUCID Support}} \& {{Help}}},
  shorttitle = {App {{Note}}},
  author = {LUCID Vision Labs},
  year = {2022},
  month = apr,
  urldate = {2023-04-27},
  abstract = {This application note will discuss interleaving of packets to address collision of packets and will shed light on various methods for synchronization of cameras.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\MPKQY3FB\app-note-bandwidth-sharing-in-multi-camera-systems.html}
}

@misc{lucidvisionlabsArenaSoftwareDevelopment2020,
  title = {Arena {{Software Development Kit}} ({{SDK}}) {{Documentation}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = may,
  urldate = {2023-05-02},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\DCANQQFC\arena-sdk-documentation.html}
}

@misc{lucidvisionlabsDownloadsLUCIDVision,
  title = {Downloads | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  urldate = {2022-05-17},
  abstract = {Download the latest LUCID Arena SDK package along with camera firmware files and technical documentation files.},
  howpublished = {https://thinklucid.com/downloads-hub/},
  langid = {american},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\QE8FMGJ8\downloads-hub.html}
}

@misc{lucidvisionlabsJumboFramesLUCID2020,
  title = {Jumbo {{Frames}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = may,
  urldate = {2023-04-27},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\LT6K79GM\jumbo-frames.html}
}

@techreport{lucidvisionlabsLUCIDGoingPolarizedWhitePaper2018,
  title = {{{LUCID-Going-Polarized-White-Paper}}},
  author = {LUCID Vision Labs},
  year = {2018},
  file = {C:\Users\emilm\Zotero\storage\DKK7MC9Z\LUCID-Going-Polarized-White-Paper.pdf}
}

@misc{lucidvisionlabsPixelFormatsLUCID2020,
  title = {Pixel {{Formats}} for {{LUCID Machine Vision Cameras}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = jul,
  urldate = {2023-06-17},
  abstract = {Color, Mono, Polarization, 3D, and Linescan pixel formats.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\UUPGK3CY\pixel-formats.html}
}

@misc{lucidvisionlabsPolarizationExplainedSony2018,
  title = {Polarization {{Explained}}: {{The Sony Polarized Sensor}} - {{LUCID Vision Labs}}},
  shorttitle = {Polarization {{Explained}}},
  author = {LUCID Vision Labs},
  year = {2018},
  month = jan,
  urldate = {2023-05-03},
  abstract = {Sony expands its sensor technology leadership by developing their first polarized sensor with their Polarsens technology. This polarization sensor features an innovative 4 pixel block design that includes 4 unique angled polarizers. Learn more about Sony's IMX250MZR CMOS sensor and its use in our Phoenix and Triton polarized cameras.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\BVGQPJJ6\polarization-explained-sony-polarized-sensor.html}
}

@misc{lucidvisionlabsReceiveBuffers2020,
  title = {Receive {{Buffers}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = may,
  urldate = {2023-04-27},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\KYT5DFY5\receive-buffers.html}
}

@misc{lucidvisionlabsSettingLLAEthernet2020,
  title = {Setting {{Up LLA}} on the {{Ethernet Adapter}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = may,
  urldate = {2023-04-27},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\2E5YVDDG\setting-up-lla-on-the-ethernet-adapter.html}
}

@misc{lucidvisionlabsTriton0MPPolarization,
  title = {Triton 5.{{0MP Polarization Camera}}, {{Sony}}'s {{IMX264MZR}} / {{MYR CMOS}} | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  urldate = {2022-05-30},
  abstract = {The Triton TRI050S1-P camera features Sony's low-cost IMX264MZR \& IMX264MYR CMOS polarized sensors with their Polarsens on-chip polarization technology. 5 MP global shutter, 2/3", 3.45{\textmu}m, up to 24 FPS over GigE . The sensor has 4 different directional polarizing filters (0{\textdegree}, 90{\textdegree}, 45{\textdegree}, and 135{\textdegree}) on every four pixels.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\IJN7G325\triton-5-0-mp-polarization-model-imx264mzrmyr.html}
}

@misc{lucidvisionlabsTritonIndustrialGigE,
  title = {Triton {{Industrial GigE Camera}} | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  urldate = {2022-05-17},
  abstract = {The Triton Machine Vision camera - Active sensor alignment for superior optical performance. Lightweight and compact with a 29 x 29 mm size. IP67 Ready. GigE Vision and GenICam compliant machine vision camera designed for all industrial environments with exceptional price performance.},
  langid = {american},
  keywords = {part},
  file = {C:\Users\emilm\Zotero\storage\4GW853EL\triton-gige-machine-vision.html}
}

@misc{lucidvisionlabsTritonMPPolarized2020,
  title = {Triton 5.0 {{MP Polarized Technical Reference Manual}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  year = {2020},
  month = apr,
  urldate = {2023-05-15},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\5B6HEK3H\triton-tri050-pq-polarized.html}
}

@article{luColorFilterArray2003,
  title = {Color Filter Array Demosaicking: New Method and Performance Measures},
  shorttitle = {Color Filter Array Demosaicking},
  author = {Lu, Wenmiao and Tan, Yap-Peng},
  year = {2003},
  month = oct,
  journal = {IEEE Transactions on Image Processing},
  volume = {12},
  number = {10},
  pages = {1194--1210},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.816004},
  abstract = {Single-sensor digital cameras capture imagery by covering the sensor surface with a color filter array (CFA) such that each sensor pixel only samples one of three primary color values. To render a full-color image, an interpolation process, commonly referred to as CFA demosaicking, is required to estimate the other two missing color values at each pixel. In this paper, we present two contributions to the CFA demosaicking: a new and improved CFA demosaicking method for producing high quality color images and new image measures for quantifying the performance of demosaicking methods. The proposed demosaicking method consists of two successive steps: an interpolation step that estimates missing color values by exploiting spatial and spectral correlations among neighboring pixels, and a post-processing step that suppresses noticeable demosaicking artifacts by adaptive median filtering. Moreover, in recognition of the limitations of current image measures, we propose two types of image measures to quantify the performance of different demosaicking methods; the first type evaluates the fidelity of demosaicked images by computing the peak signal-to-noise ratio and CIELAB /spl utri/E/sup *//sub ab/ for edge and smooth regions separately, and the second type accounts for one major demosaicking artifact-zipper effect. We gauge the proposed demosaicking method and image measures using several existing methods as benchmarks, and demonstrate their efficacy using a variety of test images.},
  keywords = {Adaptive filters,Color,Current measurement,Digital cameras,Digital filters,Image sensors,Interpolation,Pixel,Rendering (computer graphics),Sensor arrays},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3ZPNSCTJ\\Lu and Tan - 2003 - Color filter array demosaicking new method and pe.pdf;C\:\\Users\\emilm\\Zotero\\storage\\U6HA52HR\\stamp.html}
}

@misc{luitenDynamic3DGaussians2023,
  title = {Dynamic {{3D Gaussians}}: {{Tracking}} by {{Persistent Dynamic View Synthesis}}},
  shorttitle = {Dynamic {{3D Gaussians}}},
  author = {Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09713},
  eprint = {2308.09713},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-25},
  abstract = {We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Y2AUSCR6\\Luiten et al_2023_Dynamic 3D Gaussians.pdf;C\:\\Users\\emilm\\Zotero\\storage\\YTFM4G5H\\2308.html}
}

@article{luitjensCUDAStreamsBest,
  title = {{{CUDA Streams}}: {{Best Practices}} and {{Common Pitfalls}}},
  author = {Luitjens, Justin},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KTDT7RAE\Luitjens - CUDA Streams Best Practices and Common Pitfalls.pdf}
}

@misc{lukeThingsYouShould2018,
  title = {4 {{Things You Should Know About Jumbo Frames}} {$\cdot$} Blue42},
  author = {Luke, Arntz},
  year = {2018},
  month = sep,
  urldate = {2023-04-27},
  howpublished = {https://blue42.net/networking/4-things-must-know-jumbo-frames/},
  file = {C:\Users\emilm\Zotero\storage\5SC97XYY\4-things-must-know-jumbo-frames.html}
}

@misc{LumaAIVideo,
  title = {Luma {{AI}} - {{Video}} to {{3D API}}},
  journal = {Luma AI - Video to 3D API},
  urldate = {2023-12-04},
  abstract = {Capture the world in lifelike 3D},
  howpublished = {https://lumalabs.ai/luma-api}
}

@misc{makinbacon21TUTORIALUsingSdkmanager2022,
  title = {{{TUTORIAL}}: {{Using}} Sdkmanager for Flashing on {{Windows}} via {{WSL2}} + {{WSLg}}},
  shorttitle = {{{TUTORIAL}}},
  author = {{makinbacon21}},
  year = {2022},
  month = aug,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-15},
  abstract = {It took a bit of trial and error to get this working so I figured I would document what worked here:   Make sure you have the latest release of Windows 11 Install WSL2 and Ubuntu 18.04 LTS and make sure your WSL2 kernel is up to date  wsl --install -d "Ubuntu-18.04" wsl --update   (Optional) Properly install the WSL2 GPU drivers for your WSLg   NVIDIA Intel AMD   Install usbipd-win   winget install --interactive --exact dorssel.usbipd-win    Launch a new Ubuntu 18.04 WSL2 instance and install th...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/tutorial-using-sdkmanager-for-flashing-on-windows-via-wsl2-wslg/225759},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\CL3LNNND\225759.html}
}

@patent{malvarHighqualityGradientcorrectedLinear2009,
  title = {High-Quality Gradient-Corrected Linear Interpolation for Demosaicing of Color Images},
  author = {Malvar, Henrique S. and He, Li-wei and Cutler, Ross},
  year = {2009},
  month = mar,
  number = {US7502505B2},
  urldate = {2023-05-23},
  assignee = {Microsoft Corp},
  langid = {english},
  nationality = {US},
  keywords = {color,correction,gradient,interpolation,pixel},
  file = {C:\Users\emilm\Zotero\storage\7K5PNTKD\Malvar et al. - 2009 - High-quality gradient-corrected linear interpolati.pdf}
}

@misc{martensEnablePPSJetson2023,
  title = {Enable {{PPS}} on {{Jetson Linux}} 35.3.1 for {{Xavier AGX}}},
  author = {Martens, Emil},
  year = {2023},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-22},
  abstract = {My setup (before flash):  Jetson Xavier agx 16Gb  cat /etc/nv\_tegra\_release {\textrightarrow} R32 (release), REVISION: 6.1, GCID: 27863751, BOARD: t186ref, EABI: aarch64, DATE: Mon Jul 26 19:36:31 UTC 2021  Hi, I'm trying to enable PPS on JetPack 5.1.1 on out Xavier agx without success. I previously had it working in JetPack 4.4 but I cannot get it to work with the new version.  I have read through every post related to PPS I could find here on the forums and tried every suggested fix, without any success.  Cou...},
  chapter = {Autonomous Machines},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KNWX6ND7\252416.html}
}

@misc{martensPortableSensorRig2022,
  title = {A Portable Sensor Rig for Multi-Sensor Data Aquisition in Maritime Environements},
  author = {Martens, Emil},
  year = {2022},
  month = jun
}

@misc{martensPostsRedEmil,
  title = {Posts Red by Emil.Martens},
  author = {Martens, Emil and NVIDIA},
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-16},
  abstract = {NVIDIA Developer Forums},
  howpublished = {https://forums.developer.nvidia.com/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VXCUMBZV\read.html}
}

@misc{martensProblemsNvvidconvVideo2023,
  title = {Problems with ``{\ldots} ! Nvvidconv ! Video/x-Raw, format={{P010}}\_{{10LE}} ! {\ldots}''},
  author = {Martens, Emil and Yingliu},
  year = {2023},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-06-12},
  abstract = {NVIDIA Developer Forums},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\GNNAR9Z9\251929.html}
}

@misc{martensRe17896Use2023,
  title = {Re:[\#\# 17896 \#\#] {{Use}} Predefined Buffers for \_{{xDevice}}.{{xDeviceGetBuffer}}},
  author = {Martens, Emil and Fischer, Felix},
  year = {2023},
  month = may
}

@book{martinCleanCodeHandbook2009,
  title = {Clean Code: A Handbook of Agile Software Craftsmanship},
  shorttitle = {Clean Code},
  editor = {Martin, Robert C.},
  year = {2009},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, NJ}},
  isbn = {978-0-13-235088-4},
  lccn = {QA76.76.D47 C583 2009},
  keywords = {Agile software development,Computer software,Reliability}
}

@misc{martirosSymForceSymbolicComputation2022,
  title = {{{SymForce}}: {{Symbolic Computation}} and {{Code Generation}} for {{Robotics}}},
  shorttitle = {{{SymForce}}},
  author = {Martiros, Hayk and Miller, Aaron and Bucki, Nathan and Solliday, Bradley and Kennedy, Ryan and Zhu, Jack and Dang, Tung and Pattison, Dominic and Zheng, Harrison and Tomic, Teo and Henry, Peter and Cross, Gareth and VanderMey, Josiah and Sun, Alvin and Wang, Samuel and Holtz, Kristen},
  year = {2022},
  month = may,
  number = {arXiv:2204.07889},
  eprint = {2204.07889},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.07889},
  urldate = {2022-11-01},
  abstract = {We present SymForce, a library for fast symbolic computation, code generation, and nonlinear optimization for robotics applications like computer vision, motion planning, and controls. SymForce combines the development speed and flexibility of symbolic math with the performance of autogenerated, highly optimized code in C++ or any target runtime language. SymForce provides geometry and camera types, Lie group operations, and branchless singularity handling for creating and analyzing complex symbolic expressions in Python, built on top of SymPy. Generated functions can be integrated as factors into our tangent-space nonlinear optimizer, which is highly optimized for real-time production use. We introduce novel methods to automatically compute tangent-space Jacobians, eliminating the need for bug-prone handwritten derivatives. This workflow enables faster runtime code, faster development time, and fewer lines of handwritten code versus the state-of-the-art. Our experiments demonstrate that our approach can yield order of magnitude speedups on computational tasks core to robotics. Code is available at https://github.com/symforce-org/symforce.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Computer Science - Symbolic Computation,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8NX8VMFS\\Martiros et al. - 2022 - SymForce Symbolic Computation and Code Generation.pdf;C\:\\Users\\emilm\\Zotero\\storage\\CCVK86LB\\2204.html}
}

@inproceedings{matasRandomizedRANSACSequential2005,
  title = {Randomized {{RANSAC}} with Sequential Probability Ratio Test},
  booktitle = {Tenth {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}}'05) {{Volume}} 1},
  author = {Matas, J. and Chum, O.},
  year = {2005},
  month = oct,
  volume = {2},
  pages = {1727-1732 Vol. 2},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2005.198},
  abstract = {A randomized model verification strategy for RANSAC is presented. The proposed method finds, like RANSAC, a solution that is optimal with user-controllable probability n. A provably optimal model verification strategy is designed for the situation when the contamination of data by outliers is known, i.e. the algorithm is the fastest possible (on average) of all randomized RANSAC algorithms guaranteeing 1 - n confidence in the solution. The derivation of the optimality property is based on Wald's theory of sequential decision making. The R-RANSAC with SPRT which does not require the a priori knowledge of the fraction of outliers and has results close to the optimal strategy is introduced. We show experimentally that on standard test data the method is 2 to 10 times faster than the standard RANSAC and up to 4 times faster than previously published methods.},
  keywords = {Algorithm design and analysis,Computer vision,Contamination,Cost function,Cybernetics,Decision making,Robustness,Sequential analysis,Standards publication,Testing},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\U76EUQ3V\\Matas and Chum - 2005 - Randomized RANSAC with sequential probability rati.pdf;C\:\\Users\\emilm\\Zotero\\storage\\D777A5ZN\\1544925.html}
}

@misc{matulichErgonomicHandleBased,
  title = {Ergonomic Handle Based on Scientific Study by {{Anachronist}} | {{Download}} Free {{STL}} Model},
  author = {Matulich, Alex},
  journal = {Printables.com},
  urldate = {2023-05-27},
  abstract = {OpenSCAD library for parametric ergonomic handle to fit any size of hand. | Download free 3D printable STL models},
  howpublished = {https://www.printables.com/model/154837-ergonomic-handle-based-on-scientific-study},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VULVSG5M\154837-ergonomic-handle-based-on-scientific-study.html}
}

@misc{matulichWhoseHandsAre2022,
  title = {Whose Hands Are Biggest? {{You}} May Be Surprised.},
  shorttitle = {Whose Hands Are Biggest?},
  author = {Matulich, Alex},
  year = {2022},
  month = mar,
  urldate = {2023-05-27},
  abstract = {A recent project to create an ergonomic handle for 3D printing led me down a path that introduced me to anthropometric  measurements of the ...},
  file = {C:\Users\emilm\Zotero\storage\KASRM7CF\whose-hands-are-biggest-you-may-be.html}
}

@article{mcguireEfficientHighQualityBayer2008,
  title = {Efficient, {{High-Quality Bayer Demosaic Filtering}} on {{GPUs}}},
  author = {Mcguire, Morgan},
  year = {2008},
  month = jan,
  journal = {J. Graphics Tools},
  volume = {13},
  pages = {1--16},
  doi = {10.1080/2151237X.2008.10129267},
  abstract = {This paper describes a series of optimizations for implementing the high-quality Malvar-He-Cutler Bayer demosaicing filter on a GPU in OpenGL. Applying this filter is the first step in most video-processing pipelines but is generally considered too slow for real time on a CPU. The optimized implementation contains 66\% fewer ALU operations than a direct GPU implementation and can filter 40 simultaneous HD 1080p video streams at 30 fps (2728 Mpix/s) on current hardware. It is two to three times faster than a straightforward GPU implementation of the same algorithm on many GPUs. Most of the optimizations are applicable to other kinds of processors that support SIMD instructions, like CPUs and DSPs. Source code is available online.},
  file = {C:\Users\emilm\Zotero\storage\64WWNKXZ\Mcguire - 2008 - Efficient, High-Quality Bayer Demosaic Filtering o.pdf}
}

@misc{mckayWhatAreStdin2020,
  title = {What {{Are}} Stdin, Stdout, and Stderr on {{Linux}}?},
  author = {Mckay, Dave},
  year = {2020},
  month = jun,
  urldate = {2023-05-26},
  howpublished = {https://www.howtogeek.com/435903/what-are-stdin-stdout-and-stderr-on-linux/},
  file = {C:\Users\emilm\Zotero\storage\KPAIXVJA\what-are-stdin-stdout-and-stderr-on-linux.html}
}

@misc{MeanSquaredError,
  title = {Mean Squared Error: {{Love}} It or Leave It? {{A}} New Look at {{Signal Fidelity Measures}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  urldate = {2023-10-25},
  howpublished = {https://ieeexplore.ieee.org/document/4775883}
}

@misc{mehlMFUSEMultiframeFusion2022,
  title = {M-{{FUSE}}: {{Multi-frame Fusion}} for {{Scene Flow Estimation}}},
  shorttitle = {M-{{FUSE}}},
  author = {Mehl, Lukas and Jahedi, Azin and Schmalfuss, Jenny and Bruhn, Andr{\'e}s},
  year = {2022},
  month = oct,
  number = {arXiv:2207.05704},
  eprint = {2207.05704},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.05704},
  urldate = {2023-11-01},
  abstract = {Recently, neural network for scene flow estimation show impressive results on automotive data such as the KITTI benchmark. However, despite of using sophisticated rigidity assumptions and parametrizations, such networks are typically limited to only two frame pairs which does not allow them to exploit temporal information. In our paper we address this shortcoming by proposing a novel multi-frame approach that considers an additional preceding stereo pair. To this end, we proceed in two steps: Firstly, building upon the recent RAFT-3D approach, we develop an improved two-frame baseline by incorporating an advanced stereo method. Secondly, and even more importantly, exploiting the specific modeling concepts of RAFT-3D, we propose a U-Net architecture that performs a fusion of forward and backward flow estimates and hence allows to integrate temporal information on demand. Experiments on the KITTI benchmark do not only show that the advantages of the improved baseline and the temporal fusion approach complement each other, they also demonstrate that the computed scene flow is highly accurate. More precisely, our approach ranks second overall and first for the even more challenging foreground objects, in total outperforming the original RAFT-3D method by more than 16\%. Code is available at https://github.com/cv-stuttgart/M-FUSE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FKU64BLG\\Mehl et al_2022_M-FUSE.pdf;C\:\\Users\\emilm\\Zotero\\storage\\LYGGV4PG\\2207.html}
}

@misc{mekyaFirstHEVC2652020,
  title = {The {{First HEVC}} ({{H}}.265) {{Support}} in {{WebRTC}}},
  author = {{mekya}},
  year = {2020},
  month = jul,
  urldate = {2023-05-26},
  abstract = {We are very happy to announce that Ant Media Server v2.1 is released with some more great new features such as HEVC(H.265) support in WebRTC stack and WebM},
  chapter = {Announcement},
  howpublished = {https://antmedia.io/the-first-hevc-h265-support-in-webrtc/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\SNR2M6LY\the-first-hevc-h265-support-in-webrtc.html}
}

@article{MemoryPaging2023,
  title = {Memory Paging},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-05-09},
  abstract = {In computer operating systems, memory paging (or swapping on some Unix-like systems) is a memory management scheme by which a computer stores and retrieves data from secondary storage for use in main memory. In this scheme, the operating system retrieves data from secondary storage in same-size blocks called pages. Paging is an important part of virtual memory implementations in modern operating systems, using secondary storage to let programs exceed the size of available physical memory. For simplicity, main memory is called "RAM" (an acronym of random-access memory) and secondary storage is called "disk" (a shorthand for hard disk drive, drum memory or solid-state drive, etc.), but as with many aspects of computing, the concepts are independent of the technology used. Depending on the memory model, paged memory functionality is usually hardwired into a CPU/MCU by using a Memory Management Unit (MMU) or Memory Protection Unit (MPU) and separately enabled by privileged system code in the operating system's kernel. In CPUs implementing the x86 instruction set architecture (ISA) for instance, the memory paging is enabled via the CR0 control register.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1147663988},
  file = {C:\Users\emilm\Zotero\storage\H3VNBT5A\Memory_paging.html}
}

@inproceedings{meyerSoftwareDevelopersPerceptions2014,
  title = {Software Developers' Perceptions of Productivity},
  booktitle = {Proceedings of the 22nd {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Meyer, Andr{\'e} N. and Fritz, Thomas and Murphy, Gail C. and Zimmermann, Thomas},
  year = {2014},
  month = nov,
  pages = {19--29},
  publisher = {{ACM}},
  address = {{Hong Kong China}},
  doi = {10.1145/2635868.2635892},
  urldate = {2023-05-16},
  isbn = {978-1-4503-3056-5},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\DCK7JKNL\Meyer et al. - 2014 - Software developers' perceptions of productivity.pdf}
}

@misc{mhtechdevProgressPPS2023,
  title = {Progress on {{PPS}}?},
  author = {{mhtechdev} and Martens, Emil},
  year = {2023},
  month = may,
  urldate = {2023-05-22},
  abstract = {My setup (before flash):  Jetson Xavier agx 16Gb  cat /etc/nv\_tegra\_release {\textrightarrow} R32 (release), REVISION: 6.1, GCID: 27863751, BOARD: t186ref, EABI: aarch64, DATE: Mon Jul 26 19:36:31 UTC 2021  Hi, I'm trying to enable PPS on JetPack 5.1.1 on out Xavier agx without success. I previously had it working in JetPack 4.4 but I cannot get it to work with the new version.  I have read through every post related to PPS I could find here on the forums and tried every suggested fix, without any success.  Cou...},
  chapter = {Autonomous Machines},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\97J7WN3X\2.html}
}

@misc{micronMTFDHBA512TDV1AZ12AB,
  title = {{{MTFDHBA512TDV-1AZ12AB}}},
  author = {Micron},
  urldate = {2023-05-22},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\MIG7PUXJ\mtfdhba512tdv-1az12ab.html}
}

@techreport{microntechnologyMicron2300SSD2020,
  title = {{{Micron}}{\textregistered} 2300 {{SSD}} with {{NVMe}}{\texttrademark}},
  author = {Micron Technology},
  year = {2020},
  number = {A 05/20 CCM004-676576390-11460},
  file = {C:\Users\emilm\Zotero\storage\UNMNMLJA\2300_client_nvme_ssd_product_brief.pdf}
}

@misc{MicroSD3D,
  title = {Micro {{SD}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  urldate = {2022-05-15},
  howpublished = {https://grabcad.com/library/micro-sd-2},
  keywords = {cad\_file},
  file = {C:\Users\emilm\Zotero\storage\YFIHT6NJ\micro-sd-2.html}
}

@misc{microsoftDebugpyDebuggerPython2023,
  title = {Debugpy - a Debugger for {{Python}}},
  author = {Microsoft},
  year = {2023},
  month = may,
  urldate = {2023-05-30},
  abstract = {An implementation of the Debug Adapter Protocol for Python},
  howpublished = {Microsoft}
}

@article{mihoubiSurveyDemosaickingMethods2018,
  title = {Survey of {{Demosaicking Methods}} for {{Polarization Filter Array Images}}},
  author = {Mihoubi, Sofiane and Lapray, Pierre-Jean and Bigu{\'e}, Laurent},
  year = {2018},
  month = nov,
  journal = {Sensors},
  volume = {18},
  number = {11},
  pages = {3688},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18113688},
  urldate = {2023-06-20},
  abstract = {Snapshot polarization imaging has gained interest in the last few decades. Recent research and technology achievements defined the polarization Filter Array (PFA). It is dedicated to division-of-focal plane polarimeters, which permits to analyze the direction of light electric field oscillation. Its filters form a mosaicked pattern, in which each pixel only senses a fraction of the total polarization states, so the other missing polarization states have to be interpolated. As for Color or Spectral Filter Arrays (CFA or SFA), several dedicated demosaicking methods exist in the PFA literature. Such methods are mainly based on spatial correlation disregarding inter-channel correlation. We show that polarization channels are strongly correlated in images. We therefore propose to extend some demosaicking methods from CFA/SFA to PFA, and compare them with those that are PFA-oriented. Objective and subjective analysis show that the pseudo panchromatic image difference method provides the best results and can be used as benchmark for PFA demosaicking.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {demosaicing,demosaicking,division-of-focal-plane polarimeter,micro-polarizer filter array,polarization filter array,polarization imaging,spatial interpolation},
  file = {C:\Users\emilm\Zotero\storage\V8VWD8Y3\Mihoubi et al. - 2018 - Survey of Demosaicking Methods for Polarization Fi.pdf}
}

@misc{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  urldate = {2023-11-01},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\52S8E434\\Mildenhall et al_2020_NeRF.pdf;C\:\\Users\\emilm\\Zotero\\storage\\GMI3BXAD\\2003.html}
}

@misc{mildenhallNeRFRepresentingScenes2020a,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  urldate = {2023-11-08},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BVNKKZJT\\Mildenhall et al_2020_NeRF.pdf;C\:\\Users\\emilm\\Zotero\\storage\\8MN75UEW\\2003.html}
}

@misc{mildenhallNeRFRepresentingScenes2020b,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-21},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\U6CNC5YY\\Mildenhall et al_2020_NeRF.pdf;C\:\\Users\\emilm\\Zotero\\storage\\82PZEC5L\\2003.html}
}

@misc{mmmcccxxxiiiLubanVsCura2022,
  type = {Reddit {{Post}}},
  title = {Luban vs {{Cura}}},
  author = {{mmmcccxxxiii}},
  year = {2022},
  month = nov,
  journal = {r/snapmaker},
  urldate = {2023-05-27},
  file = {C:\Users\emilm\Zotero\storage\8VEG3KBG\luban_vs_cura.html}
}

@misc{ModelViewProjection2019,
  title = {Model {{View Projection}}},
  year = {2019},
  month = apr,
  journal = {jsantell.com},
  urldate = {2023-10-25},
  abstract = {Jordan Santell, focusing on open web engineering, immersive web, WebXR, WebGL, JavaScript, open source, open standards, and all things weird web.},
  howpublished = {https://jsantell.com/model-view-projection/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\6QYP5ZLU\model-view-projection.html}
}

@misc{monstersQuickOverviewMethods2020,
  title = {A {{Quick Overview}} of {{Methods}} to {{Measure}} the {{Similarity Between Images}}},
  author = {Monsters, Data},
  year = {2020},
  month = mar,
  journal = {Medium},
  urldate = {2023-10-25},
  abstract = {When you work with computer vision challenges, you must choose a method for measuring the similarity between two images to compare{\ldots}},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KS8CRZKV\a-quick-overview-of-methods-to-measure-the-similarity-between-images-f907166694ee.html}
}

@misc{mucahittoygarNVIDIAJetsonAGX2019,
  title = {{{NVIDIA Jetson AGX Xavier}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  author = {M{\"u}cahit Toygar},
  year = {3rd, 2019},
  urldate = {2022-05-17},
  howpublished = {https://grabcad.com/library/nvidia-jetson-agx-xavier-1},
  file = {C:\Users\emilm\Zotero\storage\LDIJPR2R\nvidia-jetson-agx-xavier-1.html}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author = {M{\"u}ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  year = {2022},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {41},
  number = {4},
  eprint = {2201.05989},
  primaryclass = {cs},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  urldate = {2023-11-08},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920{\textbackslash}!{\textbackslash}times{\textbackslash}!1080\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AQNGDMAG\\Müller et al_2022_Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.pdf;C\:\\Users\\emilm\\Zotero\\storage\\42CN7SYS\\2201.html}
}

@misc{munawarDeepstreamInstallationJetson2022,
  title = {Deepstream Installation on Jetson Devices},
  author = {Munawar, Muhammad Rizwan},
  year = {2022},
  month = feb,
  journal = {Nerd For Tech},
  urldate = {2023-05-02},
  abstract = {Deepstream is software development kit (sdk) developed by nvidia, mainly for embedded devices, but we can use it on pc (with gpu support){\ldots}},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\S7YXER4D\deepstream-installation-on-jetson-devices-fb55ace0587f.html}
}

@misc{nadelHowUseSmartphone2020,
  title = {How to Use a Smartphone as a Mobile Hotspot},
  author = {Nadel, Brian},
  year = {2020},
  month = jul,
  journal = {Computerworld},
  urldate = {2022-05-20},
  abstract = {Here's everything you need to know about Wi-Fi tethering from your phone.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\TQ6XY3RE\how-to-use-a-smartphone-as-a-mobile-hotspot.html}
}

@misc{nadigAnswerDebugNot2019,
  title = {Answer to: {{Debug}} Not Working with {{Gstreamer}} (Multithreaded) {$\cdot$} {{Issue}} \#1865 {$\cdot$} Microsoft/Ptvsd},
  author = {Nadig, Karthik},
  year = {2019},
  month = oct,
  journal = {GitHub},
  urldate = {2023-05-30},
  abstract = {This issue is very similar to this closed issue but the environment and used packages are different. Environment data VS Code version: 1.39.2 Extension version (available under the Extensions sideb...},
  howpublished = {https://github.com/microsoft/ptvsd/issues/1865\#issuecomment-544733169},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\BQDHVJU9\1865.html}
}

@misc{namePreparingJetsonKernel,
  title = {Preparing {{Jetson}} Kernel for {{EdgeVPN}}.Io},
  author = {Name, Your},
  journal = {EdgeVPN.io},
  urldate = {2023-05-09},
  howpublished = {https://edgevpn.io/jetson/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VB36T2XQ\jetson.html}
}

@misc{napsZHeightCalibrationOffset2023,
  title = {Z-{{Height Calibration Offset}} / {{Adjustment}}},
  author = {{naPS}},
  year = {2023},
  month = mar,
  journal = {Snapmaker: where creation happens},
  urldate = {2023-06-03},
  abstract = {I've run the bed leveling and z-height check multiple times.  The first layer of my print is always squished.  Everything is telling me to set the z=0 height via offset, but the only way I've found to do that is via this thread : Finer adjustments to z offset?  Is there a better way to do this?  I'm hoping this will also work for this IDEX machine, since that's for an Artisan machine.},
  chapter = {Snapmaker J1},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\8PJQITIG\30592.html}
}

@misc{neimarkVideoTransformerNetwork2021,
  title = {Video {{Transformer Network}}},
  author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  year = {2021},
  month = aug,
  number = {arXiv:2102.00719},
  eprint = {2102.00719},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.00719},
  urldate = {2023-09-14},
  abstract = {This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains \$16.1{\textbackslash}times\$ faster and runs \$5.1{\textbackslash}times\$ faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring \$1.5{\textbackslash}times\$ fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Z2ASSMFZ\\Neimark et al. - 2021 - Video Transformer Network.pdf;C\:\\Users\\emilm\\Zotero\\storage\\SRFZF5NC\\2102.html}
}

@techreport{NetworkedTransportRTCM2004,
  title = {Networked {{Transport}} of {{RTCM}} via {{Internet Protocol}}},
  year = {2004},
  number = {1},
  file = {C:\Users\emilm\Zotero\storage\VAHF4SJR\NtripDocumentation.pdf}
}

@misc{NetworkTimeProtocol2022,
  title = {Network {{Time Protocol}} Daemon - {{ArchWiki}}},
  year = {2022},
  month = may,
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\2XW7DSYV\Network_Time_Protocol_daemon.html}
}

@misc{newhonghuidafastenernhhdstoreM2M3100pcs,
  title = {M2 {{M3}} 100pcs {{Insert Knurled Nuts Brass Hot Melt Inset Nuts Heating Molding Copper Thread Inserts Nut Free Shipping}} - {{Nuts}} - {{AliExpress}}},
  author = {NEWHONGHUIDA Fastener NHHD Store},
  journal = {aliexpress.com},
  urldate = {2022-05-18},
  abstract = {Smarter Shopping, Better Living!  Aliexpress.com},
  howpublished = {//www.aliexpress.com/item/4001258499799.html?src=ibdm\_d03p0558e02r02\&sk=\&aff\_platform=\&aff\_trace\_key=\&af=\&cv=\&cn=\&dp=},
  file = {C:\Users\emilm\Zotero\storage\4M9DLGLI\4001258499799.html}
}

@misc{nguyenTwoStepColorPolarizationDemosaicking2022,
  title = {Two-{{Step Color-Polarization Demosaicking Network}}},
  author = {Nguyen, Vy and Tanaka, Masayuki and Monno, Yusuke and Okutomi, Masatoshi},
  year = {2022},
  month = sep,
  number = {arXiv:2209.06027},
  eprint = {2209.06027},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-06-20},
  abstract = {Polarization information of light in a scene is valuable for various image processing and computer vision tasks. A division-of-focal-plane polarimeter is a promising approach to capture the polarization images of different orientations in one shot, while it requires color-polarization demosaicking. In this paper, we propose a two-step color-polarization demosaicking network{\textasciitilde}(TCPDNet), which consists of two sub-tasks of color demosaicking and polarization demosaicking. We also introduce a reconstruction loss in the YCbCr color space to improve the performance of TCPDNet. Experimental comparisons demonstrate that TCPDNet outperforms existing methods in terms of the image quality of polarization images and the accuracy of Stokes parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\NVIFVITB\\Nguyen et al. - 2022 - Two-Step Color-Polarization Demosaicking Network.pdf;C\:\\Users\\emilm\\Zotero\\storage\\4N62BCX7\\2209.html}
}

@misc{NTRIPRev1Rev2,
  title = {{{NTRIP Rev1}} versus {{Rev2}} Formats},
  journal = {SNIP Support},
  urldate = {2022-05-02},
  abstract = {This article explains the difference between the NTRIP Client connection format style used in Rev1 (version 1.0) and Rev2 (version 2.0) and provides examples.},
  langid = {american}
}

@misc{nulizhuanzhuAGXXavier35,
  title = {{{AGX Xavier}} 35.1.0 Enable Pps 过程全记录\_nulizhuanzhu的博客-{{CSDN博客}}},
  author = {{nulizhuanzhu}},
  urldate = {2023-05-08},
  howpublished = {https://blog.csdn.net/whr19970424/article/details/129824969?spm=1001.2014.3001.5502},
  file = {C:\Users\emilm\Zotero\storage\RJMI7ZC8\129824969.html}
}

@misc{numpyArrayInterfaceProtocol,
  title = {The Array Interface Protocol {\textemdash} {{NumPy}} v1.25 {{Manual}}},
  author = {NumPy},
  urldate = {2023-06-21},
  howpublished = {https://numpy.org/doc/stable/reference/arrays.interface.html},
  file = {C:\Users\emilm\Zotero\storage\9ILJTHXP\arrays.interface.html}
}

@misc{nv-computeNsightComputeMemory2022,
  title = {Nsight Compute ---- {{Memory Chart}}\_{{UCAS}}\_{{HMM}}'s {{Blog}} - {{CSDN Blog}}},
  author = {{NV-compute}},
  year = {2022},
  month = oct,
  urldate = {2023-06-17},
  howpublished = {https://blog.csdn.net/ucas\_hmm/article/details/126521832},
  file = {C:\Users\emilm\Zotero\storage\TJLH7GJ8\126521832.html}
}

@misc{Nvcodec,
  title = {Nvcodec},
  urldate = {2023-04-11},
  howpublished = {https://gstreamer.freedesktop.org/documentation/nvcodec/index.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\YQINJEL6\index.html}
}

@misc{Nvenc10bitHevc2022,
  title = {Nvenc 10bit Hevc},
  year = {2022},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-13},
  abstract = {Do we support the 10bit HEVC encoding?  When I use the command below to do the HEVC 10bit encoding, I get non-correct data analyzed by Elecard.  ./video\_encode crowd\_run\_1920x1080\_420\_50\_100frame.yuv.P010 1920 1080 H265 crowd\_main10.265 -p main10  crowd\_main10.265 (567.5 KB)  As the yuv is too large, I only upload one frame of it! If you need the full, I can send it to your e-mail.  crowd\_run\_1920x1080\_420\_50\_1frame.yuv.P010 (5.9 MB)},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/nvenc-10bit-hevc/199535},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\HHJKA8GQ\199535.html}
}

@misc{nvidiaAcceleratedGStreamerJetson,
  title = {Accelerated {{GStreamer}} {\textemdash} {{Jetson Linux Developer Guide}} Documentation},
  author = {NVIDIA},
  urldate = {2023-06-26},
  howpublished = {https://docs.nvidia.com/jetson/archives/r35.2.1/DeveloperGuide/text/SD/Multimedia/AcceleratedGstreamer.html},
  file = {C:\Users\emilm\Zotero\storage\NHE8AX4T\AcceleratedGstreamer.html}
}

@article{nvidiaAcceleratedGStreamerUser,
  title = {Accelerated {{GStreamer User Guide}}},
  author = {NVIDIA},
  langid = {english},
  keywords = {Jetson,Xavier}
}

@misc{nvidiaaiiotNVIDIAAIIOTDeepstreamPython,
  title = {{{NVIDIA-AI-IOT}}/Deepstream\_python\_apps: {{DeepStream SDK Python}} Bindings and Sample Applications},
  author = {NVIDIA AI IOT},
  urldate = {2023-05-30},
  howpublished = {https://github.com/NVIDIA-AI-IOT/deepstream\_python\_apps/tree/master},
  file = {C:\Users\emilm\Zotero\storage\WQDY5WLY\master.html}
}

@misc{nvidiaaiiotReleasesNVIDIAAIIOTDeepstream,
  title = {Releases {$\cdot$} {{NVIDIA-AI-IOT}}/Deepstream\_python\_apps},
  author = {NVIDIA AI IOT},
  urldate = {2023-05-30},
  abstract = {DeepStream SDK Python bindings and sample applications - NVIDIA-AI-IOT/deepstream\_python\_apps},
  file = {C:\Users\emilm\Zotero\storage\LT68EJP4\deepstream_python_apps.html}
}

@misc{nvidiaCUDABestPractices2023,
  title = {{{CUDA C}}++ {{Best Practices Guide}}},
  author = {NVIDIA},
  year = {2023},
  month = apr,
  file = {C:\Users\emilm\Zotero\storage\6M8VZ8US\CUDA C++ Best Practices Guide.pdf}
}

@misc{nvidiaCUDAFTegra2023,
  title = {{{CUDA}} for {{fTegra}}},
  author = {NVIDIA},
  year = {2023},
  month = apr,
  file = {C:\Users\emilm\Zotero\storage\LMGIDAAZ\CUDA-For-Tegra-AppNote.pdf}
}

@misc{nvidiaCUDAProgrammingGuide,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  author = {NVIDIA},
  file = {C:\Users\emilm\Zotero\storage\7GA7K3YR\CUDA C++ Programming Guide.pdf}
}

@misc{NVIDIADeepStreamSDK,
  title = {{{NVIDIA DeepStream SDK API Reference}}: {{NvBufSurface Types}} and {{Functions}} | {{NVIDIA Docs}}},
  urldate = {2023-06-14},
  howpublished = {https://docs.nvidia.com/metropolis/deepstream/sdk-api/group\_\_ds\_\_aaa.html},
  file = {C:\Users\emilm\Zotero\storage\248GEB89\group__ds__aaa.html}
}

@misc{nvidiaDeepStreamSDK2016,
  title = {{{DeepStream SDK}}},
  author = {NVIDIA},
  year = {2016},
  month = sep,
  journal = {NVIDIA Developer},
  urldate = {2023-05-30},
  abstract = {Develop and deploy AI-powered intelligent video analytics apps and services faster anywhere.},
  howpublished = {https://developer.nvidia.com/deepstream-sdk},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\N59KPZ8E\deepstream-sdk.html}
}

@misc{nvidiaDeepStreamSDKGet2019,
  title = {{{DeepStream SDK}} - {{Get Started}}},
  author = {NVIDIA},
  year = {2019},
  month = jul,
  journal = {NVIDIA Developer},
  urldate = {2023-05-15},
  abstract = {Find resources to create vision AI apps with AI for image processing.},
  howpublished = {https://developer.nvidia.com/deepstream-getting-started},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\JBPSVRLW\deepstream-getting-started.html}
}

@misc{NVIDIAGstreamerNvvidconv2021,
  title = {{{NVIDIA Gstreamer}} Nvvidconv Question},
  year = {2021},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-01},
  abstract = {Hi all,  I'm trying to use nvvidconv plugin in my application.  My application uses cuda process, so I need to control gpu memory buffer.  As I know, if I add `(memory:NVMM)' in my gstreamer pipeline, it means the pipeline uses gpu memory.  So I received the data from nvvidconv gstreamerpipeline and passed it to cuda process.  But cuda process could not handle it properly.  Here is my pipeline example.  gst-launch-1.0 v4l2src device=/dev/video0 ! ``video/x-raw, format=(string)UYVY, width=(int)192...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/nvidia-gstreamer-nvvidconv-question/166063},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\CSFKMUBD\4.html}
}

@misc{nvidiaHalf2ArithmeticFunctions2023,
  type = {{{cppModule}}},
  title = {Half2 {{Arithmetic Functions}}},
  author = {NVIDIA},
  year = {2023},
  month = apr,
  urldate = {2023-06-19},
  howpublished = {https://docs.nvidia.com/cuda/cuda-math-api/index.html},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\62KCZ5WY\group__CUDA__MATH____HALF2__ARITHMETIC.html}
}

@misc{nvidiaJetPackSDK2022,
  title = {{{JetPack SDK}} 4.6.3},
  author = {NVIDIA},
  year = {2022},
  month = nov,
  journal = {NVIDIA Developer},
  urldate = {2023-05-15},
  abstract = {NVIDIA JetPack SDK is the most comprehensive solution for building end-to-end accelerated AI applications. All Jetson modules and developer kits are supported by JetPack SDK.},
  howpublished = {https://developer.nvidia.com/jetpack-sdk-463},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\W83CDJB9\jetpack-sdk-463.html}
}

@misc{nvidiaJetPackSDK2023,
  title = {{{JetPack SDK}} 5.1.1},
  author = {NVIDIA},
  year = {2023},
  month = mar,
  journal = {NVIDIA Developer},
  urldate = {2023-05-15},
  abstract = {NVIDIA JetPack SDK is the most comprehensive solution for building end-to-end accelerated AI applications. JetPack provides a full development environment for hardware-accelerated AI-at-the-edge development on Nvidia Jetson modules. JetPack includes Jetson Linux with bootloader, Linux kernel, Ubuntu desktop environment, and a complete set of libraries for acceleration of GPU computing, multimedia, graphics, and computer vision.},
  howpublished = {https://developer.nvidia.com/embedded/jetpack-sdk-511},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\IFS9RS9Z\jetpack-sdk-511.html}
}

@misc{NVIDIAJetsonAGX2018,
  title = {{{NVIDIA Jetson AGX Xavier Delivers}} 32 {{TeraOps}} for {{New Era}} of {{AI}} in {{Robotics}}},
  year = {2018},
  month = dec,
  journal = {NVIDIA Technical Blog},
  urldate = {2023-05-10},
  abstract = {The world's ultimate embedded solution for AI developers, Jetson AGX Xavier, is now shipping as standalone production modules from NVIDIA. A member of NVIDIA's AGX Systems for autonomous machines{\ldots}},
  howpublished = {https://developer.nvidia.com/blog/nvidia-jetson-agx-xavier-32-teraops-ai-robotics/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\9NAQHXE2\nvidia-jetson-agx-xavier-32-teraops-ai-robotics.html}
}

@techreport{nvidiaJetsonAGXOrin2022,
  title = {Jetson {{AGX Orin Series}} and {{Jetson AGX Xavier Series Interface Comparison}} and {{Migration}}},
  author = {NVIDIA},
  year = {2022},
  month = mar,
  number = {DA-10655-001\_v1.1},
  file = {C:\Users\emilm\Zotero\storage\GCTBWPT2\Jetson_AGX_Orin_Jetson_AGX_Xavier_IF_Comparison_Migration_DA-10655-001_v1.1.pdf}
}

@misc{nvidiaJetsonGPIOLinux2022,
  title = {Jetson.{{GPIO}} - {{Linux}} for {{Tegra}}},
  author = {NVIDIA},
  year = {2022},
  month = may,
  urldate = {2022-05-20},
  abstract = {A Python library that enables the use of Jetson's GPIOs},
  howpublished = {NVIDIA Corporation}
}

@misc{NVIDIAJetsonLinux,
  title = {{{NVIDIA Jetson Linux Developer Guide}} : {{Bootloader}} | {{NVIDIA Docs}}},
  urldate = {2023-05-12},
  howpublished = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-325/index.html\#page/Tegra\%20Linux\%20Driver\%20Package\%20Development\%20Guide/part\_config.html},
  file = {C:\Users\emilm\Zotero\storage\L837ZDHT\index.html}
}

@misc{nvidiaKernelCustomizationJetson2023,
  title = {Kernel {{Customization}} {\textemdash} {{Jetson Linux Developer Guide}} Documentation},
  author = {NVIDIA},
  year = {2023},
  urldate = {2023-05-16},
  howpublished = {https://docs.nvidia.com/jetson/archives/r35.3.1/DeveloperGuide/text/SD/Kernel/KernelCustomization.html\#to-build-the-real-time-kernel},
  file = {C:\Users\emilm\Zotero\storage\WUSBMX7K\KernelCustomization.html}
}

@misc{nvidiaNsightVisualStudio2021,
  title = {Nsight {{Visual Studio Code Edition}}},
  author = {NVIDIA},
  year = {2021},
  month = mar,
  journal = {NVIDIA Developer},
  urldate = {2023-06-26},
  abstract = {CUDA development for NVIDIA platforms integrated into Microsoft Visual Studio Code},
  howpublished = {https://developer.nvidia.com/nsight-visual-studio-code-edition},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\I5PM3JE8\nsight-visual-studio-code-edition.html}
}

@misc{nvidiaNvBufSurfaceCreateDeepstreamDeepstream2023,
  title = {{{NvBufSurfaceCreate}} {\textemdash} {{Deepstream Deepstream Version}}: 6.2 Documentation},
  author = {NVIDIA},
  year = {2023},
  month = feb,
  urldate = {2023-06-12},
  howpublished = {https://docs.nvidia.com/metropolis/deepstream/python-api/PYTHON\_API/Methods/methodsdoc.html\#nvbufsurfacecreate},
  file = {C:\Users\emilm\Zotero\storage\GYXMRRPB\methodsdoc.html}
}

@misc{nvidiaNvidiaForumExtended2023,
  title = {Nvidia Forum Extended Search},
  author = {NVIDIA},
  year = {2023},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-22},
  abstract = {Hi Emil,  Yea I managed to get it to work, by changing the last bit in ``gpios = {$<\&$}tegra\_main\_gpio 105 1{$>$};''  I am not sure why that was necessary, and I'd have to check if I changed it from 1 to 0 or vice versa, but after that it just worked.  Hope this will work for you as well.  Best Regards,  Milan},
  howpublished = {https://forums.developer.nvidia.com/t/progress-on-pps/252282/2},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\7NVBQ96S\search.html}
}

@misc{nvidiaNVIDIAJetsonAGX2019,
  title = {{{NVIDIA Jetson AGX Xavier Series System-on-Module}}},
  author = {NVIDIA},
  year = {2019},
  month = aug,
  file = {C:\Users\emilm\Zotero\storage\QHLIFRUX\Jetson-AGX-Xavier-Series-Datasheet.pdf}
}

@techreport{nvidiaNVIDIAJetsonAGX2020,
  title = {{{NVIDIA Jetson AGX Xavier Developer Kit Carrier Board}}},
  author = {NVIDIA},
  year = {2020},
  month = jun,
  number = {SP-09778-001\_v2.1},
  file = {C:\Users\emilm\Zotero\storage\BVXZ49SZ\Jetson_AGX_Xavier_Developer_Kit_Carrier_Board_Specification_SP-09778-001_v2.1.pdf}
}

@misc{nvidiaNVIDIAJetsonLinux,
  title = {{{NVIDIA Jetson Linux Developer Guide}} : {{Quick Start}} | {{NVIDIA Docs}}},
  author = {NVIDIA},
  urldate = {2023-05-03},
  howpublished = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3261/index.html\#page/Tegra\%20Linux\%20Driver\%20Package\%20Development\%20Guide/quick\_start.html\#wwpID0EAAPNHA},
  file = {C:\Users\emilm\Zotero\storage\6FGA94NQ\index.html}
}

@misc{nvidiaNVIDIAJetsonLinux2023,
  title = {{{NVIDIA Jetson Linux Release Notes Version}} 35.3.1 {{GA}}},
  author = {NVIDIA},
  year = {2023},
  month = mar,
  file = {C:\Users\emilm\Zotero\storage\5PLJ6HG5\Jetson_Linux_Release_Notes_r35.3.1.pdf}
}

@misc{nvidiaNVIDIANsightSystems2023,
  title = {{{NVIDIA Nsight Systems}} User Guide V2023.2.1},
  author = {NVIDIA},
  year = {2023},
  month = mar,
  urldate = {2023-06-26},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\MHW8Y7IC\\UserGuide.pdf;C\:\\Users\\emilm\\Zotero\\storage\\B639UXTG\\index.html}
}

@misc{nvidiaNVIDIAOptiXProgramming2023,
  title = {{{NVIDIA OptiX}} 8.0 - {{Programming~Guide}}},
  author = {NVIDIA},
  year = {2023},
  month = aug,
  urldate = {2023-12-04},
  file = {C:\Users\emilm\Zotero\storage\YEBSJVLB\index.html}
}

@misc{nvidiaNVIDIASDKManager2019,
  title = {{{NVIDIA SDK Manager}}},
  author = {NVIDIA},
  year = {2019},
  month = apr,
  urldate = {2022-05-20},
  abstract = {Everything You Need to Set Up Your Development Environment NVIDIA SDK Manager provides an end-to-end development environment setup solution for NVIDIA's DRIVE, Jetson, Clara Holoscan, Rivermax, DOCA and Ethernet Switch SDKs for both host and target devices. .deb Ubuntu .rpm CentOS/RHEL Docker ImageUbuntu 18.04 Docker ImageUbuntu 20.04 SDK Manager User Guide What's New in SDK Manager: Version 1.8: SDK Manager is now available with Docker Image based on Ubuntu 20.04. Added support for Jetson AGX Orin. Added support for NVIDIA Converged Accelerator for DOCA SDK users.},
  file = {C:\Users\emilm\Zotero\storage\XE8NBBYW\nvidia-sdk-manager.html}
}

@misc{nvidiaNVIDIATEGRALINUX,
  title = {{{NVIDIA TEGRA LINUX DRIVER PACKAGE QUICK START GUIDE}}},
  author = {NVIDIA},
  urldate = {2023-05-03},
  howpublished = {https://developer.download.nvidia.com/embedded/L4T/r32\_Release\_v1.0/Docs/l4t\_quick\_start\_guide.txt},
  file = {C:\Users\emilm\Zotero\storage\RQMQUM4C\l4t_quick_start_guide.html}
}

@misc{nvidiaOptiXQuickStart2018,
  title = {{{OptiX QuickStart}}},
  author = {NVIDIA},
  year = {2018},
  urldate = {2023-12-05},
  howpublished = {https://docs.nvidia.com/gameworks/content/gameworkslibrary/optix/optix\_quickstart.htm},
  file = {C:\Users\emilm\Zotero\storage\IBYLZU9S\optix_quickstart.html}
}

@misc{nvidiaPartitionConfigurationJetson2022,
  title = {Partition {{Configuration}} {\textemdash} {{Jetson Linux}}},
  author = {NVIDIA},
  year = {2022},
  month = may,
  urldate = {2023-05-22},
  howpublished = {https://docs.nvidia.com/jetson/archives/r35.1/DeveloperGuide/text/AR/BootArchitecture/PartitionConfiguration.html},
  file = {C:\Users\emilm\Zotero\storage\8RAHAHYN\PartitionConfiguration.html}
}

@techreport{nvidiapinmux,
  title = {{{NVIDIA Jetson AGX Xavier Developer Kit Pinmux}}},
  author = {NVIDIA},
  year = {2022},
  month = jan,
  number = {1.4},
  file = {C:\Users\emilm\Zotero\storage\VIMU6DA2\Jetson_AGX_Xavier_Series_Pinmux_Configuration_Template_v1.4.xlsm}
}

@misc{nvidiaSDKManager2019,
  title = {{{SDK Manager}}},
  author = {NVIDIA},
  year = {2019},
  month = apr,
  journal = {NVIDIA Developer},
  urldate = {2023-05-04},
  abstract = {An end-to-end development environment setup solution for DRIVE, Jetson, and more. SDKs.},
  howpublished = {https://developer.nvidia.com/sdk-manager},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\ZPRFN2LK\sdk-manager.html}
}

@misc{NVIDIATegraX2,
  title = {{{NVIDIA Tegra X2}} | {{Compiling Tegra X2 Source Code}} | {{RidgeRun}} - {{RidgeRun Developer Connection}}},
  urldate = {2023-05-06},
  howpublished = {https://developer.ridgerun.com/wiki/index.php/Compiling\_Jetson\_TX2\_source\_code},
  file = {C:\Users\emilm\Zotero\storage\G6CJ8K2K\Compiling_Jetson_TX2_source_code.html}
}

@techreport{nvidiaXavierAGXsysonmodule2021,
  title = {{{NVIDIA Jetson AGX Xavier Series System-on-Module}}},
  author = {NVIDIA},
  year = {2021},
  number = {DS-09654-002\_v1.6},
  file = {C:\Users\emilm\Zotero\storage\S94ILIF9\Jetson-AGX-Xavier-Series-Datasheet_DS09654002v1.6.pdf}
}

@techreport{nvidiaXavierAGXthermalsDesign2021,
  title = {Jetson {{AGX Xavier Series Thermal Design Guide}}},
  author = {NVIDIA},
  year = {2021},
  month = jun,
  number = {TDG-08981-001\_v1.3},
  file = {C:\Users\emilm\Zotero\storage\TXGTLGGD\Jetson_AGX_Xavier_Series_Thermal_Design_Guide_TDG-08981-001_v1.3.pdf}
}

@misc{NvivafilterDifferentInput2021,
  title = {Nvivafilter: Different Input and Output Buffers},
  shorttitle = {Nvivafilter},
  year = {2021},
  month = feb,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-11},
  abstract = {I'm developing a filter (.so library) to be used with gstreamer element nvivafilter  The official example, nvsample\_cudaprocess.cu does some changes over the incoming frame data. That is, only one frame and set of buffers exists, it is used as input data and to store output image. Overwrite on the data buffers is done during the processing of the image.  However, in my case, I can not overwrite the input while processing, I need different data storage for input and output.  I think I must follow...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/nvivafilter-different-input-and-output-buffers/167848},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\3ASZ7XRL\7.html}
}

@misc{NVMMGstreamer2019,
  title = {{{NVMM}} and {{Gstreamer}}},
  year = {2019},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-03-30},
  abstract = {I have two questions regarding NVMM memory:  First of all, what is NVMM, exactly, in technical terms? What does copying to/from normal memory to NVMM memory and back involve? I am particularly interested in whether copies involve a bus or are normal memory-to-memory copies. Also, what is its relation to CUDA memory? Are they the same things? If there is a reference describing the internals of TX2 architecture, describing how different subsystems communicate, it would greatly help me in understan...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/nvmm-and-gstreamer/69642},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\28CW9KT8\3.html}
}

@misc{NvsampleCudaprocessSrc2018,
  title = {Nvsample\_cudaprocess\_src with Opencv Compile Error},
  year = {2018},
  month = apr,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-03-30},
  abstract = {Hi all ,  I have noticed that for us we can use  gst-launch-1.0 nvcamerasrc fpsRange="30 30" ! {\textbackslash}  'video/x-raw(memory:NVMM), width=(int)3840, height=(int)2160, {\textbackslash}  format=(string)I420, framerate=(fraction)30/1' ! nvtee ! {\textbackslash}  nvivafilter cuda-process=true {\textbackslash}  customer-lib-name="libnvsample\_cudaprocess.so" ! {\textbackslash} 'video/x-raw(memory:NVMM), format=(string)NV12' ! nvoverlaysink -e  to perform pre/post/cuda image process .  Then I want to associate with opencv to do the image process . So I compile my own ...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/nvsample-cudaprocess-src-with-opencv-compile-error/59892},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\R6FV2RUD\59892.html}
}

@misc{oconnellPETGVsPLA2023,
  title = {{{PETG}} vs {{PLA Filament}}: {{The Main Differences}}},
  shorttitle = {{{PETG}} vs {{PLA Filament}}},
  author = {O'Connell, Jackson},
  year = {2023},
  month = feb,
  journal = {All3DP},
  urldate = {2023-06-16},
  abstract = {Understand PETG vs. PLA to see if your next project calls for the more durable PETG or the easier-to-use PLA.},
  howpublished = {https://all3dp.com/2/petg-vs-pla-3d-printing-filaments-compared/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\JXJW8N54\petg-vs-pla-3d-printing-filaments-compared.html}
}

@misc{omegaverkstedPricePLAFilament,
  title = {Price of {{PLA}} Filament 10 Gram at {{Omega Verksted}}},
  author = {Omega Verksted},
  urldate = {2022-05-18},
  abstract = {Omega Verksted er en forening for elektronikk- og hobbyinteresserte studenter ved Norges Teknisk-Naturvitenskapelige Universitet (NTNU)},
  howpublished = {https://www.omegav.ntnu.no/komp?search=FDM-3D-Printer+filament-PLA+10+gram},
  file = {C:\Users\emilm\Zotero\storage\CUQ4PVPI\komp.html}
}

@misc{OnlineCRC8CRC16,
  title = {Online {{CRC-8 CRC-16 CRC-32 Calculator}}},
  urldate = {2022-05-16},
  howpublished = {https://crccalc.com/},
  file = {C:\Users\emilm\Zotero\storage\B9GD6H7S\crccalc.com.html}
}

@misc{opensourcehardwareassociationResolutionRedefineSPI,
  title = {A {{Resolution}} to {{Redefine SPI Signal Names}}},
  author = {Open Source Hardware Association},
  journal = {Open Source Hardware Association},
  urldate = {2022-05-23},
  abstract = {The electronics industry has been using Master and Slave terminology unabashedly and it needs to stop. Thankfully, the industry is already making a shift.},
  howpublished = {https://www.oshwa.org/a-resolution-to-redefine-spi-signal-names/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\GPZ9BQIQ\a-resolution-to-redefine-spi-signal-names.html}
}

@misc{OpenStaxFreeTextbooks,
  title = {{{OpenStax}} | {{Free Textbooks Online}} with {{No Catch}}},
  urldate = {2023-05-12},
  abstract = {OpenStax offers free college textbooks for all types of students, making education accessible \& affordable for everyone. Browse our list of available subjects!},
  howpublished = {https://openstax.org/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\6KCV96QL\university-physics-volume-3.html}
}

@misc{oquabDINOv2LearningRobust2023,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = {2023},
  month = apr,
  number = {arXiv:2304.07193},
  eprint = {2304.07193},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.07193},
  urldate = {2023-11-01},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RKACZ8ZP\\Oquab et al_2023_DINOv2.pdf;C\:\\Users\\emilm\\Zotero\\storage\\5VLJV997\\2304.html}
}

@misc{OverviewPyGObject,
  title = {Overview {\textemdash} {{PyGObject}}},
  urldate = {2023-05-29},
  howpublished = {https://pygobject.readthedocs.io/en/latest/},
  file = {C:\Users\emilm\Zotero\storage\36GTWEYR\latest.html}
}

@inproceedings{parkIlluminationChangeRobustness2017,
  title = {Illumination Change Robustness in Direct Visual {{SLAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Park, Seonwook and Sch{\"o}ps, Thomas and Pollefeys, Marc},
  year = {2017},
  month = may,
  pages = {4523--4530},
  doi = {10.1109/ICRA.2017.7989525},
  abstract = {Direct visual odometry and Simultaneous Localization and Mapping (SLAM) methods determine camera poses by means of direct image alignment. This optimizes a photometric cost term based on the Lucas-Kanade method. Many recent works use the brightness constancy assumption in the alignment cost formulation and therefore cannot cope with significant illumination changes. Such changes are especially likely to occur for loop closures in SLAM. Alternatives exist which attempt to match images more robustly. In our paper, we perform a systematic evaluation of real-time capable methods. We determine their accuracy and robustness in the context of odometry and of loop closures, both on real images as well as synthetic datasets with simulated lighting changes. We find that for real images, a Census-based method outperforms the others. We make our new datasets available online.},
  keywords = {Brightness,Lighting,Measurement,Optimization,Robustness,Simultaneous localization and mapping,Visualization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\49CWQBZ9\\Park et al. - 2017 - Illumination change robustness in direct visual SL.pdf;C\:\\Users\\emilm\\Zotero\\storage\\EK4MM4UZ\\7989525.html}
}

@article{parkSpatiotemporalCameraLiDARCalibration2020,
  title = {Spatiotemporal {{Camera-LiDAR Calibration}}: {{A Targetless}} and {{Structureless Approach}}},
  shorttitle = {Spatiotemporal {{Camera-LiDAR Calibration}}},
  author = {Park, Chanoh and Moghadam, Peyman and Kim, Soohwan and Sridharan, Sridha and Fookes, Clinton},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2001.06175},
  urldate = {2022-11-26},
  abstract = {The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this paper, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not \{require\} an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Robotics (cs.RO)},
  file = {C:\Users\emilm\Zotero\storage\D9MEBM6Q\Park et al. - 2020 - Spatiotemporal Camera-LiDAR Calibration A Targetl.pdf}
}

@misc{PassingNVMMFrames2020,
  title = {Passing {{NVMM}} Frames to {{Gstreamer}} Appsink to Apply Custom Processing},
  year = {2020},
  month = jul,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-08},
  abstract = {Hello There,   TL;DR  How do you access NVMM buffers from nvarguscamerassrc within an appsink callback for furthrue processing with CUDA?  In Detail I'm trying to port a computer vision application on to the Jetson Xavier AGX platform.  The use-case focuses on low latency and high throughput so we're looking to implement an efficient and low overhead processing pipeline using NVIDIA's libraries.  the basic architecture should look like this  nvarguscamerassrc (memory:NVMM) -{$>$} nvvideoconvert (mem...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/passing-nvmm-frames-to-gstreamer-appsink-to-apply-custom-processing/142286},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\D5F6SZJB\142286.html}
}

@misc{PETGVsPLA2021,
  title = {{{PETG}} vs {{PLA Filament}}: {{The Differences}}},
  shorttitle = {{{PETG}} vs {{PLA Filament}}},
  year = {2021},
  month = apr,
  journal = {All3DP},
  urldate = {2022-05-19},
  abstract = {Learn the differences between PLA and PETG filament to see if your next project calls for durable PETG or environmentally-friendly PLA.},
  howpublished = {https://all3dp.com/2/petg-vs-pla-3d-printing-filaments-compared/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\MLA4Q2W8\petg-vs-pla-3d-printing-filaments-compared.html}
}

@misc{photoEnglishJohnMoulton2004,
  title = {English:  {{The John Moulton Barn}} on {{Mormon Row}} at the Base of the {{Grand Tetons}}, {{Wyoming}}.},
  shorttitle = {English},
  author = {Photo, PD, Jon Sullivan},
  year = {Taken on~19 June 2004},
  urldate = {2023-06-14},
  copyright = {Public domain},
  file = {C:\Users\emilm\Zotero\storage\EKEKMBD8\FileBarns_grand_tetons.html}
}

@article{piascoSurveyVisualBasedLocalization2018,
  title = {A Survey on {{Visual-Based Localization}}: {{On}} the Benefit of Heterogeneous Data},
  shorttitle = {A Survey on {{Visual-Based Localization}}},
  author = {Piasco, Nathan and Sidib{\'e}, D{\'e}sir{\'e} and Demonceaux, C{\'e}dric and {Gouet-Brunet}, Val{\'e}rie},
  year = {2018},
  month = feb,
  journal = {Pattern Recognition},
  volume = {74},
  pages = {90--109},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.09.013},
  urldate = {2022-11-27},
  abstract = {We are surrounded by plenty of information about our environment. From these multiple sources, numerous data could be extracted: set of images, 3D model, coloured points cloud... When classical localization devices failed (e.g. GPS sensor in cluttered environments), aforementioned data could be used within a localization framework. This is called Visual Based Localization (VBL). Due to numerous data types that can be collected from a scene, VBL encompasses a large amount of different methods. This paper presents a survey about recent methods that localize a visual acquisition system according to a known environment. We start by categorizing VBL methods into two distinct families: indirect and direct localization systems. As the localization environment is almost always dynamic, we pay special attention to methods designed to handle appearances changes occurring in a scene. Thereafter, we highlight methods exploiting heterogeneous types of data. Finally, we conclude the paper with a discussion on promising trends that could permit to a localization system to reach high precision pose estimation within an area as large as possible.},
  langid = {english},
  keywords = {Camera relocalisation,Image-based localization,Pose estimation,Visual geo-localization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CY33I5PV\\Piasco et al. - 2018 - A survey on Visual-Based Localization On the bene.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3F4PBITU\\S0031320317303448.html}
}

@techreport{PicoDatasheet,
  title = {Raspberry {{Pi Pico Datasheet}}},
  author = {Raspberry Pi Trading Ltd},
  year = {2021},
  month = nov,
  number = {150df05-clean},
  file = {C:\Users\emilm\Zotero\storage\HQTNXTRK\Raspberry Pi Pico Datasheet.pdf}
}

@techreport{PicoSDK,
  type = {Datasheet},
  title = {Raspberry {{Pi Pico C}}/{{C}}++ {{SDK}}},
  author = {Raspberry Pi Trading Ltd},
  year = {2021},
  month = nov,
  number = {150df05-clean},
  pages = {375},
  urldate = {2022-05-02},
  keywords = {datasheet},
  file = {C:\Users\emilm\Zotero\storage\IYDRHITR\Raspberry Pi Pico CC++ SDK.pdf}
}

@misc{pinboard[@pinboard]ProgrammersCredoWe2016,
  type = {Tweet},
  title = {The {{Programmers}}' {{Credo}}: We Do These Things Not Because They Are Easy, but Because We Thought They Were Going to Be Easy},
  author = {{Pinboard [@Pinboard]}},
  year = {2016},
  month = aug,
  journal = {Twitter},
  urldate = {2023-05-27},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\XSCDT8FP\761656824202276864.html}
}

@misc{player_onePermanentMountsLinux,
  title = {Permanent Mounts - {{Linux Filesystems}} 101 - {{Block Devices}}},
  author = {{player\_one}},
  journal = {CodinGame},
  urldate = {2022-05-11},
  abstract = {Explore this playground and try new concepts right into your browser},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\A8DC89EW\permanent-mounts.html}
}

@misc{PleaseProvideMore2022,
  title = {Please Provide More Detailed Guidance to Flash {{Jetson Nano}} with {{Windows}} 11 {{PC}}},
  year = {2022},
  month = dec,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-15},
  abstract = {RESOLVED 13 December:  After a couple of days of frustration, this is the best advice for Windows 11 users not experienced with Linux other than running containers in WSL2.  It is best to get two USB or external drives with size at least 64 GB, or even buy a small PC to install Linux on it. The small PC will avoid you might kill/ format your Windows installation as I did.  If you have 2 USB's you install first ISO on USB 1 and then boot from that USB to install on USB 2 your final image. This is...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/please-provide-more-detailed-guidance-to-flash-jetson-nano-with-windows-11-pc/236808},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\G5FG7CZW\236808.html}
}

@misc{plotlyClientsideCallbacksDash,
  title = {Clientside {{Callbacks}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-06-01},
  howpublished = {https://dash.plotly.com/clientside-callbacks},
  file = {C:\Users\emilm\Zotero\storage\KHDQZLJ2\clientside-callbacks.html}
}

@misc{plotlyLiveUpdatesDash,
  title = {Live {{Updates}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-06-01},
  howpublished = {https://dash.plotly.com/live-updates},
  file = {C:\Users\emilm\Zotero\storage\5YRY443Q\live-updates.html}
}

@misc{plotlyLiveUpdatesDasha,
  title = {Live {{Updates}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-06-08},
  howpublished = {https://dash.plotly.com/live-updates},
  file = {C:\Users\emilm\Zotero\storage\YEKWSH28\live-updates.html}
}

@misc{plotlyMinimalDashApp,
  title = {A {{Minimal Dash App}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-05-15},
  howpublished = {https://dash.plotly.com/minimal-app},
  file = {C:\Users\emilm\Zotero\storage\AR2B82DF\minimal-app.html}
}

@misc{plotlyMultiPageAppsURL,
  title = {Multi-{{Page Apps}} and {{URL Support}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-06-03},
  howpublished = {https://dash.plotly.com/urls},
  file = {C:\Users\emilm\Zotero\storage\CDWHV3YH\urls.html}
}

@misc{plotlyPartBasicCallbacks,
  title = {Part 2. {{Basic Callbacks}} | {{Dash}} for {{Python Documentation}} | {{Plotly}}},
  author = {Plotly},
  urldate = {2023-06-01},
  howpublished = {https://dash.plotly.com/basic-callbacks},
  file = {C:\Users\emilm\Zotero\storage\WWEMEXJA\basic-callbacks.html}
}

@misc{plotlyPlotlyLowCodeData,
  title = {Plotly: {{Low-Code Data App Development}}},
  shorttitle = {Plotly},
  author = {Plotly},
  urldate = {2023-06-08},
  abstract = {With roots in the open-source community and worldwide customers, Plotly is a category-defining leader in enabling data-driven decisions via data apps.},
  howpublished = {https://plotly.com/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\RAPY5T3Z\plotly.com.html}
}

@inproceedings{pokorny2014a,
  title = {Grasp Moduli Spaces and Spherical Harmonics},
  booktitle = {Proc. of the {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Pokorny, Florian T. and Bekiroglu, Yasemin and Kragic, Danica},
  year = {2014},
  address = {{Hongkong, China}}
}

@misc{pokornyFlorianPokornyKTH2023,
  title = {Florian {{T}}. {{Pokorny}} | {{KTH}}},
  author = {Pokorny, Florian T.},
  year = {2023},
  urldate = {2023-12-06},
  howpublished = {https://www.csc.kth.se/{\textasciitilde}fpokorny/codeanddata/harmonicregression.html},
  file = {C:\Users\emilm\Zotero\storage\Z9ESHC4V\harmonicregression.html}
}

@misc{Polarization2016,
  title = {1.8: {{Polarization}}},
  shorttitle = {1.8},
  year = {2016},
  month = nov,
  journal = {Physics LibreTexts},
  urldate = {2023-05-12},
  abstract = {Polarization is the attribute that wave oscillations have a definite direction relative to the direction of propagation of the wave. The direction of polarization is defined to be the direction {\ldots}},
  howpublished = {https://phys.libretexts.org/Bookshelves/University\_Physics/Book\%3A\_University\_Physics\_(OpenStax)/University\_Physics\_III\_-\_Optics\_and\_Modern\_Physics\_(OpenStax)/01\%3A\_The\_Nature\_of\_Light/1.08\%3A\_Polarization},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\IQXIVBYV\1.html}
}

@article{PolarizationPhysics2023,
  title = {Polarization (Physics)},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-05-12},
  abstract = {Polarization (also polarisation) is a property of transverse waves which specifies the geometrical orientation of the oscillations.  In a transverse wave, the direction of the oscillation is perpendicular to the direction of motion of the wave.  A simple example of a polarized transverse wave is vibrations traveling along a taut string (see image); for example, in a musical instrument like a guitar string. Depending on how the string is plucked, the vibrations can be in a vertical direction, horizontal direction, or at any angle perpendicular to the string.  In contrast, in longitudinal waves, such as sound waves in a liquid or gas, the displacement of the particles in the oscillation is always in the direction of propagation, so these waves do not exhibit polarization.  Transverse waves that exhibit polarization include electromagnetic waves such as light and radio waves, gravitational waves, and transverse sound waves (shear waves) in solids. An electromagnetic wave such as light consists of a coupled oscillating electric field and magnetic field which are always perpendicular to each other; by convention, the "polarization" of electromagnetic waves refers to the direction of the electric field.  In linear polarization, the fields oscillate in a single direction.  In circular or elliptical polarization, the fields rotate at a constant rate in a plane as the wave travels, either in the right-hand or in the left-hand direction. Light or other electromagnetic radiation from many sources, such as the sun, flames, and incandescent lamps, consists of short wave trains with an equal mixture of polarizations; this is called unpolarized light.  Polarized light can be produced by passing unpolarized light through a polarizer, which allows waves of only one polarization to pass through. The most common optical materials do not affect the polarization of light, but some materials{\textemdash}those that exhibit birefringence, dichroism, or optical activity{\textemdash}affect light differently depending on its polarization.  Some of these are used to make polarizing filters.  Light also becomes partially polarized when it reflects at an angle from a surface. According to quantum mechanics, electromagnetic waves can also be viewed as streams of particles called photons.  When viewed in this way, the polarization of an electromagnetic wave is determined by a quantum mechanical property of photons called their spin.  A photon has one of two possible spins: it can either spin in a right hand sense or a left hand sense about its direction of travel. Circularly polarized electromagnetic waves are composed of photons with only one type of spin, either right- or left-hand.  Linearly polarized waves consist of photons that are in a superposition of right and left circularly polarized states, with equal amplitude and phases synchronized to give oscillation in a plane.Polarization is an important parameter in areas of science dealing with transverse waves, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1152119129},
  file = {C:\Users\emilm\Zotero\storage\3GSG4SMW\Polarization_(physics).html}
}

@misc{PolarizedThermalHybrid2020,
  title = {Polarized {{Thermal Hybrid Cameras}} for {{Improved Detection}} of {{Oil Spills}} | {{Inside Oi}}},
  year = {2020},
  month = sep,
  urldate = {2023-02-06},
  abstract = {LWIR imaging has proven useful in detecting oil spills on water... This article describes testing that was done at Ohmsett and the improved oil detection that resulted from the addition of polarization data to an image.},
  howpublished = {https://inside.oceanologyinternational.com/2020/09/13/polarized-thermal-hybrid-cameras-for-improved-detection-of-oil-spills/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\CAN85CN8\polarized-thermal-hybrid-cameras-for-improved-detection-of-oil-spills.html}
}

@article{Polarizer2023,
  title = {Polarizer},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-05-12},
  abstract = {A polarizer or polariser (see spelling differences) is an optical filter that lets light waves of a specific polarization pass through while blocking light waves of other polarizations. It can filter a beam of light of undefined or mixed polarization into a beam of well-defined polarization, that is polarized light. The common types of polarizers are linear polarizers and circular polarizers. Polarizers are used in many optical techniques and instruments, and polarizing filters find applications in photography and LCD technology.  Polarizers can also be made for other types of electromagnetic waves besides visible light, such as radio waves, microwaves, and X-rays.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1152014034},
  file = {C:\Users\emilm\Zotero\storage\TUXAN4WC\Polarizer.html}
}

@misc{polymerdatabasePolyMethylMethacrylate,
  title = {Poly(Methyl Methacrylate)},
  author = {{polymerdatabase}},
  urldate = {2022-05-18},
  howpublished = {https://polymerdatabase.com/polymers/polymethylmethacrylate.html},
  file = {C:\Users\emilm\Zotero\storage\TSYL7V7P\polymethylmethacrylate.html}
}

@article{pomerleauReviewPointCloud2015,
  title = {A {{Review}} of {{Point Cloud Registration Algorithms}} for {{Mobile Robotics}}},
  author = {Pomerleau, Fran{\c c}ois and Colas, Francis and Siegwart, Roland},
  year = {2015},
  journal = {Foundations and Trends in Robotics},
  volume = {4},
  number = {1},
  pages = {1--104},
  publisher = {{Now Publishers}},
  doi = {10.1561/2300000035},
  urldate = {2022-11-27},
  abstract = {The topic of this review is geometric registration in robotics. Registration algorithms associate sets of data into a common coordinate system. They have been used extensively in object reconstruction, inspection, medical application, and localization of mobile robotics. We focus on mobile robotics applications in which point clouds are to be registered. While the underlying principle of those algorithms is simple, many variations have been proposed for many different applications. In this review, we give a historical perspective of the registration problem and show that the plethora of solutions can be organized and differentiated according to a few elements. Accordingly, we present a formalization of geometric registration and cast algorithms proposed in the literature into this framework. Finally, we review a few applications of this framework in mobile robotics that cover different kinds of platforms, environments, and tasks. These examples allow us to study the specific requirements of each use case and the necessary configuration choices leading to the registration implementation. Ultimately, the objective of this review is to provide guidelines for the choice of geometric registration configuration.},
  file = {C:\Users\emilm\Zotero\storage\AJZZZP3X\Pomerleau et al. - 2015 - A Review of Point Cloud Registration Algorithms fo.pdf}
}

@misc{pommereningFingerJoints2022,
  title = {Finger {{Joints}}},
  author = {Pommerening, Florian},
  year = {2022},
  month = apr,
  urldate = {2022-05-16},
  abstract = {Fusion 360 add-in for creating finger joints},
  keywords = {spare-time}
}

@misc{pooleDreamFusionTextto3DUsing2022,
  title = {{{DreamFusion}}: {{Text-to-3D}} Using {{2D Diffusion}}},
  shorttitle = {{{DreamFusion}}},
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14988},
  eprint = {2209.14988},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-25},
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8E538773\\Poole et al_2022_DreamFusion.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3APKQ4DE\\2209.html}
}

@misc{PPSJetsonNano2020,
  title = {{{PPS}} Jetson Nano},
  year = {2020},
  month = may,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-22},
  abstract = {Hey  I want to enable PPS on jetson nano for pin 18. I have read various threads on this forum, but things are a bit unclear for me.  I have e.g read this topic   So I downloaded the kernel sources from this link L4T Driver Package (BSP) Sources  and made these changes  pps \{         gpios = {$<\&$}gpio TEGRA\_GPIO(B, 7) 0{$>$};          compatible = "pps-gpio";         status = "okay";     \};  just below  gpio: gpio@6000d000  (as suggested in another thread)  I made the change in  Linux\_for\_Tegra/sources...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/pps-jetson-nano/121921},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\K894EKSU\1.html}
}

@misc{PPSNotWorking,
  title = {{{PPS}} Not Working}
}

@article{prasadAreObjectDetection2020,
  title = {Are {{Object Detection Assessment Criteria Ready}} for {{Maritime Computer Vision}}?},
  author = {Prasad, Dilip K. and Dong, Huixu and Rajan, Deepu and Quek, Chai},
  year = {2020},
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {21},
  number = {12},
  pages = {5295--5304},
  issn = {1558-0016},
  doi = {10.1109/TITS.2019.2954464},
  abstract = {Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.},
  keywords = {Artificial intelligence,Computer vision,Image edge detection,intelligent vehicles,Intelligent vehicles,marine vehicles,Marine vehicles,Object detection,performance evaluation,Performance evaluation,Sensors},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Q79HJYIP\\Prasad et al. - 2020 - Are Object Detection Assessment Criteria Ready for.pdf;C\:\\Users\\emilm\\Zotero\\storage\\8XYEJL7W\\8911242.html}
}

@article{prasadVideoProcessingElectroOptical2017,
  title = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}: {{A Survey}}},
  shorttitle = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}},
  author = {Prasad, Dilip K. and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {18},
  number = {8},
  pages = {1993--2016},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2016.2634580},
  urldate = {2022-07-19},
  keywords = {Annette\_tip},
  file = {C:\Users\emilm\Zotero\storage\47WJ7NTT\Prasad et al. - 2017 - Video Processing From Electro-Optical Sensors for .pdf}
}

@misc{projectgutenbergCompleteWorksWilliam1994,
  title = {The {{Complete Works}} of {{William Shakespeare}}},
  author = {Project Gutenberg},
  year = {1994},
  month = jan,
  urldate = {2023-06-15},
  file = {C:\Users\emilm\Zotero\storage\RE48FHL4\t8.shakespeare.html}
}

@techreport{proto-advantagePCB3005A1ChipQuik,
  title = {{{PCB3005A1 Chip Quik Inc}}. | {{Prototyping}}, {{Fabrication Products}} | {{DigiKey}}},
  author = {{Proto-Advantage}},
  urldate = {2022-05-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\C9KTSAAW\\PCB3005A1.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3QPTPM8U\\5978207.html}
}

@misc{pygobjectTypingStubsPyGObject2023,
  title = {Typing {{Stubs}} for {{PyGObject}}},
  author = {PyGObject},
  year = {2023},
  month = may,
  urldate = {2023-05-29},
  abstract = {PEP 561 Typing Stubs for PyGObject},
  copyright = {LGPL-2.1},
  howpublished = {PyGObject}
}

@misc{pythonsoftwarefoundationAsyncioAsynchronous,
  title = {Asyncio {\textemdash} {{Asynchronous I}}/{{O}}},
  author = {Python Software Foundation},
  journal = {Python documentation},
  urldate = {2023-06-26},
  abstract = {Hello World!: asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance n...},
  howpublished = {https://docs.python.org/3/library/asyncio.html},
  file = {C:\Users\emilm\Zotero\storage\Z32AXUWY\asyncio.html}
}

@misc{pythonsoftwarefoundationMultiprocessingProcessbasedParallelism,
  title = {Multiprocessing {\textemdash} {{Process-based}} Parallelism},
  author = {Python Software Foundation},
  journal = {Python documentation},
  urldate = {2023-06-26},
  abstract = {Source code: Lib/multiprocessing/ Availability: not Emscripten, not WASI. This module does not work or is not available on WebAssembly platforms wasm32-emscripten and wasm32-wasi. See WebAssembly p...},
  howpublished = {https://docs.python.org/3/library/multiprocessing.html},
  file = {C:\Users\emilm\Zotero\storage\NV7CGRQY\multiprocessing.html}
}

@misc{pythonsoftwarefoundationPythonProfilers,
  title = {The {{Python Profilers}}},
  author = {Python Software Foundation},
  journal = {Python documentation},
  urldate = {2023-06-26},
  abstract = {Source code: Lib/profile.py and Lib/pstats.py Introduction to the profilers: cProfile and profile provide deterministic profiling of Python programs. A profile is a set of statistics that describes...},
  howpublished = {https://docs.python.org/3/library/profile.html},
  file = {C:\Users\emilm\Zotero\storage\NF97IZ6A\profile.html}
}

@misc{pythonsoftwarefoundationSubprocessSubprocessManagement,
  title = {Subprocess {\textemdash} {{Subprocess}} Management},
  author = {Python Software Foundation},
  journal = {Python documentation},
  urldate = {2023-06-26},
  abstract = {Source code: Lib/subprocess.py The subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace seve...},
  howpublished = {https://docs.python.org/3/library/subprocess.html},
  file = {C:\Users\emilm\Zotero\storage\9RVFFQF2\subprocess.html}
}

@misc{PythonWhatLimitations,
  title = {Python - {{What}} Limitations Does the {{GIL}} Impose on Using {{GStreamer}} through {{PyGObject}}? - {{Stack Overflow}}},
  urldate = {2023-04-08},
  howpublished = {https://stackoverflow.com/questions/70709534/what-limitations-does-the-gil-impose-on-using-gstreamer-through-pygobject},
  file = {C:\Users\emilm\Zotero\storage\5AAECDXY\what-limitations-does-the-gil-impose-on-using-gstreamer-through-pygobject.html}
}

@misc{pytorchcontributorsCUDASemanticsPyTorch2023,
  title = {{{CUDA}} Semantics {\textemdash} {{PyTorch}} 2.1 Documentation},
  author = {PyTorch Contributors},
  year = {2023},
  urldate = {2023-12-05},
  howpublished = {https://pytorch.org/docs/stable/notes/cuda.html\#cuda-streams},
  file = {C:\Users\emilm\Zotero\storage\9EC8N33R\cuda.html}
}

@misc{Pyubx22022,
  title = {Pyubx2},
  year = {2022},
  month = may,
  urldate = {2022-05-28},
  abstract = {Python library for parsing and generating UBX GPS/GNSS protocol messages.},
  copyright = {BSD-3-Clause},
  howpublished = {SEMU Consulting},
  keywords = {gnss,gps,gps-library,u-blox,ubx,ubx-gps-library,ubx-messages,ubx-parser,ubx-protocol}
}

@misc{quartUsingWebsocketsQuart,
  title = {Using Websockets {\textemdash} {{Quart}} 0.17.0 Documentation},
  author = {Quart},
  urldate = {2023-06-03},
  howpublished = {https://pgjones.gitlab.io/quart/how\_to\_guides/websockets.html},
  file = {C:\Users\emilm\Zotero\storage\WDMZYU6I\websockets.html}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2023-11-01},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RFDZLT54\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf;C\:\\Users\\emilm\\Zotero\\storage\\9I6RLUBF\\2103.html}
}

@misc{RadixSortRevisited,
  title = {Radix {{Sort Revisited}}},
  urldate = {2023-10-24},
  howpublished = {https://www.codercorner.com/RadixSortRevisited.htm},
  file = {C:\Users\emilm\Zotero\storage\RGGMPBQ3\RadixSortRevisited.html}
}

@article{raguramUSACUniversalFramework2013,
  title = {{{USAC}}: {{A Universal Framework}} for {{Random Sample Consensus}}},
  shorttitle = {{{USAC}}},
  author = {Raguram, Rahul and Chum, Ondrej and Pollefeys, Marc and Matas, Jiri and Frahm, Jan-Michael},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {2022--2038},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.257},
  abstract = {A computational problem that arises frequently in computer vision is that of estimating the parameters of a model from data that have been contaminated by noise and outliers. More generally, any practical system that seeks to estimate quantities from noisy data measurements must have at its core some means of dealing with data contamination. The random sample consensus (RANSAC) algorithm is one of the most popular tools for robust estimation. Recent years have seen an explosion of activity in this area, leading to the development of a number of techniques that improve upon the efficiency and robustness of the basic RANSAC algorithm. In this paper, we present a comprehensive overview of recent research in RANSAC-based robust estimation by analyzing and comparing various approaches that have been explored over the years. We provide a common context for this analysis by introducing a new framework for robust estimation, which we call Universal RANSAC (USAC). USAC extends the simple hypothesize-and-verify structure of standard RANSAC to incorporate a number of important practical and computational considerations. In addition, we provide a general-purpose C++ software library that implements the USAC framework by leveraging state-of-the-art algorithms for the various modules. This implementation thus addresses many of the limitations of standard RANSAC within a single unified package. We benchmark the performance of the algorithm on a large collection of estimation problems. The implementation we provide can be used by researchers either as a stand-alone tool for robust estimation or as a benchmark for evaluating new techniques.},
  keywords = {Algorithm design and analysis,Computational modeling,Context,Data models,Estimation,RANSAC,robust estimation,Robustness,Standards},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5Y7TV54N\\Raguram et al. - 2013 - USAC A Universal Framework for Random Sample Cons.pdf;C\:\\Users\\emilm\\Zotero\\storage\\2RK53FRS\\6365642.html}
}

@misc{rAnswerHowDisplay2013,
  title = {Answer to "{{How}} to Display an Image Stored as Byte Array in {{HTML}}/{{JavaScript}}?"},
  shorttitle = {Answer to "{{How}} to Display an Image Stored as Byte Array in {{HTML}}/{{JavaScript}}?},
  author = {R, Sridhar},
  year = {2013},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2023-05-26},
  file = {C:\Users\emilm\Zotero\storage\XTN2AIM6\how-to-display-an-image-stored-as-byte-array-in-html-javascript.html}
}

@article{raoMultimodalLearningInference2017,
  title = {Multimodal Learning and Inference from Visual and Remotely Sensed Data},
  author = {Rao, Dushyant and De Deuge, Mark and {Nourani{\textendash}Vatani}, Navid and Williams, Stefan B and Pizarro, Oscar},
  year = {2017},
  month = jan,
  journal = {The International Journal of Robotics Research},
  volume = {36},
  number = {1},
  pages = {24--43},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364916679892},
  urldate = {2023-11-01},
  abstract = {Autonomous vehicles are often tasked to explore unseen environments, aiming to acquire and understand large amounts of visual image data and other sensory information. In such scenarios, remote sensing data may be available a priori, and can help to build a semantic model of the environment and plan future autonomous missions. In this paper, we introduce two multimodal learning algorithms to model the relationship between visual images taken by an autonomous underwater vehicle during a survey and remotely sensed acoustic bathymetry (ocean depth) data that is available prior to the survey. We present a multi-layer architecture to capture the joint distribution between the bathymetry and visual modalities. We then propose an extension based on gated feature learning models, which allows the model to cluster the input data in an unsupervised fashion and predict visual image features using just the ocean depth information. Our experiments demonstrate that multimodal learning improves semantic classification accuracy regardless of which modalities are available at classification time, allows for unsupervised clustering of either or both modalities, and can facilitate mission planning by enabling class-based or image-based queries.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\XLJFIBNE\Rao et al_2017_Multimodal learning and inference from visual and remotely sensed data.pdf}
}

@misc{RaspberryPiPico2023,
  title = {Raspberry {{Pi Pico SDK Examples}}},
  year = {2023},
  month = jun,
  urldate = {2023-06-10},
  copyright = {BSD-3-Clause},
  howpublished = {Raspberry Pi}
}

@misc{RaspberryPiPicoR3,
  title = {Raspberry {{Pi Pico-R3}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  urldate = {2022-05-15},
  howpublished = {https://grabcad.com/library/raspberry-pi-pico-r3-1},
  file = {C:\Users\emilm\Zotero\storage\8YZMSB7U\raspberry-pi-pico-r3-1.html}
}

@article{Rec6012023,
  title = {Rec. 601},
  year = {2023},
  month = jun,
  journal = {Wikipedia},
  urldate = {2023-06-14},
  abstract = {ITU-R Recommendation BT.601, more commonly known by the abbreviations Rec. 601 or BT.601 (or its former name CCIR 601) is a standard originally issued in 1982 by the CCIR (an organization, which has since been renamed as the International Telecommunication Union {\textendash}  Radiocommunication sector) for encoding interlaced analog video signals in digital video form. It includes methods of encoding 525-line 60 Hz and 625-line 50 Hz signals, both with an active region covering 720 luminance samples and 360 chrominance samples per line. The color encoding system is known as YCbCr 4:2:2. The Rec. 601 video raster format has been re-used in a number of later standards, including the ISO/IEC MPEG and ITU-T H.26x compressed formats, although compressed formats for consumer applications usually use chroma subsampling reduced from the 4:2:2 sampling specified in Rec. 601 to 4:2:0. The standard has been revised several times in its history. Its seventh edition, referred to as BT.601-7, was approved in March 2011 and was formally published in October 2011.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1158794188},
  file = {C:\Users\emilm\Zotero\storage\HETKU5LQ\Rec.html}
}

@misc{red5proKeyReasonsAV12023,
  title = {5 {{Key Reasons AV1 Will Play}} a {{Big Role}} in {{WebRTC Streaming}}},
  author = {Red5 Pro},
  year = {2023},
  month = feb,
  journal = {Red5 Pro},
  urldate = {2023-05-26},
  abstract = {The Red5 Pro Blog. Milliseconds to Millions.},
  howpublished = {https://www.red5pro.com/blog/av1-webrtc-streaming/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\K3HTCJZI\av1-webrtc-streaming.html}
}

@misc{redhat10ChangingNetwork,
  title = {10.2. {{Changing Network Kernel Settings Red Hat Enterprise Linux}} 5 | {{Red Hat Customer Portal}}},
  author = {Red Hat},
  urldate = {2023-05-02},
  howpublished = {https://access.redhat.com/documentation/en-us/red\_hat\_enterprise\_linux/5/html/tuning\_and\_optimizing\_red\_hat\_enterprise\_linux\_for\_oracle\_9i\_and\_10g\_databases/sect-oracle\_9i\_and\_10g\_tuning\_guide-adjusting\_network\_settings-changing\_network\_kernel\_settings},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\8PQHU6VM\sect-oracle_9i_and_10g_tuning_guide-adjusting_network_settings-changing_network_kernel_settings.html}
}

@article{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  journal = {arXiv:1506.02640 [cs]},
  eprint = {1506.02640},
  primaryclass = {cs},
  urldate = {2022-05-02},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,nn,yolo},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SPZEHMCH\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3VFZJJ3L\\1506.html}
}

@article{ReferenceFrameVideo2023,
  title = {Reference Frame (Video)},
  year = {2023},
  month = feb,
  journal = {Wikipedia},
  urldate = {2023-06-01},
  abstract = {Reference frames are frames of a compressed video that are used to define future frames. As such, they are only used in inter-frame compression techniques. In older video encoding standards, such as MPEG-2, only one reference frame {\textendash} the previous frame {\textendash} was used for P-frames. Two reference frames (one past and one future) were used for B-frames.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1140001239},
  file = {C:\Users\emilm\Zotero\storage\RNFQSHTS\Reference_frame_(video).html}
}

@misc{ReversePathFiltering,
  title = {Reverse {{Path Filtering}}},
  urldate = {2023-05-02},
  howpublished = {https://tldp.org/HOWTO/Adv-Routing-HOWTO/lartc.kernel.rpf.html},
  file = {C:\Users\emilm\Zotero\storage\4M8E4ENM\lartc.kernel.rpf.html}
}

@inproceedings{richardsonAprilCalAssistedRepeatable2013,
  title = {{{AprilCal}}: {{Assisted}} and Repeatable Camera Calibration},
  shorttitle = {{{AprilCal}}},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Richardson, Andrew and Strom, Johannes and Olson, Edwin},
  year = {2013},
  month = nov,
  pages = {1814--1821},
  publisher = {{IEEE}},
  address = {{Tokyo}},
  doi = {10.1109/IROS.2013.6696595},
  urldate = {2022-11-25},
  isbn = {978-1-4673-6358-7 978-1-4673-6357-0},
  file = {C:\Users\emilm\Zotero\storage\FQB4ZUW4\Richardson et al. - 2013 - AprilCal Assisted and repeatable camera calibrati.pdf}
}

@misc{ridgerunNVIDIAH265Encoding2021,
  title = {{{NVIDIA H265 Encoding Configurations}} | {{NVIDIA H265 Encoding}} | {{RidgeRun}} - {{RidgeRun Developer Connection}}},
  author = {RidgeRun},
  year = {2021},
  month = dec,
  urldate = {2023-06-01},
  howpublished = {https://developer.ridgerun.com/wiki/index.php/NVIDIA\_H265\_Encoding\_Configurations},
  file = {C:\Users\emilm\Zotero\storage\GXRRMEN2\NVIDIA_H265_Encoding_Configurations.html}
}

@misc{rigbyWhatPolarisedLight2021,
  title = {What Is Polarised Light?},
  author = {Rigby, Sara},
  year = {2021},
  month = apr,
  journal = {BBC Science Focus Magazine},
  urldate = {2023-05-12},
  abstract = {What is polarisation and why is it in my sunglasses?},
  howpublished = {https://www.sciencefocus.com/science/what-is-polarised-light/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\WF24SZM8\what-is-polarised-light.html}
}

@misc{rigerunNVIDIAJetpackFlashing2021,
  title = {{{NVIDIA Jetpack Flashing}} with Initrd | {{RidgeRun Developer}} - {{RidgeRun Developer Connection}}},
  author = {RigeRun},
  year = {2021},
  month = sep,
  urldate = {2023-05-22},
  howpublished = {https://developer.ridgerun.com/wiki/index.php?title=NVIDIA\_Jetpack\_Flashing\_with\_initrd},
  file = {C:\Users\emilm\Zotero\storage\SJ85BKYU\index.html}
}

@misc{rigerunNVIDIAJetsonHacks2023,
  title = {{{NVIDIA Jetson Hacks}} | {{Jetson Hacks}} | {{RidgeRun}} - {{RidgeRun Developer Connection}}},
  author = {RigeRun},
  year = {2023},
  month = feb,
  urldate = {2023-05-08},
  howpublished = {https://developer.ridgerun.com/wiki/index.php/NVIDIA\_Jetson\_Hacks},
  file = {C:\Users\emilm\Zotero\storage\BD6KFXD5\NVIDIA_Jetson_Hacks.html}
}

@misc{rigerunNVIDIAJetsonXavier2023,
  title = {{{NVIDIA Jetson Xavier}} - {{GPU}} - {{RidgeRun Developer Connection}}},
  author = {RigeRun},
  year = {2023},
  month = feb,
  urldate = {2023-05-23},
  howpublished = {https://developer.ridgerun.com/wiki/index.php/Xavier/Processors/GPU},
  file = {C:\Users\emilm\Zotero\storage\RDQF5M6G\GPU.html}
}

@incollection{rodsethSocietalImpactsAutonomous2023,
  title = {The {{Societal Impacts}} of {{Autonomous Ships}}: {{The Norwegian Perspective}}},
  shorttitle = {The {{Societal Impacts}} of {{Autonomous Ships}}},
  booktitle = {Autonomous {{Vessels}} in {{Maritime Affairs}}: {{Law}} and {{Governance Implications}}},
  author = {R{\o}dseth, {\O}rnulf Jan and Nesheim, Dag Atle and Rialland, Agathe and Holte, Even Ambros},
  editor = {Johansson, Tafsir Matin and Fern{\'a}ndez, Jonatan Echebarria and Dalaklis, Dimitrios and Pastra, Aspasia and Skinner, Jon A.},
  year = {2023},
  series = {Studies in {{National Governance}} and {{Emerging Technologies}}},
  pages = {357--376},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-24740-8_18},
  urldate = {2023-09-12},
  abstract = {It is often assumed that the driving force behind autonomous ship discussions is reduction in crew cost or societal concerns like improved maritime safety due to reduction in human errors, better working conditions for crew in land-based control centers, or sustainability goals achieved through higher efficiency and lower emissions. But in assessing actual projects that are underway other factors also stand out. For the container ship Yara Birkeland, for example, the major drivers were reduction in local truck transport through urban areas and the realizing of a completely green electric transport. For ASKO cargo ferries, improved reliability of their inter-warehouse transport was a significant additional factor. This chapter will discuss possible societal benefits and potential drawbacks of autonomous ships, as presented in literature as well as from our own research particularly from the European Union (EU) projects Advanced, Efficient and Green Intermodal Systems (AEGIS) and Autonomous Shipping Initiative for European Waters (AUTOSHIP), and the Norwegian project ``Smartere Transport''. The focus is on cargo transport, but some concerns for passenger transport will also be explored. The analysis is mostly qualitative, but some quantitative key performance indicators (KPI) will be proposed. The perspective is mainly Norwegian, a society with a high living standard and a sparsely populated nation, where advanced ship technology is a necessary part of life and in general positively regarded. Nevertheless, many of the observations are deemed highly applicable to other countries and regions.},
  isbn = {978-3-031-24740-8},
  langid = {english},
  keywords = {Autonomous shipping,Autonomy,KPI,Societal impact},
  file = {C:\Users\emilm\Zotero\storage\7MUMUY7I\Rødseth et al. - 2023 - The Societal Impacts of Autonomous Ships The Norw.pdf}
}

@misc{rosskorskyDXFLaserFusion,
  title = {{{DXF}} for {{Laser}} | {{Fusion}} 360 | {{Autodesk App Store}}},
  author = {Ross Korsky},
  urldate = {2022-05-16},
  howpublished = {https://apps.autodesk.com/FUSION/en/Detail/Index?id=7634902334100976871\&appLang=en\&os=Mac},
  file = {C:\Users\emilm\Zotero\storage\CC9RQHTS\Index.html}
}

@techreport{RP2040Datasheet,
  title = {{{RP2040 Datasheet}}},
  author = {Raspberry Pi Trading Ltd},
  year = {2021},
  month = nov,
  number = {150df05-clean},
  file = {C:\Users\emilm\Zotero\storage\TWLQHE2D\rp2040-datasheet.pdf}
}

@misc{rscomponentsRoseKriegerConnecting,
  title = {Rose+{{Krieger Connecting Component}}, {{Parallel Clamp}}, Strut Profile 30 Mm | {{RS Components}}},
  author = {RS Components},
  urldate = {2022-05-16},
  howpublished = {https://no.rs-online.com/web/p/connecting-components/4489615/?sra=pmpn},
  file = {C:\Users\emilm\Zotero\storage\87XYKYBK\4489615.html}
}

@misc{rscomponentsRSPROAluminium,
  title = {{{RS PRO Aluminium Strut}}, 40 x 40 Mm, 8mm {{Groove}} , 1000mm {{Length}} | {{RS Components}}},
  author = {RS Components},
  urldate = {2022-05-16},
  howpublished = {https://no.rs-online.com/web/p/tubing-and-profile-struts/7613319},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4BBQAQWV\\0900766b8157c300.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AXQUFMF8\\7613319.html}
}

@misc{rtcmspecialcommitteeno.104RTCM10410Standard,
  title = {{{RTCM}} 10410.1 {{Standard}} for {{Networked Transport}} of {{RTCM}} via {{Internet Protocol}} ({{Ntrip}}) {{Version}} 2.0 with {{Amendment}} 2, {{January}} 12, 2021},
  author = {RTCM SPECIAL COMMITTEE NO.104},
  journal = {Radio Technical Commission for Maritime Services},
  urldate = {2022-05-02},
  abstract = {Networked Transport of RTCM via Internet Protocol (Ntrip) is an application-level protocol that supports streaming Global Navigation Satellite System (GNSS) data over the Internet. Ntrip is a generic, stateless protocol based on the Hypertext Transfer Protocol HTTP/1.1. The HTTP objects are extended to GNSS data stream},
  howpublished = {https://rtcm.myshopify.com/products/rtcm-10410-1-standard-for-networked-transport-of-rtcm-via-internet-protocol-ntrip-version-2-0-with-amendment-1-june-28-2011},
  file = {C:\Users\emilm\Zotero\storage\ABJ9LDI6\rtcm-10410-1-standard-for-networked-transport-of-rtcm-via-internet-protocol-ntrip-version-2-0-w.html}
}

@misc{rtishchevMantine,
  title = {Mantine},
  author = {Rtishchev, Vitaly},
  urldate = {2023-06-26},
  abstract = {React components and hooks library with native dark theme support and focus on usability, accessibility and developer experience},
  howpublished = {https://mantine.dev/},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\YJ62QQ9A\mantine.dev.html}
}

@misc{RTKLIBOpenSource,
  title = {{{RTKLIB}}: {{An Open Source Program Package}} for {{GNSS Positioning}}},
  urldate = {2022-05-30},
  howpublished = {http://www.rtklib.com/},
  file = {C:\Users\emilm\Zotero\storage\Q6A6GK6N\www.rtklib.com.html}
}

@inproceedings{rubleeORBEfficientAlternative2011,
  title = {{{ORB}}: {{An}} Efficient Alternative to {{SIFT}} or {{SURF}}},
  shorttitle = {{{ORB}}},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  year = {2011},
  month = nov,
  pages = {2564--2571},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCV.2011.6126544},
  urldate = {2023-09-14},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8}
}

@misc{sadowskiEnablingPPSJetson2020,
  title = {Enabling {{PPS}} on {{Jetson Nano}}},
  author = {Sadowski, Mateusz},
  year = {2020},
  month = apr,
  journal = {msadowski blog},
  urldate = {2022-05-19},
  abstract = {It took me quite a while to add PPS support on Jetson Nano. Hopefully this tutorial helps speed up the process for you.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\3RL8L4SZ\pps-support-jetson-nano.html}
}

@techreport{safranSTIM300Datasheet,
  type = {Datasheet},
  title = {{{STIM300 Datasheet}}},
  author = {Safran},
  number = {TS1524 rev.28},
  urldate = {2022-05-02},
  lccn = {+47 3303 5000},
  file = {C:\Users\emilm\Zotero\storage\G7C62NM5\ts1524-r28-datasheet-stim300.pdf}
}

@techreport{safranSTIM300ProductBrief,
  title = {{{STIM300 Product}} Brief},
  author = {Safran},
  file = {C:\Users\emilm\Zotero\storage\8CXX7KW2\2022-04-06-product-brief-stim300-a4_high-quality-print.pdf}
}

@misc{sainiUnableReadNet2021,
  title = {Unable to Read Net.Core.Rmem\_default inside Container {$\cdot$} {{Issue}} \#42282 {$\cdot$} Moby/Moby},
  author = {Saini, Param},
  year = {2021},
  month = apr,
  journal = {GitHub},
  urldate = {2023-05-02},
  abstract = {Description I am running docker/container and set the kernel parameters net.core.rmem\_default at the docker host level. However, inside the container, I am unable to access the net. core.rmem\_defau...},
  howpublished = {https://github.com/moby/moby/issues/42282},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\XB887HFZ\42282.html}
}

@misc{sarlinCoarseFineRobust2019,
  title = {From {{Coarse}} to {{Fine}}: {{Robust Hierarchical Localization}} at {{Large Scale}}},
  shorttitle = {From {{Coarse}} to {{Fine}}},
  author = {Sarlin, Paul-Edouard and Cadena, Cesar and Siegwart, Roland and Dymczyk, Marcin},
  year = {2019},
  month = apr,
  number = {arXiv:1812.03506},
  eprint = {1812.03506},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.03506},
  urldate = {2022-11-27},
  abstract = {Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\J5XULVRC\\Sarlin et al. - 2019 - From Coarse to Fine Robust Hierarchical Localizat.pdf;C\:\\Users\\emilm\\Zotero\\storage\\W2A6PB26\\1812.html}
}

@misc{sarlinSuperGlueLearningFeature2020,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  number = {arXiv:1911.11763},
  eprint = {1911.11763},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.11763},
  urldate = {2023-09-14},
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IZTIWYR2\\Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf;C\:\\Users\\emilm\\Zotero\\storage\\XRLXHFPS\\1911.html}
}

@misc{sattlerUnderstandingLimitationsCNNbased2019,
  title = {Understanding the {{Limitations}} of {{CNN-based Absolute Camera Pose Regression}}},
  author = {Sattler, Torsten and Zhou, Qunjie and Pollefeys, Marc and {Leal-Taixe}, Laura},
  year = {2019},
  month = mar,
  number = {arXiv:1903.07504},
  eprint = {1903.07504},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.07504},
  urldate = {2022-11-27},
  abstract = {Visual localization is the task of accurate camera pose estimation in a known scene. It is a key problem in computer vision and robotics, with applications including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality. Traditionally, the localization problem has been tackled using 3D geometry. Recently, end-to-end approaches based on convolutional neural networks have become popular. These methods learn to directly regress the camera pose from an input image. However, they do not achieve the same level of pose accuracy as 3D structure-based methods. To understand this behavior, we develop a theoretical model for camera pose regression. We use our model to predict failure cases for pose regression techniques and verify our predictions through experiments. We furthermore use our model to show that pose regression is more closely related to pose approximation via image retrieval than to accurate pose estimation via 3D structure. A key result is that current approaches do not consistently outperform a handcrafted image retrieval baseline. This clearly shows that additional research is needed before pose regression algorithms are ready to compete with structure-based methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2B3UV4PD\\Sattler et al. - 2019 - Understanding the Limitations of CNN-based Absolut.pdf;C\:\\Users\\emilm\\Zotero\\storage\\TKYZQGV6\\1903.html}
}

@misc{schneiderGuideUnderstandingLiPo21,
  title = {A {{Guide}} to {{Understanding LiPo Batteries}}},
  author = {Schneider, Brian},
  year = {21},
  month = oct,
  journal = {Roger's Hobby Center},
  urldate = {2022-05-29},
  howpublished = {https://rogershobbycenter.com/lipoguide},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\DZ6J3ATD\lipoguide.html}
}

@article{schneiderValidationGeometricModels2009,
  title = {Validation of Geometric Models for Fisheye Lenses},
  author = {Schneider, D. and Schwalbe, E. and Maas, H.-G.},
  year = {2009},
  month = may,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {64},
  number = {3},
  pages = {259--266},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2009.01.001},
  urldate = {2022-11-27},
  langid = {english}
}

@misc{schopsWhyHaving102020,
  title = {Why {{Having}} 10,000 {{Parameters}} in {{Your Camera Model}} Is {{Better Than Twelve}}},
  author = {Sch{\"o}ps, Thomas and Larsson, Viktor and Pollefeys, Marc and Sattler, Torsten},
  year = {2020},
  month = jun,
  number = {arXiv:1912.02908},
  eprint = {1912.02908},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-11-25},
  abstract = {Camera calibration is an essential first step in setting up 3D Computer Vision systems. Commonly used parametric camera models are limited to a few degrees of freedom and thus often do not optimally fit to complex real lens distortion. In contrast, generic camera models allow for very accurate calibration due to their flexibility. Despite this, they have seen little use in practice. In this paper, we argue that this should change. We propose a calibration pipeline for generic models that is fully automated, easy to use, and can act as a drop-in replacement for parametric calibration, with a focus on accuracy. We compare our results to parametric calibrations. Considering stereo depth estimation and camera pose estimation as examples, we show that the calibration error acts as a bias on the results. We thus argue that in contrast to current common practice, generic models should be preferred over parametric ones whenever possible. To facilitate this, we released our calibration pipeline at https://github.com/puzzlepaint/camera\_calibration, making both easy-to-use and accurate camera calibration available to everyone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6NT9XM8J\\Schöps et al. - 2020 - Why Having 10,000 Parameters in Your Camera Model .pdf;C\:\\Users\\emilm\\Zotero\\storage\\FQVZVYRI\\1912.html}
}

@misc{schweberWhatDoesAnalog2017,
  title = {What Does an Analog Driver/Buffer Do?},
  author = {Schweber, Bill},
  year = {2017},
  month = jun,
  journal = {Analog IC Tips},
  urldate = {2022-05-23},
  abstract = {Although buffers and drivers don't add functionality to a circuit, these apparently simple interface elements are essential to viable circuit design and operation.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\HGMWI7FV\what-does-an-analog-driverbuffer-do.html}
}

@misc{SettingUsbBaud,
  title = {Setting the Usb Baud Rate ? - {{Raspberry Pi Forums}}},
  urldate = {2023-06-10},
  howpublished = {https://forums.raspberrypi.com/viewtopic.php?t=308326},
  file = {C:\Users\emilm\Zotero\storage\H5ZACM43\viewtopic.html}
}

@misc{sfiautoshipAnnualReport20222022,
  title = {Annual {{Report}} 2022},
  author = {SFI Autoship},
  year = {2022},
  file = {C:\Users\emilm\Zotero\storage\FQ2BH8BA\fce424b8-9192-b037-407d-a4aecbd14e64.pdf}
}

@misc{SGLUTGnsstk2022,
  title = {{{SGL-UT}}/Gnsstk},
  year = {2022},
  month = may,
  urldate = {2022-05-30},
  abstract = {The goal of the gnsstk project is to provide an open source library to the satellite navigation community--to free researchers to focus on research, not lower level coding.},
  howpublished = {Space and Geophysics Laboratory of ARL:UT at Austin}
}

@misc{shaikhHTTPRequestresponseFlow2022,
  title = {{{HTTP}} Request-Response Flow: For Dummies},
  shorttitle = {{{HTTP}} Request-Response Flow},
  author = {Shaikh, Reem},
  year = {2022},
  month = mar,
  journal = {Medium},
  urldate = {2023-06-03},
  abstract = {Hi meows! In this tutorial we'll be understanding the flow of client-server architecture, format of the request and response message as{\ldots}},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\VQCYQVFI\http-request-response-flow-for-dummies-a0f52af83af3.html}
}

@misc{shanecccThereAnySpidev2021,
  title = {Is There Any Spidev*.* in {{Jetson AGX}} with Native {{JetPack}} 4.6? - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  shorttitle = {Is There Any Spidev*.* in {{Jetson AGX}} with Native {{JetPack}} 4.6?},
  author = {ShaneCCC},
  year = {2021},
  month = sep,
  journal = {NVIDIA Developer Forums},
  urldate = {2022-05-20},
  abstract = {Insert the spidev module by below command.  sudo modprobe spidev},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\V7V32YA6\5.html}
}

@misc{shgargHOWIncreaseGNNS2020,
  title = {{{HOW}} to {{Increase GNNS}} Timing Module on {{Xavier}}? - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  shorttitle = {{{HOW}} to {{Increase GNNS}} Timing Module on {{Xavier}}?},
  author = {{shgarg}},
  year = {2020},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2022-05-20},
  abstract = {AON is always on island of the chip.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\LEJ6ZDQT\8.html}
}

@techreport{siliconimagingincBayerColorConversion,
  title = {Bayer Color Conversion and Processing},
  author = {Silicon Imaging, Inc},
  file = {C:\Users\emilm\Zotero\storage\55ZHURGI\AN3 - Bayer Color Processing.pdf}
}

@misc{simonCameraCalibTools2023,
  title = {{{CameraCalibTools}}},
  author = {Simon},
  year = {2023},
  month = jun,
  urldate = {2023-06-11},
  abstract = {List of Camera Calibration Tools + Patterns},
  keywords = {camera-calibration,photogrammetry}
}

@misc{snapmakerSnapmakerJ1High,
  title = {Snapmaker {{J1 High Speed IDEX 3D Printer}}},
  author = {Snapmaker},
  journal = {Snapmaker},
  urldate = {2023-06-04},
  abstract = {Buy Snapmaker J1 3D Printer for IDEX printing available at discounted price. This is the best dual extruder IDEX 3D printer offers the cleanest two-extruder solution that prevents cross-contamination.},
  howpublished = {https://shop.snapmaker.com/products/snapmaker-j1-independent-dual-extruder-3d-printer},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\7E4J94A3\snapmaker-j1-independent-dual-extruder-3d-printer.html}
}

@misc{snapmakerSnapmakerLogo2020,
  title = {Snapmaker {{Logo}}},
  author = {Snapmaker},
  year = {2020},
  month = sep
}

@misc{snapmakerSnapmakerLubanIntuitive,
  title = {Snapmaker {{Luban}} | {{An Intuitive}} and {{Powerful 3D Printing Software}} - {{Snapmaker}}},
  author = {Snapmaker},
  urldate = {2023-05-27},
  howpublished = {https://snapmaker.com/snapmaker-luban},
  file = {C:\Users\emilm\Zotero\storage\35BF4NCY\snapmaker-luban.html}
}

@misc{snapmakerSnapmakerPluginUltimaker,
  title = {Snapmaker {{Plugin}} - {{Ultimaker Cura Marketplace}}},
  author = {Snapmaker},
  urldate = {2023-05-27},
  howpublished = {https://marketplace.ultimaker.com/app/cura/plugins/Snapmaker/SnapmakerPlugin},
  file = {C:\Users\emilm\Zotero\storage\TH99QFZA\SnapmakerPlugin.html}
}

@misc{snipRTCMMessageCheat2020,
  title = {An {{RTCM}} 3 Message Cheat Sheet},
  author = {SNIP},
  year = {2020},
  month = nov,
  journal = {SNIP Support},
  urldate = {2022-05-02},
  abstract = {We are often asked what the most common RTCM version 3 messages are. Here is a handy decoder cheat sheet to answer this. SNIP's message decoder view can also be used.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\FNE7SWPV\an-rtcm-message-cheat-sheet.html}
}

@misc{solaMicroLieTheory2021,
  title = {A Micro {{Lie}} Theory for State Estimation in Robotics},
  author = {Sol{\`a}, Joan and Deray, Jeremie and Atchuthan, Dinesh},
  year = {2021},
  month = dec,
  number = {arXiv:1812.01537},
  eprint = {1812.01537},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-10-26},
  abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IJ8GE9ZH\\Solà et al. - 2021 - A micro Lie theory for state estimation in robotic.pdf;C\:\\Users\\emilm\\Zotero\\storage\\M4UPB22V\\1812.html}
}

@article{sonnenbergSerialCommunicationsRS2322018,
  title = {Serial {{Communications RS232}}, {{RS485}}, {{RS422}}},
  author = {Sonnenberg, John},
  year = {2018},
  pages = {6},
  abstract = {Electronic communications is all about interlinking circuits (processors or other integrated circuits) to create a symbiotic system. For those individual circuits to swap information, they must share a common standard communication protocol. Many communication protocols have been designed to achieve data exchange.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\YGWBUZ8H\Sonnenberg - 2018 - Serial Communications RS232, RS485, RS422.pdf}
}

@techreport{sourceiexDegreesProtection,
  title = {Degrees of {{Protection}}},
  author = {Source IEx},
  file = {C:\Users\emilm\Zotero\storage\T6MLAANP\IP Degress Testing Details.pdf}
}

@misc{SourceSyncFailing2021,
  title = {Source {{Sync}} Is {{Failing}}},
  year = {2021},
  month = oct,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-06},
  abstract = {Hello,  I am running the source\_sync.sh script with the following command w/ L4T 32.6.1:  sudo ./source\_sync.sh -t tegra-l4t-r32.6.1  I get the following error:  Cloning into '/Linux\_for\_Tegra/sources/kernel/nvidia'... fatal: unable to connect to nv-tegra.nvidia.com: nv-tegra.nvidia.com[0: 34.216.216.36]: errno=Connection refused nv-tegra.nvidia.com[1: 35.82.181.43]: errno=Connection refused  Has anyone run into this? I can manually clone via nv-tegra.nvidia.com, but I would prefer to use the sc...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/source-sync-is-failing/192695},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\AQ947GCT\192695.html}
}

@misc{sparkfunOpenLogArtemisHookup,
  title = {{{OpenLog Artemis Hookup Guide}} - Learn.Sparkfun.Com},
  author = {Sparkfun},
  urldate = {2022-05-30},
  howpublished = {https://learn.sparkfun.com/tutorials/openlog-artemis-hookup-guide},
  file = {C:\Users\emilm\Zotero\storage\JIJD26W8\openlog-artemis-hookup-guide.html}
}

@misc{sparkfunSparkFunGPSRTKSMABreakout2022,
  title = {{{SparkFun GPS-RTK-SMA Breakout}} - {{ZED-F9P}} ({{Qwiic}}) - {{GPS-16481}} - {{SparkFun Electronics}}},
  author = {Sparkfun},
  year = {2022},
  urldate = {2022-05-21},
  howpublished = {https://www.sparkfun.com/products/16481},
  file = {C:\Users\emilm\Zotero\storage\WDPWJ666\16481.html}
}

@misc{sparkfunSparkFunOpenLogArtemis,
  title = {{{SparkFun OpenLog Artemis}} - {{DEV-16832}} - {{SparkFun Electronics}}},
  author = {Sparkfun},
  urldate = {2022-05-30},
  howpublished = {https://www.sparkfun.com/products/16832},
  file = {C:\Users\emilm\Zotero\storage\HLIX46U5\16832.html}
}

@misc{spennbergWhatKerfParts2019,
  title = {What Is {{Kerf}}? - {{Parts Badger}} - {{Your Online Machine Shop}}},
  shorttitle = {What Is {{Kerf}}?},
  author = {Spennberg, Brandon},
  year = {2019},
  month = dec,
  journal = {Parts Badger},
  urldate = {2022-05-18},
  abstract = {Kerf is derived from Middle English "kerf, kirf, kyrf", from Old English "cyrf", from Proto-Germanic "kurbiz", and from Proto-Indo-European "gerb"....},
  howpublished = {https://parts-badger.com/whats-a-kerf/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\C9A7NSI3\whats-a-kerf.html}
}

@inproceedings{spoteJointDemosaicingColour2021,
  title = {Joint Demosaicing of Colour and Polarisation from Filter Arrays},
  booktitle = {Color and {{Imaging Conference}}},
  author = {Spote, Alexandra and Lapray, Pierre-Jean and Thomas, Jean-Baptiste and Farup, Ivar},
  year = {2021},
  volume = {2021},
  pages = {288--293},
  publisher = {{Society for Imaging Science and Technology}},
  file = {C:\Users\emilm\Zotero\storage\AZHMWQWX\Spote et al. - 2021 - Joint demosaicing of colour and polarisation from .pdf}
}

@misc{stevo-88EnglishMostWidely2010,
  title = {English:  {{Most}} Widely Used Chroma Subsampling Formats.},
  shorttitle = {English},
  author = {{Stevo-88}},
  year = {2010},
  month = jul,
  urldate = {2023-06-14},
  copyright = {Public domain},
  file = {C:\Users\emilm\Zotero\storage\CIAMBMEQ\FileCommon_chroma_subsampling_ratios.html}
}

@article{StokesParameters2023,
  title = {Stokes Parameters},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-05-12},
  abstract = {The Stokes parameters are a set of values that describe the polarization state of electromagnetic radiation. They were defined by George Gabriel Stokes in 1852, as a mathematically convenient alternative to the more common description of incoherent or partially polarized radiation in terms of its total intensity (I), (fractional) degree of polarization (p), and the shape parameters of the polarization ellipse. The effect of an optical system on the polarization of light can be determined by constructing the Stokes vector for the input light and applying Mueller calculus, to obtain the Stokes vector of the light leaving the system. The original Stokes paper was discovered independently by Francis Perrin in 1942 and by Subrahamanyan Chandrasekhar in 1947, who named it as the Stokes parameters.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1148569528},
  file = {C:\Users\emilm\Zotero\storage\2Q5HFPSH\Stokes_parameters.html}
}

@techreport{stotonTOP106GNSSL1,
  title = {{{TOP106 GNSS L1}}/{{L2 Multiband Antenna}}},
  author = {STOTON},
  urldate = {2022-05-26}
}

@phdthesis{sundvollCamerabasedPerceptionSystem,
  title = {A {{Camera-based Perception System}} for {{Autonomous Quadcopter Landing}} on a {{Marine Vessel}}},
  author = {Sundvoll, Thomas},
  abstract = {Sm{\aa}, ubemannede luftfart{\o}yer (UAVer) har tiltrukket seg mye oppmerksomhet de siste {\aa}rene, og et av de mest studerte UAVene er quadcopteret. Et quadcopter g{\aa}r ogs{\aa} under kategorien VTOL-fart{\o}y som er et begrep som brukes om fart{\o}y som kan ta av og lande vertikalt. Dette gj{\o}r at quadcoptere har en fordel n{\aa}r de opererer i omr{\aa}der med lite plass. Sammen med deres h{\o}ye man{\o}vrerbarhet er de et godt vert{\o}y til {\aa} utf{\o}re mange oppgaver, slik som inspeksjon, transport av sm{\aa} pakker og overv{\aa}kning for s{\o}k- og redningsopperasjoner. For {\aa} {\o}ke flygetiden og redusere kostnadene ved {\aa} manuelt styre slike fart{\o}y, er det i det siste forsket mye p{\aa} autonome quadcoptre. Deler av en autonom flytur, og s{\ae}rlig landingen, krever et presist posisjonsestimat. Denne oppgaven unders{\o}ker et bruksomr{\aa}de hvor landingsplassen er betydelig begrenset n{\aa}r det kommer til st{\o}rrelse, nemlig {\aa} lande p{\aa} et lite, sj{\o}g{\aa}ende fart{\o}y. I dette tilfellet kan landingsplassen v{\ae}re omtrent p{\aa} samme st{\o}rrelse som quadcopteret selv, noe som krever et posisjonsestimat med enda h{\o}yere presisjon. I dette tilfellet vil ikke vanlige GPS-m{\aa}linger v{\ae}re presist nok til {\aa} utf{\o}re autonom landing. Derfor unders{\o}ker denne oppgaven bruken av kamera som hovedsensor {\aa} estimere posisjonen til et quadcopter, med forventning om at dette vil gi et bedre estimat. En landingsplattform er designet og bygget for {\aa} fungere som landingsplass i eksperimentene. Den er designet for {\aa} etterligne en standard landingsplattform som vanligvis er {\aa} finne p{\aa} sj{\o}g{\aa}ende fart{\o}y og marine installasjoner. Fart{\o}yet som til slutt vil bruke landingsplattformen er modellskipet ReVolt som er laget av DNV GL, s{\aa} designet er tilpasset for at landingsplattformen skal passe til dette spesifikke skipet. Et datasyn-system er utviklet med hovedhensikt {\aa} estimere quadcopterets posisjon relativt til landingsplattformen. Hovedutfordringen med et datasyn-system p{\aa} sj{\o}en er mangelen p{\aa} faste punkter {\aa} navigere etter, siden sj{\o}en er i konstant bevegelse. For {\aa} l{\o}se dette problemet er tradisjonelle datasyn-metoder brukt, blant annet fargesegmentering, deteksjon av kanter og deteksjon av hj{\o}rner, for {\aa} hente ut allerede kjente kjennetegn p{\aa} landingsplattformen. Ut fra dette er posisjonen estimert ved bruk av hullkamera-modellen og kjente m{\aa}l p{\aa} landingsplattformen. Metodene og algoritmene for posisjonsestimatet er utviklet ved bruk av OpenCV-biblioteket i Python, og datasyn-systemet er integrert inn i rammeverket Robot Operating System (ROS). I tillegg er en bestikkregning-modul utviklet for {\aa} gi et estimat basert p{\aa} interne m{\aa}linger hos quadcopteret, for bruk n{\aa}r ingen datasyn-estimat er tilgjengelig. Systemet er testet b{\aa}de i en simulator og med et fysisk quadcopter og landingsplattform, med n{\o}yaktige resultat i simulatoren og lovende, men st{\o}yfulle resultat med det fysiske quadcopteret. Til slutt er det gitt noen forslag til forbedringer av metodene og fremtidig arbeid p{\aa} temaet.},
  keywords = {Annette\_tip}
}

@misc{SupportFilamentPETG2023,
  title = {Support {{Filament}} --{$>$} {{PETG}} for {{PLA}} and {{PLA}} for {{PETG}} and More},
  year = {2023},
  month = feb,
  journal = {Bambu Lab Community Forum},
  urldate = {2023-06-16},
  abstract = {Hello all,  because the PLA Support Filament in the Bambu Store is almost always sold out there is also another possibility.  You can also use PETG for PLA and just PLA for PETG. PETG and PLA are printed at similar temperatures, but they can't combine and so they are very good as a release material.  This wisdom is not from me, but from the Youtube channel of Manbo. In this video it is explained again and shown the settings that should be made. Here is shown ``only'' PETG as suppoort for PLA. The ...},
  chapter = {Bambu Filament and Accesories},
  howpublished = {https://forum.bambulab.com/t/support-filament-petg-for-pla-and-pla-for-petg-and-more/5942},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\4LPF27PC\5942.html}
}

@misc{SupportingHalfprecisionFloats,
  title = {Supporting Half-Precision Floats Is Really Annoying},
  urldate = {2023-04-17},
  howpublished = {https://futhark-lang.org/blog/2021-08-05-half-precision-floats.html},
  file = {C:\Users\emilm\Zotero\storage\CDD5XWNF\2021-08-05-half-precision-floats.html}
}

@article{tabbSolvingRobotWorldHandEye2017,
  title = {Solving the {{Robot-World Hand-Eye}}(s) {{Calibration Problem}} with {{Iterative Methods}}},
  author = {Tabb, Amy and Yousef, Khalil M. Ahmad},
  year = {2017},
  month = aug,
  journal = {Machine Vision and Applications},
  volume = {28},
  number = {5-6},
  eprint = {1907.12425},
  primaryclass = {cs},
  pages = {569--590},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-017-0841-7},
  urldate = {2022-11-26},
  abstract = {Robot-world, hand-eye calibration is the problem of determining the transformation between the robot end-effector and a camera, as well as the transformation between the robot base and the world coordinate system. This relationship has been modeled as \${\textbackslash}mathbf\{AX\}={\textbackslash}mathbf\{ZB\}\$, where \${\textbackslash}mathbf\{X\}\$ and \${\textbackslash}mathbf\{Z\}\$ are unknown homogeneous transformation matrices. The successful execution of many robot manipulation tasks depends on determining these matrices accurately, and we are particularly interested in the use of calibration for use in vision tasks. In this work, we describe a collection of methods consisting of two cost function classes, three different parameterizations of rotation components, and separable versus simultaneous formulations. We explore the behavior of this collection of methods on real datasets and simulated datasets, and compare to seven other state-of-the-art methods. Our collection of methods return greater accuracy on many metrics as compared to the state-of-the-art. The collection of methods is extended to the problem of robot-world hand-multiple-eye calibration, and results are shown with two and three cameras mounted on the same robot.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3HGPSCME\\Tabb and Yousef - 2017 - Solving the Robot-World Hand-Eye(s) Calibration Pr.pdf;C\:\\Users\\emilm\\Zotero\\storage\\MS3R5J2J\\1907.html}
}

@inproceedings{taipalmaaHighResolutionWaterSegmentation2019,
  title = {High-{{Resolution Water Segmentation}} for {{Autonomous Unmanned Surface Vehicles}}: A {{Novel Dataset}} and {{Evaluation}}},
  shorttitle = {High-{{Resolution Water Segmentation}} for {{Autonomous Unmanned Surface Vehicles}}},
  booktitle = {2019 {{IEEE}} 29th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Taipalmaa, Jussi and Passalis, Nikolaos and Zhang, Honglei and Gabbouj, Moncef and Raitoharju, Jenni},
  year = {2019},
  month = oct,
  pages = {1--6},
  issn = {1551-2541},
  doi = {10.1109/MLSP.2019.8918694},
  abstract = {Even though Unmanned Surface Vehicles (USVs) are increasingly used to perform various laborious and expensive offshore tasks, they still require an extensive dedicated crew supporting and ensuring the safety of their operations. The recent developments in computer vision and robotics further fueled the interest on developing autonomous USVs that will overcome the aforementioned limitations, unleashing their full potential. One of the most vital and fundamental tasks in order to automate and ensure the safety of USV operations is to perform water segmentation. Despite the importance of developing such segmentation methods, there is a lack of highresolution publicly available datasets, which are suitable for training and evaluating deep learning methods. The main contribution of this paper is collecting, annotating and releasing a publicly available high-resolution dataset for developing deep learning algorithms for water segmentation in a Nordic lake environment. Furthermore, we adapt a deep learning algorithm previously applied for road segmentation to the water segmentation task. While the algorithm obtains a high accuracy, the results also allow for identifying critical limitations of the approach. Finally, we propose and evaluate a novel lightweight fully convolutional neural network architecture, fully adapted to the needs of water segmentation from highresolution images.},
  keywords = {Boats,deep learning,fully convolutional networks,Image segmentation,Lakes,Machine learning,Roads,Task analysis,Training,unmanned surface vessel,Water segmentation},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3LD2ARLP\\Taipalmaa et al. - 2019 - High-Resolution Water Segmentation for Autonomous .pdf;C\:\\Users\\emilm\\Zotero\\storage\\8R5L37GC\\8918694.html}
}

@misc{tairaThisRightPlace2019,
  title = {Is {{This The Right Place}}? {{Geometric-Semantic Pose Verification}} for {{Indoor Visual Localization}}},
  shorttitle = {Is {{This The Right Place}}?},
  author = {Taira, Hajime and Rocco, Ignacio and Sedlar, Jiri and Okutomi, Masatoshi and Sivic, Josef and Pajdla, Tomas and Sattler, Torsten and Torii, Akihiko},
  year = {2019},
  month = sep,
  number = {arXiv:1908.04598},
  eprint = {1908.04598},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.04598},
  urldate = {2022-11-27},
  abstract = {Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there is considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\W4E38MHL\\Taira et al. - 2019 - Is This The Right Place Geometric-Semantic Pose V.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3BPVHHJ4\\1908.html}
}

@misc{tamiyaTamiyaExtraThin,
  title = {Tamiya {{Extra Thin Cement}} w/{{Brush}} 40ml 300087038 - {{Craft}} Accessories - {{Accessories}} - {{Categories}} - Www.Tamiya.De},
  author = {TAMIYA},
  urldate = {2022-05-18},
  howpublished = {https://www.tamiya.de/tamiya\_en/categories/accessories/craft-accessories/tamiya-extra-thin-cement-wbrush-40ml-300087038-en.html},
  file = {C:\Users\emilm\Zotero\storage\RLBTWKLH\tamiya-extra-thin-cement-wbrush-40ml-300087038-en.html}
}

@misc{tangestuenSkipeneSomSkal2022,
  title = {{Skipene som skal redusere vogntogtrafikken}},
  author = {Tangestuen, Viktor},
  year = {2022},
  month = aug,
  journal = {NRK},
  urldate = {2023-09-12},
  abstract = {To elektriske ferger uten folk om bord skal frakte trailere over Oslofjorden for {\aa} spare veiene for tungtransport. Kystverket h{\aa}per flere vil satse p{\aa} teknologien.},
  chapter = {dk},
  howpublished = {https://www.nrk.no/vestfoldogtelemark/skipene-som-skal-redusere-vogntogtrafikken-1.16064543},
  langid = {norwegianbokmal},
  file = {C:\Users\emilm\Zotero\storage\EDKAG943\skipene-som-skal-redusere-vogntogtrafikken-1.html}
}

@misc{tarasJackerssonGstreamerpython2023,
  title = {Jackersson/Gstreamer-Python},
  author = {Taras},
  year = {2023},
  month = may,
  urldate = {2023-05-30}
}

@misc{Tcpclientsrc,
  title = {Tcpclientsrc},
  urldate = {2023-05-26},
  howpublished = {https://gstreamer.freedesktop.org/documentation/tcp/tcpclientsrc.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\BKUQ6XY4\tcpclientsrc.html}
}

@misc{Tcpclientsrca,
  title = {Tcpclientsrc},
  urldate = {2023-05-26},
  howpublished = {https://gstreamer.freedesktop.org/documentation/tcp/tcpclientsrc.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\YPIRIXR2\tcpclientsrc.html}
}

@misc{Tcpserversink,
  title = {Tcpserversink},
  urldate = {2023-05-26},
  howpublished = {https://gstreamer.freedesktop.org/documentation/tcp/tcpserversink.html?gi-language=c},
  file = {C:\Users\emilm\Zotero\storage\VC5SIZ8G\tcpserversink.html}
}

@misc{Tegra194gpioSourceCode2019,
  title = {Tegra194-Gpio.h Source Code [Linux/Include/Dt-Bindings/Gpio/Tegra194-Gpio.h] - {{Woboq Code Browser}}},
  year = {2019},
  month = mar,
  urldate = {2022-05-20},
  file = {C:\Users\emilm\Zotero\storage\REMBJ673\tegra194-gpio.h.html}
}

@misc{TEGRAGPIOPort2021,
  title = {{{TEGRA}}\_{{GPIO}}("port", "Offset")?},
  year = {2021},
  month = jun,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-05},
  abstract = {Hi -  I'm trying to map a couple GPIO pins in a device tree and none of the documents I've read explain what values to use for the (``port'', ``offset'').  Specifically I want to use gpio79 (physical pin 12) and  gpio216 (physical pin 7) on the Jetson Nano B01 version.  How do I determine the port and offset values?  test \{         compatible = "gpio-test";         status = "okay";         test-name \{                 test-gpios = {$<\&$}gpio TEGRA\_GPIO(??, ??) GPIO\_ACTIVE\_HIGH{$>$},                 test-gpio...\vphantom{\}\}}},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/tegra-gpio-port-offset/179621},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\MNMAH9NT\179621.html}
}

@misc{teledyneSettingIPAddress01,
  title = {{Setting an IP address for a GigE camera to be recognized in Linux}},
  author = {Teledyne, {\relax FLIR}},
  year = {1},
  urldate = {2023-05-02},
  howpublished = {https://www.flir.com.mx/support-center/iis/machine-vision/knowledge-base/setting-an-ip-address-for-a-gige-camera-to-be-recognized-in-linux/},
  langid = {mexican},
  file = {C:\Users\emilm\Zotero\storage\BM4KJ7UI\setting-an-ip-address-for-a-gige-camera-to-be-recognized-in-linux.html}
}

@misc{teledyneSettingPersistentIP,
  title = {Setting a Persistent {{IP}} Address (for All {{GigE Vision}} Cameras)},
  author = {Teledyne},
  urldate = {2023-05-03},
  howpublished = {https://flir.custhelp.com/app/answers/detail/a\_id/3032/{\textasciitilde}/setting-a-persistent-ip-address-\%28for-all-gige-vision-cameras\%29},
  file = {C:\Users\emilm\Zotero\storage\222NJEHB\setting-a-persistent-ip-address-(for-all-gige-vision-cameras).html}
}

@misc{telegartnerJ01150A0069Telegartner50,
  title = {{{J01150A0069}} | {{Telegartner}} 50{\textbackslash}{{Omega Right Angle Cable Mount}}, {{SMA Connector}} , {{Plug}} | {{RS Components}}},
  author = {Telegartner},
  urldate = {2022-05-26},
  howpublished = {https://no.rs-online.com/web/p/coaxial-connectors/1938984},
  file = {C:\Users\emilm\Zotero\storage\PI5SD64F\1938984.html}
}

@techreport{telegartnerMontageanweisungAssemblyInstruction,
  title = {Montageanweisung / {{Assembly Instruction C04xx}}},
  author = {Telegartner},
  urldate = {2022-05-26},
  file = {C:\Users\emilm\Zotero\storage\K6X9RECS\0900766b80154af1.pdf}
}

@misc{tewariAdvancesNeuralRendering2022,
  title = {Advances in {{Neural Rendering}}},
  author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and {Martin-Brualla}, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
  year = {2022},
  month = mar,
  number = {arXiv:2111.05849},
  eprint = {2111.05849},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.05849},
  urldate = {2022-11-28},
  abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IEHS7BKA\\Tewari et al. - 2022 - Advances in Neural Rendering.pdf;C\:\\Users\\emilm\\Zotero\\storage\\6TB2HDZD\\2111.html}
}

@techreport{texasinstrumentsDS8921xDifferentialLine2015,
  type = {Datasheet},
  title = {{{DS8921x Differential Line Driver}} and {{Receiver Pair}}},
  author = {Texas Instruments},
  year = {2015},
  month = jan,
  number = {SNLS374D {\textendash}MAY 1998{\textendash}},
  urldate = {2022-05-12},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\C9SXB89F\\ds8921.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AVR2IYTQ\\366600.html}
}

@misc{thekerneldevelopmentcommunityInterruptsLinuxKernel,
  title = {Interrupts {\textemdash} {{The Linux Kernel}} Documentation},
  author = {{The kernel development community}},
  urldate = {2023-05-16},
  howpublished = {https://linux-kernel-labs.github.io/refs/heads/master/lectures/interrupts.html},
  file = {C:\Users\emilm\Zotero\storage\BN796HDV\interrupts.html}
}

@misc{thelinkedinteamHowYouCompare2023,
  title = {How Do You Compare {{RTSP}} with Other Streaming Protocols Such as {{HLS}} and {{DASH}}?},
  author = {The LinkedIn Team},
  year = {2023},
  month = apr,
  urldate = {2023-05-26},
  abstract = {Learn how to compare RTSP with other streaming protocols such as HLS and DASH, and how to choose the best one for your streaming application.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KVH5IW6X\how-do-you-compare-rtsp-other-streaming.html}
}

@book{therrienDecisionEstimationClassification1989,
  title = {Decision, {{Estimation}}, and {{Classification}}: {{An Introduction}} to {{Pattern Recognition}} and {{Related Topics}}},
  shorttitle = {Decision, {{Estimation}}, and {{Classification}}},
  author = {Therrien, Charles W.},
  year = {1989},
  publisher = {{Wiley}},
  googlebooks = {rmJIAAAACAAJ},
  isbn = {978-0-471-50416-0},
  langid = {english},
  keywords = {rudolf\_tip},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4UJHN646\\Therrien1989-Chap3.pdf;C\:\\Users\\emilm\\Zotero\\storage\\WL46NHSQ\\Therrien1989-38-41.pdf}
}

@misc{ThingsFirstNew2023,
  title = {Things to Do First with a New {{J1}}},
  year = {2023},
  month = feb,
  journal = {Snapmaker: where creation happens},
  urldate = {2023-03-02},
  abstract = {In order to make things easier for new users (and for myself when my J1 will arrive as it seems to take a little longer than expected), this is an attempt to make a list of the hardware-related things to do first when you get your J1, based on the topics I found here in the forum so far. I referenced the forum entries where I got the corresponding information from. Please refer to these entries for details. Feel free to add anyting I might have forgotten!   when you unpack your J1, slowly pull o...},
  chapter = {Snapmaker J1},
  howpublished = {https://forum.snapmaker.com/t/things-to-do-first-with-a-new-j1/29184},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\FT7D2YKS\29184.html}
}

@misc{thinkyheadWaitHotendTemperature2023,
  title = {Wait for {{Hotend Temperature}}},
  author = {{thinkyhead}},
  year = {2023},
  month = may,
  journal = {Marlin Firmware},
  urldate = {2023-06-04},
  abstract = {This command optionally sets a new target hot end temperature and waits for the target temperature to be reached before proceeding. If the temperature is set with S then M109 waits only when heating. If the temperature is set with R then M109 will also wait for the temperature to go down.},
  howpublished = {https://marlinfw.org/docs/gcode/M109.html},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\S5IV93UA\M109.html}
}

@misc{thomsSpidevPythonBindings,
  title = {Spidev: {{Python}} Bindings for {{Linux SPI}} Access through Spidev},
  shorttitle = {Spidev},
  author = {Thoms, Volker},
  urldate = {2022-05-20},
  copyright = {MIT License},
  keywords = {Software Development,System - Hardware,System - Hardware - Hardware Drivers},
  file = {C:\Users\emilm\Zotero\storage\TU7Y4WUU\spidev.html}
}

@misc{tingUnderstandingWebSocketsIts2020,
  title = {Understanding {{WebSockets}} and Its {{Possibilities}}},
  author = {Ting, William},
  year = {2020},
  month = aug,
  journal = {Medium},
  urldate = {2023-06-03},
  abstract = {The technology that makes it possible to open a two-way interactive communication session between the user's browser and a server},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\AUN96TJL\understanding-websockets-and-its-possibilities-ef4f5b48973f.html}
}

@misc{TipsTricksJetson,
  title = {Tips and {{Tricks}} for {{Jetson AGX Xavier}}},
  journal = {Gist},
  urldate = {2023-05-11},
  abstract = {Tips and Tricks for Jetson AGX Xavier. GitHub Gist: instantly share code, notes, and snippets.},
  howpublished = {https://gist.github.com/andrewssobral/ae77483b8fa147cce98b5b92f1a5ae17},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\7AYM67YQ\ae77483b8fa147cce98b5b92f1a5ae17.html}
}

@misc{todorovHowInstallUse,
  title = {How to {{Install}} and {{Use Chrony}} in {{Linux}}},
  author = {Todorov, Marin},
  urldate = {2022-05-20},
  abstract = {Chrony is a flexible implementation of the Network Time Protocol (NTP), which is used to synchronize the system clock from different NTP servers, reference clocks or via manual input.},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\F7ZI9XIM\install-chrony-in-centos-ubuntu-linux.html}
}

@article{toftLongTermVisualLocalization2022,
  title = {Long-{{Term Visual Localization Revisited}}},
  author = {Toft, Carl and Maddern, Will and Torii, Akihiko and Hammarstrand, Lars and Stenborg, Erik and Safari, Daniel and Okutomi, Masatoshi and Pollefeys, Marc and Sivic, Josef and Pajdla, Tomas and Kahl, Fredrik and Sattler, Torsten},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {4},
  pages = {2074--2088},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3032010},
  abstract = {Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing conditions, including day-night changes, as well as weather and seasonal variations, while providing highly accurate six degree-of-freedom (6DOF) camera pose estimates. In this paper, we extend three publicly available datasets containing images captured under a wide variety of viewing conditions, but lacking camera pose information, with ground truth pose information, making evaluation of the impact of various factors on 6DOF camera pose estimation accuracy possible. We also discuss the performance of state-of-the-art localization approaches on these datasets. Additionally, we release around half of the poses for all conditions, and keep the remaining half private as a test set, in the hopes that this will stimulate research on long-term visual localization, learned local image features, and related research areas. Our datasets are available at visuallocalization.net, where we are also hosting a benchmarking server for automatic evaluation of results on the test set. The presented state-of-the-art results are to a large degree based on submissions to our server.},
  keywords = {6DOF pose estimation,benchmark,Benchmark testing,Cameras,long-term localization,relocalization,Robots,Solid modeling,Three-dimensional displays,Trajectory,Visual localization,Visualization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Y5BC4KMM\\Toft et al. - 2022 - Long-Term Visual Localization Revisited.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AMNES45A\\9229078.html}
}

@misc{tomlogan501EnablingPPSXavier2020,
  title = {Enabling {{PPS}} on {{Xavier AGX}} - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  author = {Tomlogan501},
  year = {2020},
  month = dec,
  journal = {NVIDIA Developer Forums},
  urldate = {2022-05-20},
  abstract = {Finally I got something for the PPS injection in the AGX Xavier},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\2HY8GH59\16.html}
}

@misc{tornettaAustinVSCode2023,
  title = {Austin {{VS Code Extension}}},
  author = {Tornetta, Gabriele N.},
  year = {2023},
  month = jun,
  urldate = {2023-06-26},
  abstract = {Austin extension for Visual Studio Code},
  copyright = {MIT},
  keywords = {closember,hacktoberfest,profiling,python,vscode-extension}
}

@misc{torresAV1VsHEVC2022,
  title = {{{AV1}} vs {{HEVC}}: {{Is AV1}} a {{Better Codec}} than {{HEVC}} for the {{Future}}?},
  author = {Torres, Kaley},
  year = {2022},
  month = oct,
  urldate = {2023-06-20},
  howpublished = {https://www.winxdvd.com/convert-hevc-video/av1-vs-hevc.htm},
  file = {C:\Users\emilm\Zotero\storage\JDH889X3\av1-vs-hevc.html}
}

@article{torrMLESACNewRobust2000,
  title = {{{MLESAC}}: {{A New Robust Estimator}} with {{Application}} to {{Estimating Image Geometry}}},
  shorttitle = {{{MLESAC}}},
  author = {Torr, P. H. S. and Zisserman, A.},
  year = {2000},
  month = apr,
  journal = {Computer Vision and Image Understanding},
  volume = {78},
  number = {1},
  pages = {138--156},
  issn = {1077-3142},
  doi = {10.1006/cviu.1999.0832},
  urldate = {2022-11-27},
  abstract = {A new method is presented for robustly estimating multiple view relations from point correspondences. The method comprises two parts. The first is a new robust estimator MLESAC which is a generalization of the RANSAC estimator. It adopts the same sampling strategy as RANSAC to generate putative solutions, but chooses the solution that maximizes the likelihood rather than just the number of inliers. The second part of the algorithm is a general purpose method for automatically parameterizing these relations, using the output of MLESAC. A difficulty with multiview image relations is that there are often nonlinear constraints between the parameters, making optimization a difficult task. The parameterization method overcomes the difficulty of nonlinear constraints and conducts a constrained optimization. The method is general and its use is illustrated for the estimation of fundamental matrices, image{\textendash}image homographies, and quadratic transformations. Results are given for both synthetic and real images. It is demonstrated that the method gives results equal or superior to those of previous approaches.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TXAMWPHP\\Torr and Zisserman - 2000 - MLESAC A New Robust Estimator with Application to.pdf;C\:\\Users\\emilm\\Zotero\\storage\\Q2VEP8FG\\S1077314299908329.html}
}

@misc{triss64738SignalDelaysComponents2021,
  title = {Signal Delays between Components in the {{RP2040}} - {{Raspberry Pi Forums}}},
  author = {{triss64738}},
  year = {2021},
  month = jun,
  urldate = {2022-05-21},
  file = {C:\Users\emilm\Zotero\storage\Z49HYEZH\viewtopic.html}
}

@misc{TrueColorKodaka,
  title = {True {{Color Kodak Images}}},
  urldate = {2023-05-23},
  howpublished = {https://r0k.us/graphics/kodak/},
  file = {C:\Users\emilm\Zotero\storage\RX9YS32G\kodak.html}
}

@misc{TypingStubsPyGObject2023,
  title = {Typing {{Stubs}} for {{PyGObject}}},
  year = {2023},
  month = may,
  urldate = {2023-05-30},
  abstract = {PEP 561 Typing Stubs for PyGObject},
  copyright = {LGPL-2.1},
  howpublished = {PyGObject}
}

@techreport{u-bloxGPSEssentialsSatellite2009,
  title = {{{GPS Essentials}} of {{Satellite Navigation}}},
  author = {{u-blox}},
  year = {2009},
  number = {GPS-X-02007-D},
  file = {C:\Users\emilm\Zotero\storage\4INXAH4Y\gps_compendiumgps-x-02007.pdf}
}

@article{u-bloxZEDF9P04BDataSheet,
  title = {{{ZED-F9P-04B Data}} Sheet},
  author = {{u-blox}},
  pages = {25},
  abstract = {This data sheet describes the ZED-F9P high precision module with multiband GNSS receiver. The module provides multi-band RTK with fast convergence times, reliable performance and easy integration of RTK for fast time-to-market. It has a high update rate for highly dynamic applications and centimeter-level accuracy in a small and energy-efficient module.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\2FIMBAH9\ZED-F9P-04B Data sheet.pdf}
}

@techreport{u-bloxZEDF9PIntegrationManual,
  type = {Datasheet},
  title = {{{ZED-F9P Integration}} Manual},
  author = {{u-blox}},
  number = {UBX-18010802 - R11},
  pages = {119},
  urldate = {2022-05-02},
  abstract = {This document describes the features and application of the ZED-F9P, a multi-band GNSS module with integrated RTK offering centimeter-level accuracy.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\M4X9KVAU\ZED-F9P Integration manual.pdf}
}

@techreport{u-bloxZEDF9PInterfaceDescription,
  type = {Datasheet},
  title = {{{ZED-F9P Interface Description}}},
  author = {{u-blox}},
  number = {UBX-18010854 - R07},
  urldate = {2022-05-02},
  abstract = {The Interface Description describes the UBX (version 27. 11), NMEA and RTCM protocols and serves as a reference manual for the u-blox ZED-F9P high precision positioning receiver.},
  file = {C:\Users\emilm\Zotero\storage\HXCZQB48\PM-15136.pdf}
}

@techreport{u-bloxZEDF9PMovingBase,
  type = {Datasheet},
  title = {{{ZED-F9P Moving}} Base Applications},
  author = {{u-blox}},
  number = {UBX-19009093 - R02},
  urldate = {2022-05-02},
  file = {C:\Users\emilm\Zotero\storage\83PM3ULH\ZED-F9P-MovingBase_AppNote_(UBX-19009093).pdf}
}

@misc{UnableAchievePPS2022,
  title = {Unable to Achieve {{PPS}} with {{Jetpack}} 5.0.2},
  year = {2022},
  month = oct,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-04},
  abstract = {Hi,  We have previously used PPS with our Jetson Xavier NX (JP4.4) successfully, to get a precise time sync (PPS from GPS to NX). I have now upgraded the NX to Jetpack 5.0.2 and tried to follow the same procedure to enable PPS support, and have been able to get pps0 to show, but not pps1. Pps0 is the ktimer signal, I need the gpio-based one.  Here is what I have done:  Built kernel from source, with the following set in .config:   CONFIG\_PPS=y  CONFIG\_PPS\_CLIENT\_KTIMER=y  CONFIG\_PPS\_CLIENT\_LDISC...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/unable-to-achieve-pps-with-jetpack-5-0-2/232101},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\RIBQE8VM\24.html}
}

@misc{UnintendedContrastSpread2021,
  title = {Unintended Contrast Spread When Using {{Jetson}}, {{GStreamer}}, and {{H265}} to Encode Videos},
  year = {2021},
  month = jun,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-04-13},
  abstract = {Hi everyone,  While encoding grayscale images into videos using GStreamer on Jetson Xavier AGX, I experienced a shift in brightness values. After some investigation, I found out that a spreading of the contrast occurs: dark pixels became darker, light pixels became lighter.  Does anyone of you also experience this issue? Is there a way, e.g. a setting, to prevent this?},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/unintended-contrast-spread-when-using-jetson-gstreamer-and-h265-to-encode-videos/179488},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\WHLX5G2S\14.html}
}

@misc{UpdateL4T272018,
  title = {About {{Update L4T}} 27.1 -{$>$} 28.1 Error},
  year = {2018},
  month = jan,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-06},
  abstract = {Hi I'm Jang  I would appreciate your understanding that English is not my first language.  I have TX2 kit, L4T version is 27.1  In order to update the L4T version from 27.1 to 28.1, I have seen and followed the following manuals.  1. Download the L4T R28.1 for TX2 from link below.  https://www.dropbox.com/sh/8p3kgws42csrulu/AADYMwGXYE2\_qKjtPLVDawgta?dl=0  and follow below steps to flash the R28.1 image the TX2.  Set the TX2 to recovery mode  sudo tar xpf Tegra186\_Linux\_R28.1.0\_aarch64.tbz2  cd L...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/about-update-l4t-27-1-28-1-error/57156},
  langid = {english}
}

@misc{UsingSharedMemory2013,
  title = {Using {{Shared Memory}} in {{CUDA C}}/{{C}}++},
  year = {2013},
  month = jan,
  journal = {NVIDIA Technical Blog},
  urldate = {2023-04-14},
  abstract = {In the previous post, I looked at how global memory accesses by a group of threads can be coalesced into a single transaction, and how alignment and stride affect coalescing for various generations of{\ldots}},
  howpublished = {https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\LYBJW44N\using-shared-memory-cuda-cc.html}
}

@misc{utenriksdepartementetSvarPaSporsmal2023,
  type = {{ResponseToParliament}},
  title = {{Svar p{\aa} sp{\o}rsm{\aa}l om autonome v{\aa}pensystem}},
  author = {Utenriksdepartementet},
  year = {2023},
  month = jul,
  journal = {Regjeringen.no},
  publisher = {{regjeringen.no}},
  urldate = {2023-09-12},
  abstract = {Utenriksminister Anniken Huitfeldts svar p{\aa} et sp{\o}rsm{\aa}l fra Ingrid Fiskaa (SV) om regulering av autonome v{\aa}pensystemer.},
  howpublished = {https://www.regjeringen.no/no/aktuelt/svar-pa-sporsmal-om-autonome-vapensystem/id2989638/},
  langid = {norsk},
  file = {C:\Users\emilm\Zotero\storage\MTEYAK5U\id2989638.html}
}

@misc{valiantReplyStandbyTemperature2022,
  title = {Reply to "{{Standby Temperature}} Is Wrong \#12348"},
  author = {Valiant, Greg},
  year = {2022},
  month = may
}

@misc{VeiledningOmGrunnleggende,
  title = {{Veiledning om de grunnleggende personvernprinsippene}},
  journal = {Datatilsynet},
  urldate = {2023-09-13},
  abstract = {Reglene i personopplysningsloven bygger p{\aa} noen grunnleggende prinsipper. Alle som behandler personopplysninger m{\aa} opptre i samsvar med disse prinsippene.},
  howpublished = {https://www.datatilsynet.no/rettigheter-og-plikter/personvernprinsippene/grunnleggende-personvernprinsipper/},
  langid = {norsk},
  file = {C:\Users\emilm\Zotero\storage\RJ3Z778A\grunnleggende-personvernprinsipper.html}
}

@misc{vijayanagarBframesDifferencesUse2020,
  title = {I, {{P}}, and {{B-frames}} - {{Differences}} and {{Use Cases Made Easy}} - {{OTTVerse}}},
  author = {Vijayanagar, Krishna Rao},
  year = {2020},
  month = dec,
  urldate = {2023-04-12},
  abstract = {I-frames, P-frames, and B-frames are very important in video compression. I-frames help restore quality and resilience, while P \& B-frames improve compression.},
  chapter = {Compression},
  howpublished = {https://ottverse.com/i-p-b-frames-idr-keyframes-differences-usecases/},
  langid = {american},
  file = {C:\Users\emilm\Zotero\storage\MXRJLCBC\i-p-b-frames-idr-keyframes-differences-usecases.html}
}

@misc{vijayDashMantineComponents,
  title = {Dash {{Mantine Components}}},
  author = {Vijay, Snehil},
  urldate = {2023-06-26},
  abstract = {Official documentation and collection of ready-made Plotly Dash Components created using Dash Mantine Components. Dash Mantine Components is an extensive UI components library for Plotly Dash with over 70 components which support dark theme natively.},
  howpublished = {https://www.dash-mantine-components.com/},
  file = {C:\Users\emilm\Zotero\storage\BFRDS4J9\www.dash-mantine-components.com.html}
}

@misc{vijaySnehilvjAsyncdash2023,
  title = {Snehilvj/Async-Dash},
  author = {Vijay, Snehil},
  year = {2023},
  month = jun,
  urldate = {2023-06-02},
  abstract = {Async port of the official Plotly Dash library},
  keywords = {async,dash,plotly,plotly-dash,python,quart}
}

@misc{vijaySnehilvjAsyncdash2023a,
  title = {Snehilvj/Async-Dash},
  author = {Vijay, Snehil},
  year = {2023},
  month = jun,
  urldate = {2023-06-03},
  abstract = {Async port of the official Plotly Dash library}
}

@misc{visualstudiocodeDebuggingConfigurationsPython2023,
  title = {Debugging Configurations for {{Python}} Apps in {{Visual Studio Code}}},
  author = {Visual Studio Code},
  year = {2023},
  month = jan,
  urldate = {2023-05-30},
  abstract = {Details on configuring the Visual Studio Code debugger for different Python applications.},
  howpublished = {https://code.visualstudio.com/docs/python/debugging},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\ISHCF6NA\debugging.html}
}

@misc{visualstudiocodeDevelopingRemoteMachines2022,
  title = {Developing on {{Remote Machines}} Using {{SSH}} and {{Visual Studio Code}}},
  author = {Visual Studio Code},
  year = {2022},
  month = may,
  urldate = {2022-05-19},
  abstract = {Developing on Remote Machines or VMs using Visual Studio Code Remote Development and SSH},
  howpublished = {https://code.visualstudio.com/docs/remote/ssh},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\2IVQGSK2\ssh.html}
}

@phdthesis{volkovLatencyHiding2016,
  title = {Understanding Latency Hiding on {{GPUs}}},
  author = {Volkov, Vasily},
  year = {2016},
  month = aug,
  abstract = {Modern commodity processors such as GPUs may execute up to about a thousand of physical threads per chip to better utilize their numerous execution units and hide execution latencies. Understanding this novel capability, however, is hindered by the overall complexity of the hardware and complexity of typical workloads. In this dissertation, we suggest a better way to understand modern multithreaded performance by considering a family of synthetic workloads, which use the same key hardware capabilities {\textendash} memory access, arithmetic operations, and multithreading {\textendash} but are otherwise as simple as possible. One of our surprising findings is that prior performance models for GPUs fail on these workloads: they mispredict observed throughputs by factors of up to 1.7. We analyze these prior approaches, identify a number of common pitfalls, and discuss the related subtleties in understanding concurrency and Little's Law. Also, we help to further our understanding by considering a few basic questions, such as on how different latencies compare with each other in terms of latency hiding, and how the number of threads needed to hide latency depends on basic parameters of executed code such as arithmetic intensity. Finally, we outline a performance modeling framework that is free from the found limitations. As a tangential development, we present a number of novel experimental studies, such as on how mean memory latency depends on memory throughput, how latencies of individual memory accesses are distributed around the mean, and how occupancy varies during execution.},
  school = {EECS Department, University of California, Berkeley},
  file = {C:\Users\emilm\Zotero\storage\2RVMPJE8\Volkov - Understanding Latency Hiding on GPUs.pdf}
}

@article{volkovUnderstandingLatencyHiding,
  title = {Understanding {{Latency Hiding}} on {{GPUs}}},
  author = {Volkov, Vasily},
  langid = {english}
}

@misc{wallarmWebSocketVsHTTP,
  title = {{{WebSocket}} vs {{HTTP}}: {{Detailed Comparison}} 2023},
  shorttitle = {{{WebSocket}} vs {{HTTP}}},
  author = {Wallarm},
  urldate = {2023-06-03},
  abstract = {While devs sought to pick viable communication protocols, HTTP and WebSockets are commonly used terms that they will come across. Websocket pros and cons.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\F8FEP5QV\websocket-vs-http-how-are-these-2-different.html}
}

@misc{wangDirectShapeDirectPhotometric2020,
  title = {{{DirectShape}}: {{Direct Photometric Alignment}} of {{Shape Priors}} for {{Visual Vehicle Pose}} and {{Shape Estimation}}},
  shorttitle = {{{DirectShape}}},
  author = {Wang, Rui and Yang, Nan and Stueckler, Joerg and Cremers, Daniel},
  year = {2020},
  month = mar,
  number = {arXiv:1904.10097},
  eprint = {1904.10097},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.10097},
  urldate = {2022-11-28},
  abstract = {Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\KZ2TYTG4\\Wang et al. - 2020 - DirectShape Direct Photometric Alignment of Shape.pdf;C\:\\Users\\emilm\\Zotero\\storage\\WT7RBMXX\\1904.html}
}

@misc{wangDiskWritePerformance2022,
  title = {Disk Write Performance Issue on {{Jetson AGX Orin}}},
  author = {Wang, Zhongpin},
  year = {2022},
  month = oct,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-22},
  abstract = {Hi,  During the test of the Jetson AGX Orin dev board, we encountered continuosly IO bottleneck of the built-in storage. The problem first appeared when we tried to pull and build docker images. It took very long to download and extract files, especially for large files.  Then, we performed the following disk write test both on Jetson AGX Xavier and Jetson AGX Orin. Both of them are dev boards.  countArr=(\$(seq 1 1 10))  for count in "\$\{countArr[@]\}" do     echo "Count: \$\{count\}"     sudo dd if=...},
  chapter = {Autonomous Machines},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KB2MBAK2\231956.html}
}

@misc{wangFastOnlineObject2019,
  title = {Fast {{Online Object Tracking}} and {{Segmentation}}: {{A Unifying Approach}}},
  shorttitle = {Fast {{Online Object Tracking}} and {{Segmentation}}},
  author = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H. S.},
  year = {2019},
  month = may,
  number = {arXiv:1812.05050},
  eprint = {1812.05050},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.05050},
  urldate = {2023-11-01},
  abstract = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/{\textasciitilde}qwang/SiamMask.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\H5FVYWQI\\Wang et al_2019_Fast Online Object Tracking and Segmentation.pdf;C\:\\Users\\emilm\\Zotero\\storage\\I52EDY2V\\1812.html}
}

@misc{wardExamplesMultipageApps03Jul22,
  title = {Examples of Multi-Page Apps with {{Dash Pages}}},
  author = {Ward, Ann Marie},
  year = {3 Jul '22},
  journal = {Plotly Community Forum},
  urldate = {2023-06-03},
  abstract = {I made this GitHub repo  to help people get started making multi-page apps with the new Dash Pages feature available in Dash 2.5.1.  It has lots of examples you can use to explore some of the great new features when using pages.  You will also find other handy tips and tricks as well.  If you have ideas for other examples you would like to see, please open and issue. Pull requests are welcome too!      Dash pages multi-page app demos  This repo contains minimal examples of multi-page apps using ...},
  chapter = {Dash Python},
  howpublished = {https://community.plotly.com/t/examples-of-multi-page-apps-with-dash-pages/66489},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\TIWV3J4H\66489.html}
}

@misc{waynewwwScriptBypassAccountJun2819,
  title = {Script to Bypass the Account Setting},
  author = {WayneWWW},
  year = {Jun 28 '19},
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-16},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/jetson-nano-all-usb-ports-suddenly-stopped-working/75784/37?u=emil.martens},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\WFWMR5B4\34.html}
}

@misc{wenzelEigenPybind11Documentation2017,
  title = {Eigen - Pybind11 Documentation},
  author = {Wenzel, Jakob},
  year = {2017},
  urldate = {2023-06-26},
  howpublished = {https://pybind11.readthedocs.io/en/stable/advanced/cast/eigen.html},
  file = {C:\Users\emilm\Zotero\storage\ZSHG6QTB\eigen.html}
}

@misc{westfwSDKIrqLatency2021,
  title = {C {{SDK}} Irq Latency Is Only  1.06{\textmu}s 200ns - {{Raspberry Pi Forums}}},
  author = {WestfW},
  year = {2021},
  month = mar,
  urldate = {2022-05-21},
  file = {C:\Users\emilm\Zotero\storage\7L4B2Y64\viewtopic.html}
}

@misc{WhyNotSet2021,
  title = {Why Not Set the Latest Tag Defaultly in Source\_sync.Sh?},
  year = {2021},
  month = dec,
  journal = {NVIDIA Developer Forums},
  urldate = {2023-05-03},
  abstract = {Hi,  In source\_sync.sh, if you run it without -t, you should input TAG or Enter again and again.  On the other hand, if you want to figure out tag names excactly, you should use command git tag -l tegra-l4t* but the repo have not been initialized you cannot get correct output by this command.  So, why not set the latest tag as default? It's helpful to users who are not familiar with L4T. On the other hand, set -t parameter to git describe --tags \$(git rev-list --tags --max-count=1) is not harmfu...},
  chapter = {Autonomous Machines},
  howpublished = {https://forums.developer.nvidia.com/t/why-not-set-the-latest-tag-defaultly-in-source-sync-sh/198575},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\ILEED8W3\198575.html}
}

@misc{willsWebVideoServer,
  title = {Web\_video\_server - {{ROS Wiki}}},
  author = {Wills, Mitchell},
  urldate = {2022-05-30},
  howpublished = {http://wiki.ros.org/web\_video\_server},
  file = {C:\Users\emilm\Zotero\storage\SU8PRLQ3\web_video_server.html}
}

@misc{WinFspWindowsFile2023,
  title = {{{WinFsp}} {$\cdot$} {{Windows File System Proxy}}},
  year = {2023},
  month = may,
  urldate = {2023-05-06},
  abstract = {Windows File System Proxy - FUSE for Windows},
  howpublished = {WinFsp},
  keywords = {driver,filesystem,fuse,gplv3,kernel,windows,windows-kernel}
}

@article{winterCisco3ComApplied2009,
  title = {Dell: {{Cisco}}: {{3Com}}: {{Applied Micro}}: {{Ethernet Alliance}}: {{NetApp}}: {{Force}} 10: {{Intel}}: {{Qlogic}}:},
  author = {Winter, Robert and Hernandez, Rich and Chawla, Gaurav and Faustini, Anthony and Solder, Carl and Scheibe, Thomas and Law, David and Ayandeh, Siamick and Booth, Brad and Kohl, Blaine and Lavacchia, Charlie and Krishnamurthy, Subi and Karthikeyan, Raja and Multanen, Eric and Wadekar, Manoj},
  year = {2009},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\KFY43FLZ\Winter et al. - 2009 - Dell Cisco 3Com Applied Micro Ethernet Allianc.pdf}
}

@misc{wongMEMSIMUsUltimate2017,
  title = {{{MEMS IMUs}}: {{The Ultimate}} in {{Sensor Fusion}}},
  shorttitle = {{{MEMS IMUs}}},
  author = {Wong, William G.},
  year = {2017},
  month = dec,
  journal = {Electronic Design},
  urldate = {2022-06-07},
  abstract = {From wearables to UAVs, MEMS inertial measurement units (IMUs) pack the performance needed for today's advanced applications.},
  howpublished = {https://www.electronicdesign.com/technologies/test-measurement/article/21805896/mems-imus-the-ultimate-in-sensor-fusion},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\QE76CDHW\mems-imus-the-ultimate-in-sensor-fusion.html}
}

@misc{WSGIServers,
  title = {{{WSGI Servers}}},
  urldate = {2023-05-15},
  abstract = {A Web Server Gateway Interface (WSGI) server runs Python code to create a web application. Learn more about WSGI servers on Full Stack Python.},
  howpublished = {https://www.fullstackpython.com/wsgi-servers.html},
  langid = {english}
}

@misc{wu4DGaussianSplatting2023,
  title = {{{4D Gaussian Splatting}} for {{Real-Time Dynamic Scene Rendering}}},
  author = {Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei, Wei and Liu, Wenyu and Tian, Qi and Wang, Xinggang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08528},
  eprint = {2310.08528},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.08528},
  urldate = {2023-11-22},
  abstract = {Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800\${\textbackslash}times\$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QTTTIV73\\Wu et al_2023_4D Gaussian Splatting for Real-Time Dynamic Scene Rendering.pdf;C\:\\Users\\emilm\\Zotero\\storage\\XU8CCBC6\\2310.html}
}

@article{wuketichGeneralizedPlasmocytomaUnusually1963,
  title = {{[Generalized plasmocytoma with an unusually high degree of infiltration of the stomach]}},
  author = {Wuketich, S. and Maehr, G.},
  year = {1963},
  month = mar,
  journal = {Wiener Klinische Wochenschrift},
  volume = {75},
  pages = {224--232},
  issn = {0043-5325},
  langid = {german},
  pmid = {14002037},
  keywords = {Multiple Myeloma,MULTIPLE MYELOMA,Neoplasm Metastasis,NEOPLASM METASTASIS,Plasmacytoma,Stomach Neoplasms,STOMACH NEOPLASMS}
}

@misc{X264NumberFrames,
  title = {X264 : Number of {{B}} Frames and Ref [{{Archive}}] - {{Doom9}}'s {{Forum}}},
  urldate = {2023-04-12},
  howpublished = {http://forum.doom9.org/archive/index.php/t-165490.html},
  file = {C:\Users\emilm\Zotero\storage\6IY2HMDQ\t-165490.html}
}

@misc{xkcdAutomation2014,
  title = {Automation},
  author = {XKCD},
  year = {2014},
  month = jan
}

@misc{xkcdCautionary2008,
  title = {Cautionary},
  author = {XKCD},
  year = {2008},
  month = jul,
  urldate = {2023-05-06}
}

@misc{xkcdEstimatingTime2016,
  title = {Estimating {{Time}}},
  author = {XKCD},
  year = {2016},
  month = mar,
  urldate = {2023-05-27},
  file = {C:\Users\emilm\Zotero\storage\9DXTB3ML\1658.html}
}

@misc{xuPointNeRFPointbasedNeural2023,
  title = {Point-{{NeRF}}: {{Point-based Neural Radiance Fields}}},
  shorttitle = {Point-{{NeRF}}},
  author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
  year = {2023},
  month = mar,
  number = {arXiv:2201.08845},
  eprint = {2201.08845},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.08845},
  urldate = {2023-11-08},
  abstract = {Volumetric neural rendering methods like NeRF generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30X faster training time. Point-NeRF can be combined with other 3D reconstruction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. The experiments on the DTU, the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets demonstrate Point-NeRF can surpass the existing methods and achieve the state-of-the-art results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\EEUGGIZU\\Xu et al_2023_Point-NeRF.pdf;C\:\\Users\\emilm\\Zotero\\storage\\5UIYH9YV\\2201.html}
}

@article{yamadaGuidingLabellingEffort2023,
  title = {Guiding {{Labelling Effort}} for {{Efficient Learning With Georeferenced Images}}},
  author = {Yamada, Takaki and {Massot-Campos}, Miquel and {Pr{\"u}gel-Bennett}, Adam and Pizarro, Oscar and Williams, Stefan B. and Thornton, Blair},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {01},
  pages = {593--607},
  publisher = {{IEEE Computer Society}},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2021.3140060},
  urldate = {2023-11-01},
  abstract = {We describe a novel semi-supervised learning method that reduces the labelling effort needed to train convolutional neural networks (CNNs) when processing georeferenced imagery. This allows deep learning CNNs to be trained on a per-dataset basis, which is useful in domains where there is limited learning transferability across datasets. The method identifies representative subsets of images from an unlabelled dataset based on the latent representation of a location guided autoencoder. We assess the method's sensitivities to design options using four different ground-truthed datasets of georeferenced environmental monitoring images, where these include various scenes in aerial and seafloor imagery. Efficiency gains are achieved for all the aerial and seafloor image datasets analysed in our experiments, demonstrating the benefit of the method across application domains. Compared to CNNs of the same architecture trained using conventional transfer and active learning, the method achieves equivalent accuracy with an order of magnitude fewer annotations, and 85 \% of the accuracy of CNNs trained conventionally with approximately 10,000 human annotations using just 40 prioritised annotations. The biggest gains in efficiency are seen in datasets with unbalanced class distributions and rare classes that have a relatively small number of observations.},
  langid = {english},
  file = {C:\Users\emilm\Zotero\storage\C84LJ7AW\Yamada et al_2023_Guiding Labelling Effort for Efficient Learning With Georeferenced Images.pdf}
}

@article{yamadaLeveragingMetadataRepresentation2021,
  title = {Leveraging {{Metadata}} in {{Representation Learning With Georeferenced Seafloor Imagery}}},
  author = {Yamada, Takaki and {Massot-Campos}, Miquel and {Prugel-Bennett}, A. and Williams, Stefan and Pizarro, Oscar and Thornton, Blair},
  year = {2021},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  pages = {7815--7822},
  doi = {10.1109/LRA.2021.3101881},
  abstract = {Camera equipped Autonomous Underwater Vehicles (AUVs) are now routinely used in seafloor surveys. Obtaining effective representations from the images they collect can enable perception-aware robotic exploration such as information-gain-guided path planning and target-driven visual navigation. This letter develops a novel self-supervised representation learning method for seafloor images collected by AUVs. The method allows deep-learning convolutional autoencoders to leverage multiple sources of metadata to regularise their learning, prioritising features observed in images that can be correlated with patterns in their metadata. The impact of the proposed regularisation is examined on a dataset consisting of more than 30 k colour seafloor images gathered by an AUV off the coast of Tasmania. The metadata used to regularise learning in this dataset consists of the horizontal location and depth of the observed seafloor. The results show that including metadata in self-supervised representation learning can increase image classification accuracy by up to 15\% and never degrades learning performance. We show how effective representation learning can be applied to achieve class balanced representative image identification for summarised understanding of imbalanced class distributions in an unsupervised way.},
  file = {C:\Users\emilm\Zotero\storage\MDM8AQV2\Yamada et al_2021_Leveraging Metadata in Representation Learning With Georeferenced Seafloor.pdf}
}

@misc{yannickilcherNeRFRepresentingScenes2021,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}} ({{ML Research Paper Explained}})},
  shorttitle = {{{NeRF}}},
  author = {{Yannic Kilcher}},
  year = {2021},
  month = apr,
  urldate = {2023-10-18},
  abstract = {\#nerf \#neuralrendering \#deeplearning View Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency. OUTLINE: 0:00 - Intro \& Overview 4:50 - View Synthesis Task Description 5:50 - The fundamental difference to classic Deep Learning 7:00 - NeRF Core Concept 15:30 - Training the NeRF from sparse views 20:50 - Radiance Field Volume Rendering 23:20 - Resulting View Dependence 24:00 - Positional Encoding 28:00 - Hierarchical Volume Sampling 30:15 - Experimental Results 33:30 - Comments \& Conclusion Paper: https://arxiv.org/abs/2003.08934 Website \& Code: https://www.matthewtancik.com/nerf My Video on SIREN: ~~~{\textbullet}~SIREN:~Implicit~Neural~Representation...~~ Abstract: We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction ({\texttheta},{$\phi$})) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons. Authors: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng Links: TabNine Code Completion (Referral): http://bit.ly/tabnine-yannick YouTube: ~~~/~yannickilcher~~ Twitter: https://twitter.com/ykilcher Discord: https://discord.gg/4H8xxDF BitChute: https://www.bitchute.com/channel/yann... Minds: https://www.minds.com/ykilcher Parler: https://parler.com/profile/YannicKilcher LinkedIn: https://www.linkedin.com/in/yannic-ki... BiliBili: https://space.bilibili.com/1824646584 If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: https://www.patreon.com/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n}
}

@article{YCbCr2023,
  title = {{{YCbCr}}},
  year = {2023},
  month = apr,
  journal = {Wikipedia},
  urldate = {2023-06-14},
  abstract = {YCbCr, Y{${'}$}CbCr, or Y Pb/Cb Pr/Cr, also written as YCBCR or Y{${'}$}CBCR, is a family of color spaces used as a part of the color image pipeline in video and digital photography systems. Y{${'}$} is the luma component and CB and CR are the blue-difference and red-difference chroma components. Y{${'}$} (with prime) is distinguished from Y, which is luminance, meaning that light intensity is nonlinearly encoded based on gamma corrected RGB primaries. Y{${'}$}CbCr color spaces are defined by a mathematical coordinate transformation from an associated RGB primaries and white point. If the underlying RGB color space is absolute, the Y{${'}$}CbCr color space is an absolute color space as well; conversely, if the RGB space is ill-defined, so is Y{${'}$}CbCr. The transformation is defined in ITU-T H.273. Nevertheless that rule does not apply to P3-D65 primaries used by Netflix with BT.2020-NCL matrix, so that means matrix was not derived from primaries, but now Netflix allows BT.2020 primaries (since 2021). Same happens with JPEG: it has BT.601 matrix derived from System M primaries, yet the primaries of most images are BT.709.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1147788722},
  file = {C:\Users\emilm\Zotero\storage\UGYQGXVL\YCbCr.html}
}

@phdthesis{yeganehCrossDynamicRange2014,
  title = {Cross {{Dynamic Range And Cross Resolution Objective Image Quality Assessment With Applications}}},
  author = {Yeganeh, Hojatollah},
  year = {2014},
  month = jul,
  doi = {10.13140/RG.2.1.1900.2320},
  abstract = {In recent years, image and video signals have become an indispensable part of human life. There has been an increasing demand for high quality image and video products and services. To monitor, maintain and enhance image and video quality objective image and video quality assessment tools play crucial roles in a wide range of applications throughout the {\dbend}eld of image and video processing, including image and video acquisition, communication, interpolation, retrieval, and displaying. A number of objective image and video quality measures have been introduced in the last decades such as mean square error (MSE), peak signal to noise ratio (PSNR), and structural similarity index (SSIM). However, they are not applicable when the dynamic range or spatial resolution of images being compared is di{\dbend}erent from that of the corresponding reference images. In this thesis, we aim to tackle these two main problems in the {\dbend}eld of image quality assessment. Tone mapping operators (TMOs) that convert high dynamic range (HDR) to low dynamic range (LDR) images provide practically useful tools for the visualization of HDR images on standard LDR displays. Most TMOs have been designed in the absence of a well-established and subject-validated image quality assessment (IQA) model, without which fair comparisons and further improvement are di{\dbend}cult. We propose an objective quality assessment algorithm for tone-mapped images using HDR images as references by combining 1) a multi-scale signal {\dbend}delity measure based on a modi{\dbend}ed structural similarity (SSIM) index; and 2) a naturalness measure based on intensity statistics of natural images. To evaluate the proposed Tone-Mapped image Quality Index (TMQI), its performance in several applications and optimization problems is provided. Speci{\dbend}cally, the main component of TMQI known as structural {\dbend}delity is modi{\dbend}ed and adopted to enhance the visualization of HDR medical images on standard displays. Moreover, a substantially di{\dbend}erent approach to design TMOs is presented, where instead of using any pre-de{\dbend}ned systematic computational structure (such as image transformation or contrast/edge enhancement) for tone-mapping, we navigate in the space of all LDR images, searching for the image that maximizes structural {\dbend}delity or TMQI. There has been an increasing number of image interpolation and image super-resolution (SR) algorithms proposed recently to create images with higher spatial resolution from lowresolution (LR) images. However, the evaluation of such SR and interpolation algorithms is cumbersome. Most existing image quality measures are not applicable because LR and resultant high resolution (HR) images have di{\dbend}erent spatial resolutions. We make one of the {\dbend}rst attempts to develop objective quality assessment methods to compare LR and HR images. Our method adopts a framework based on natural scene statistics (NSS) where image quality degradation is gauged by the deviation of its statistical features from NSS models trained upon high quality natural images. In particular, we extract frequency energy fallo{\dbend}, dominant orientation and spatial continuity statistics from natural images and build statistical models to describe such statistics. These models are then used to measure statistical naturalness of interpolated images. We carried out subjective tests to validate our approach, which also demonstrates promising results. The performance of the proposed measure is further evaluated when applied to parameter tuning in image interpolation algorithms.},
  file = {C:\Users\emilm\Zotero\storage\2SBZUANK\Yeganeh_2014_Cross Dynamic Range And Cross Resolution Objective Image Quality Assessment.pdf}
}

@misc{yuPlenoxelsRadianceFields2021a,
  title = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle = {Plenoxels},
  author = {Yu, Alex and {Fridovich-Keil}, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  year = {2021},
  month = dec,
  number = {arXiv:2112.05131},
  eprint = {2112.05131},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-25},
  abstract = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Y65YUGCL\\Yu et al_2021_Plenoxels.pdf;C\:\\Users\\emilm\\Zotero\\storage\\WEY466LC\\2112.html}
}

@article{YUV2023,
  title = {{{YUV}}},
  year = {2023},
  month = mar,
  journal = {Wikipedia},
  urldate = {2023-03-23},
  abstract = {YUV is a color model typically used as part of a color image pipeline. It encodes a color image or video taking human perception into account, allowing reduced bandwidth for chrominance components, compared to a "direct" RGB-representation. Historically, the terms YUV and Y{${'}$}UV were used for a specific analog encoding of color information in television systems. Today, the term YUV is commonly used in the computer industry to describe colorspaces that are encoded using YCbCr. The YUV model defines one luminance component (Y) meaning physical linear-space brightness, and two chrominance components, called U (blue projection) and V (red projection) respectively. It can be used to convert to and from the RGB model, and with different color spaces. The closely related Y{${'}$}UV model uses the luma component (Y{${'}$}) {\textendash} nonlinear perceptual brightness, with the prime symbols (') denoting gamma correction. Y{${'}$}UV is used in the PAL analogue color TV standard (excluding PAL-N). Previous black-and-white systems used only luma (Y{${'}$}) information.  Color information (U and V) was added separately via a subcarrier so that a black-and-white receiver would still be able to receive and display a color picture transmission in the receiver's native black-and-white format, with no need for extra transmission bandwidth. As for etymology, Y, Y{${'}$}, U, and V are not abbreviations. The use of the letter Y for luminance can be traced back to the choice of XYZ primaries. This lends itself naturally to the usage of the same letter in luma (Y{${'}$}), which approximates a perceptually uniform correlate of luminance. Likewise, U and V were chosen to differentiate the U and V axes from those in other spaces, such as the x and y chromaticity space. See the equations below or compare the historical development of the math.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1143386199},
  file = {C:\Users\emilm\Zotero\storage\AWNGK7M8\YUV.html}
}

@article{YUV2023a,
  title = {{{YUV}}},
  year = {2023},
  month = mar,
  journal = {Wikipedia},
  urldate = {2023-06-14},
  abstract = {YUV is a color model typically used as part of a color image pipeline. It encodes a color image or video taking human perception into account, allowing reduced bandwidth for chrominance components, compared to a "direct" RGB-representation. Historically, the terms YUV and Y{${'}$}UV were used for a specific analog encoding of color information in television systems. Today, the term YUV is commonly used in the computer industry to describe colorspaces that are encoded using YCbCr. The YUV model defines one luminance component (Y) meaning physical linear-space brightness, and two chrominance components, called U (blue projection) and V (red projection) respectively. It can be used to convert to and from the RGB model, and with different color spaces. The closely related Y{${'}$}UV model uses the luma component (Y{${'}$}) {\textendash} nonlinear perceptual brightness, with the prime symbols (') denoting gamma correction. Y{${'}$}UV is used in the PAL analogue color TV standard (excluding PAL-N). Previous black-and-white systems used only luma (Y{${'}$}) information.  Color information (U and V) was added separately via a subcarrier so that a black-and-white receiver would still be able to receive and display a color picture transmission in the receiver's native black-and-white format, with no need for extra transmission bandwidth. As for etymology, Y, Y{${'}$}, U, and V are not abbreviations. The use of the letter Y for luminance can be traced back to the choice of XYZ primaries. This lends itself naturally to the usage of the same letter in luma (Y{${'}$}), which approximates a perceptually uniform correlate of luminance. Likewise, U and V were chosen to differentiate the U and V axes from those in other spaces, such as the x and y chromaticity space. See the equations below or compare the historical development of the math.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1143386199},
  file = {C:\Users\emilm\Zotero\storage\ETJPMESN\YUV.html}
}

@misc{zhangEncodingMethodNERF2022,
  title = {Encoding {{Method}} for {{NERF}}},
  author = {Zhang, Qiang},
  year = {2022},
  month = sep,
  journal = {Qiang Zhang},
  urldate = {2023-11-21},
  abstract = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding tries to reduce inference cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality. This is achieved via a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are op- timized through stochastic gradient descent.},
  howpublished = {https://www.zhqiang.org/nerf-encoding/},
  file = {C:\Users\emilm\Zotero\storage\FH3PFHRL\nerf-encoding.html}
}

@article{zhangReferencePoseGeneration2021,
  title = {Reference {{Pose Generation}} for {{Long-term Visual Localization}} via {{Learned Features}} and {{View Synthesis}}},
  author = {Zhang, Zichao and Sattler, Torsten and Scaramuzza, Davide},
  year = {2021},
  month = apr,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {4},
  eprint = {2005.05179},
  primaryclass = {cs},
  pages = {821--844},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01399-8},
  urldate = {2022-11-27},
  abstract = {Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/ night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day-Night dataset, showing that state-of-the-art visual localization methods perform better (up to \$47{\textbackslash}\%\$) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\PGJA8DE5\\Zhang et al. - 2021 - Reference Pose Generation for Long-term Visual Loc.pdf;C\:\\Users\\emilm\\Zotero\\storage\\4JTQK2PQ\\2005.html}
}

@article{zhangRILIOReflectivityImage2023,
  title = {{{RI-LIO}}: {{Reflectivity Image Assisted Tightly-Coupled LiDAR-Inertial Odometry}}},
  shorttitle = {{{RI-LIO}}},
  author = {Zhang, Yanfeng and Tian, Yunong and Wang, Wanguo and Yang, Guodong and Li, Zhishuo and Jing, Fengshui and Tan, Min},
  year = {2023},
  month = mar,
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {3},
  pages = {1802--1809},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3243528},
  urldate = {2023-11-27},
  abstract = {In this letter, we propose RI-LIO, a new reflectivity image assisted tightly-coupled LiDAR-inertial odometry (LIO) framework that introduces additional reflectivity texture information to efficiently reduce the drift of geometric-only methods. To achieve this, we construct an iterated extended Kalman filter framework by blending the point-to-plane geometric measurement and the reflectivity image measurement. Specifically, the geometric measurement is defined as the distance from the raw point of a new scan to its nearest neighbor plane in the global incremental kd-tree map. The searched nearest neighbor point is used to render a sparse reflectivity image after LiDAR motion distortion information is given by its corresponding raw point. Then, the reflectivity measurement is built to align the sparse reflectivity image with the dense reflectivity image of the current scan by minimizing the photometric errors directly. In addition, based on the mechanism of high-resolution LiDAR, a corrected spherical projection model is proposed to project spatial points into the image frame. Finally, extensive experiments are conducted in structured, unstructured and challenging open field scenarios. The results demonstrate that the proposed method outperforms existing geometric-only methods in terms of robustness and accuracy, especially in the rotation direction.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FV92DD66\\Zhang et al_2023_RI-LIO.pdf;C\:\\Users\\emilm\\Zotero\\storage\\BGC7LX7Y\\10041769.html}
}

@article{zhengWildTerrestrialAnimal2022,
  title = {Wild {{Terrestrial Animal Re-Identification Based}} on an {{Improved Locally Aware Transformer}} with a {{Cross-Attention Mechanism}}},
  author = {Zheng, Zhaoxiang and Zhao, Yaqin and Li, Ao and Yu, Qiuping},
  year = {2022},
  month = jan,
  journal = {Animals},
  volume = {12},
  number = {24},
  pages = {3503},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-2615},
  doi = {10.3390/ani12243503},
  urldate = {2023-11-01},
  abstract = {The wildlife re-identification recognition methods based on the camera trap were used to identify different individuals of the same species using the fur, stripes, facial features and other features of the animal body surfaces in the images, which is an important way to count the individual number of a species. Re-identification of wild animals can provide solid technical support for the in-depth study of the number of individuals and living conditions of rare wild animals, as well as provide accurate and timely data support for population ecology and conservation biology research. However, due to the difficulty of recording the shy wild animals and distinguishing the similar fur of different individuals, only a few papers have focused on the re-identification recognition of wild animals. In order to fill this gap, we improved the locally aware transformer (LA transformer) network structure for the re-identification recognition of wild terrestrial animals. First of all, at the stage of feature extraction, we replaced the self-attention module of the LA transformer with a cross-attention block (CAB) in order to calculate the inner-patch attention and cross-patch attention, so that we could efficiently capture the global information of the animal body's surface and local feature differences of fur, colors, textures, or faces. Then, the locally aware network of the LA transformer was used to fuse the local and global features. Finally, the classification layer of the network realized wildlife individual recognition. In order to evaluate the performance of the model, we tested it on a dataset of Amur tiger torsos and the face datasets of six different species, including lions, golden monkeys, meerkats, red pandas, tigers, and chimpanzees. The experimental results showed that our wildlife re-identification model has good generalization ability and is superior to the existing methods in mAP (mean average precision), and obtained comparable results in the metrics Rank 1 and Rank 5.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cross-attention mechanism,local feature differences,locally aware transformer,wild terrestrial animal re-identification},
  file = {C:\Users\emilm\Zotero\storage\MKBTHGL7\Zheng et al_2022_Wild Terrestrial Animal Re-Identification Based on an Improved Locally Aware.pdf}
}

@article{zhouTransVODEndtoEndVideo2023,
  title = {{{TransVOD}}: {{End-to-End Video Object Detection}} with {{Spatial-Temporal Transformers}}},
  shorttitle = {{{TransVOD}}},
  author = {Zhou, Qianyu and Li, Xiangtai and He, Lu and Yang, Yibo and Cheng, Guangliang and Tong, Yunhai and Ma, Lizhuang and Tao, Dacheng},
  year = {2023},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {6},
  eprint = {2201.05047},
  primaryclass = {cs},
  pages = {7853--7869},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3223955},
  urldate = {2023-09-14},
  abstract = {Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, the first end-to-end video object detection system based on spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3\%-4\% mAP) on the ImageNet VID dataset. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0\% mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7\% mAP while running at around 30 FPS on a single V100 GPU device.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\V5YZTNAU\\Zhou et al. - 2023 - TransVOD End-to-End Video Object Detection with S.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AXEWK3YU\\2201.html}
}

@article{zitnickHighqualityVideoView2004,
  title = {High-Quality Video View Interpolation Using a Layered Representation},
  author = {Zitnick, C. Lawrence and Kang, Sing Bing and Uyttendaele, Matthew and Winder, Simon and Szeliski, Richard},
  year = {2004},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {23},
  number = {3},
  pages = {600--608},
  issn = {0730-0301},
  doi = {10.1145/1015706.1015766},
  urldate = {2023-11-08},
  abstract = {The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis. Finally, a novel temporal two-layer compressed representation that handles matting is developed for rendering at interactive rates.},
  keywords = {Computer Vision,Dynamic Scenes,Image-Based Rendering},
  file = {C:\Users\emilm\Zotero\storage\PH7WN4AS\Zitnick et al_2004_High-quality video view interpolation using a layered representation.pdf}
}

@misc{zotero-1288,
  urldate = {2023-11-22},
  howpublished = {https://dynamic3dgaussians.github.io/},
  file = {C:\Users\emilm\Zotero\storage\2FZBA8J9\dynamic3dgaussians.github.io.html}
}

@article{zotero-145,
  type = {Article}
}

@misc{zouObjectDetection202019,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  year = {2019},
  month = may,
  number = {arXiv:1905.05055},
  eprint = {1905.05055},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-07-19},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
  archiveprefix = {arxiv},
  keywords = {Annette\_tip,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CDB92B5D\\Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf;C\:\\Users\\emilm\\Zotero\\storage\\KZQ6GS3Y\\1905.html}
}
