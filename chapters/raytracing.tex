\section{A New Approach}
While the Gaussian splatting method is very impressive, this section will discuss a potential new path for \gls{nvs} inspired by the Gaussian splatting method.
The core idea is to leverage the power of hardware-accelerated ray tracing and replace the current rasterization pipeline with a ray tracing pipeline.
Additionally, the use of polarization cameras might prove useful in this regard, as reflected light has a distinct polarization signature \cite{lingUniversityPhysicsVolume2016}.

\subsection{Ray Tracing}
Ray tracing is a rendering technique that simulates how rays of light traverse a scene.
Unlike rasterization, which confines rendering to a single viewpoint at a time by projecting the scene onto a two-dimensional image plane, ray tracing can simultaneously render rays originating from multiple viewpoints \cite{caulfieldWhatPathTracing2022}.
This capability lets ray tracing programs simulate lifelike lighting scenarios by estimating how light scatters and reflects off objects in the scene \cite{caulfieldWhatPathTracing2022}.

As ray tracing has become a key component of rendering in the video game industry, specialized hardware has been developed to accelerate the process to achieve real-time performance.
Modern NVIDIA GPUs have dedicated \gls{rt} cores that can be used to accelerate ray tracing through the NVIDIA OptiX\textsuperscript{TM} API \cite{nvidiaNVIDIAOptiXProgramming2023}.

The main computational bottleneck of ray tracing is finding the intersections between the rays and the scene geometry.
To speed up this process, a \gls{bvh} is used to store the scene geometry \cite{nvidiaNVIDIAOptiXProgramming2023}.
A \gls{bvh} is a tree data structure that recursively divides the scene into smaller and smaller bounding boxes making it possible to quickly discard large parts of the scene that the ray does not intersect with \cite{nvidiaNVIDIAOptiXProgramming2023}.
The \gls{rt} cores are specialized hardware that can traverse the \gls{bvh} and find intersections between rays and scene objects very fast \cite{nvidiaNVIDIAOptiXProgramming2023}.


\subsection{Triangle Representation}
While Optix supports custom primitive types, making the use of Gaussians possible, operations on custom primitives are only partially accelerated by the \gls{rt} cores \cite{nvidiaOptiXQuickStart2018}\cite{nvidiaNVIDIAOptiXProgramming2023}.
This makes the use of triangles a more attractive option.
The implementation in the paper projects the Gaussians onto the image plane, i.e. they are not performing any volumetric rendering,
making it likely that using triangles also should be possible.

Instead of storing position, orientation and scale parameters, we can store the position color and opacity of the three vertices of the triangle.
This representation is simpler than the Gaussian representation and removes the necessity for activation functions used to ensure a positive definite covariance matrix.
The density function of each triangle can be calculated using the barycentric coordinates of the intersection point, as this is supplied by Optix \cite{nvidiaNVIDIAOptiXProgramming2023}.
Several different polynomial-based functions have been tested to obtain a smooth opacity function, as shown in Figure \ref{fig:alphas}.

Hypothetically, it could be possible to use meshes instead of individual triangles, that is multiple triangles sharing the same vertices, as this is also supported by Optix \cite{nvidiaNVIDIAOptiXProgramming2023}.
This will make a scene representation more compact but will make the optimization problem more complex.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/alphas.png}
    \caption{Experimentation with different polynomial functions on barycentric coordinate values to obtain a smooth opacity function. The lower right vertex of each triangle has opacity 0 to test vertex-dependent opacity.}
    \label{fig:alphas}
\end{figure}

\subsection{Program Structure}
The program structure will be similar to the current implementation, with a few key differences.
As ray tracing is used, there is no need for any projection step as described in Section \ref{sec:projection}.
The rendering process for each pixel in the paper will be matched by a rendering process for each ray in the new implementation, using the same blending function as described in Equation \ref{eq:alpha_blending}.

The optimization process will however be different.
During the rendering process, information about each intersection will be captured and stored in a buffer.
Taking inspiration from the paper, after the rendering passes the collection of intersections can be sorted by the index of the triangle they intersect with.
Then for each triangle, an optimization function can be defined based on the collection of intersections with that triangle.
Having information about all the intersections on each triangle can be leveraged to perform smarter optimization steps, as illustrated in Figure \ref{fig:optimization}.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/optimization.png}
    \caption{Four illustrative examples of how intersection information can be used to delete, reshape move and regularize triangles. The color of the rays is the color of the pixel from which it originates.}
    \label{fig:optimization}
\end{figure}

\subsection{Remove Popping Artifacts}
As discussed, a key issue with the current implementation is popping artifacts where Gaussians suddenly appear or disappear behind other Gaussians.
Raytracing will solve most of this issue as the projection approximation described in Section \ref{sec:projection} is no longer needed.
Still, in the intersection area between two triangles, the gradients will be discontinuous due to how alpha blending works, as shown in Equation \ref{eq:alpha_blending}.

A potential approach to solving this, achieving a smooth transition when triangles move through each other,
is to use a dynamic alpha blending function where the ray distance between the current triangle intersection and the next triangle intersection is taken into account.
This will however create a lot of dependence between the triangles, as the alpha value of each triangle is dependent on the alpha value of the next triangle which is dependent on the alpha value of the next triangle and so on.

\subsection{Inter Triangle Regularization}
A key benefit of capturing the intersections between rays and triangles is that we can perform inter-triangle regularization.
For each intersection point with a triangle, we can capture information about the distance and color of the previous and next intersection points.
If the distances are within some adaptive threshold, we can add a regularization term to the loss function, such that neighboring triangles should stick together.
Hopefully, this will help towards having smoother surfaces and aligning the face normal of neighboring triangles.
This is illustrated in the rightmost example in Figure \ref{fig:optimization}.

\subsection{Reflections}
Reflections in the scene pose a significant challenge in the field of \gls{nvs}, as it requires the ability to capture view-dependent color.
As discussed earlier, the use of spherical harmonics solves this to some degree, but it comes at a significant memory cost.
When using ray tracing, it might be possible to circumvent the need for view-dependent color by using ray-traced reflections.

Without the use of ray tracing cores, this is impossible, as the whole rendering pipeline for the Gaussian splats, discussed in Section \ref{sec:rasterization}, would have to be performed for each reflected ray as they would not share the same origin.
This would make the rendering process several million times slower.
But with the use of ray tracing cores, rendering a reflected ray is no different than rendering a regular ray \footnote{It might be slightly worse due to data locality, but this should be handled by the hardware.}.

It is however still very likely that it is infeasible to optimize the scene with reflections.
A core reason for this is that it is very expensive to capture glossy reflections, such as light reflected from a table surface, as statistical sampling with a large number of sample rays is required to get a good result.
Another key challenge is that a very small change in the orientation of a triangle can have large effects on the reflected light, resulting in very sharp gradients.


\subsection{Leverage Polarization}
Using polarization cameras might be a way to circumvent the need for view-dependent color.
The simple approach would be to remove reflected light from the scene.
This approach only targets the input data and should thus be easy to evaluate using the implementation from the paper.
The results from this approach will however look different than what a regular camera would capture, as the reflected light is partially removed.

Another, arguably more interesting, approach would be to use the polarization information to estimate the surface normal of each triangle and calculate ray-traced reflections.
As the polarization cameras can capture the angle of polarization for each pixel, using multiple views it should be possible to estimate the three-dimensional surface normals with some degree of accuracy.
This approach has been presented by one manufacturer of polarization cameras as shown in Figure \ref{fig:polarization} \cite{lucidvisionlabs3DDepthSurface2021}.

def
\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{images/polarization_normals.png}
    \caption{Angle of orientation and estimated surface normals from polarized light \cite{lucidvisionlabs3DDepthSurface2021}}.
    \label{fig:polarization}
\end{figure}
